{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Trabajo Fin de Máster <br/> Diseño de una arquitectura multimodal para descripción textual de pares imagen-audio\n",
        "\n",
        "## Script 4. Entrenamiento del modelo CLIP con base de datos propia\n",
        "\n",
        "En este notebook, empleamos la base de datos de imágenes que obtuvimos en el Script 3 para realizar fine-tuning sobre el modelo preentrenado CLIP (Contrastive Language-Image Pre-Training), creado por OpenAI."
      ],
      "metadata": {
        "id": "PFvqxngSmK7p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 1. Importación de paquetes"
      ],
      "metadata": {
        "id": "2smUTVGegCuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"..\")\n",
        "os.getcwd()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": "'/mnt/batch/tasks/shared/LS_root/mounts/clusters/tfm-cpu/code/Users/jose.puche/Scripts'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1724868047328
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import clip\n",
        "from torchvision.datasets import ImageFolder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, Subset, SubsetRandomSampler, DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from tfm_lib.EarlyStopping import EarlyStopping\n",
        "from tfm_lib.modelos import CLIP_adapted\n",
        "from tfm_lib.datasets import ImageDataset"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n  warnings.warn(\"Can't initialize NVML\")\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "executionInfo": {
          "elapsed": 15676,
          "status": "ok",
          "timestamp": 1721325782392,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          },
          "user_tz": -120
        },
        "id": "xTCFxiapgCuV",
        "gather": {
          "logged": 1724868073539
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(0)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "executionInfo": {
          "elapsed": 13,
          "status": "ok",
          "timestamp": 1721325782393,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          },
          "user_tz": -120
        },
        "id": "axySI_Oq7Mvf",
        "gather": {
          "logged": 1724868073830
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.autograd.set_detect_anomaly(True)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f8d3eae69b0>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 11,
          "status": "ok",
          "timestamp": 1721325782393,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          },
          "user_tz": -120
        },
        "id": "bSvjOYHgw44U",
        "outputId": "6267b2ea-498a-4c92-cfdf-173e80014e98",
        "gather": {
          "logged": 1724868074114
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 2. Configuración e inicialización de wandb"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1724868075792
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login #wandb_token"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/azureuser/.netrc\r\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = './../Final_Database/image'\n",
        "num_epochs = 20\n",
        "BATCH_SIZE = 16\n",
        "data_augmentation = True\n",
        "da = \"_DA\" if data_augmentation else \"\"\n",
        "lr = 5e-4\n",
        "output_dim = 2\n",
        "selected_model = 'ViT-B/32'\n",
        "\n",
        "model_parameters_file = f\"./modelos/clip/CLIP_{selected_model.replace('/','')}_{output_dim}pers_lr{f'{lr:.0e}'}_bs{BATCH_SIZE}_{num_epochs}ep{da}.pt\"\n",
        "print(model_parameters_file)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "./modelos/clip/CLIP_ViT-B32_2pers_lr5e-04_bs16_20ep_DA.pt\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1724868081002
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WandB – Initialize a new run\n",
        "run_name = model_parameters_file.split(\"/\")[-1].replace('.pt', '')\n",
        "wandb.init(entity=\"josealbertoap\", project='TFM', name = run_name, tags=[\"image\"])\n",
        "\n",
        "# WandB – Config is a variable that holds and saves hyperparameters and inputs\n",
        "config = wandb.config          # Initialize config\n",
        "config.batch_size = BATCH_SIZE          # input batch size for training (default: 64)\n",
        "config.test_batch_size = BATCH_SIZE    # input batch size for testing (default: 1000)\n",
        "config.epochs = num_epochs             # number of epochs to train (default: 10)\n",
        "config.lr = lr              # learning rate (default: 0.01)\n",
        "config.momentum = 0          # SGD momentum (default: 0.5)\n",
        "config.no_cuda = True         # disables CUDA training\n",
        "config.seed = 0               # random seed (default: 42)\n",
        "config.log_interval = 1     # how many batches to wait before logging training status\n",
        "config.num_classes = output_dim"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjosealbertoap\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "wandb version 0.17.7 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.17.5"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/mnt/batch/tasks/shared/LS_root/mounts/clusters/tfm-cpu/code/Users/jose.puche/Scripts/wandb/run-20240828_180122-x160qp4z</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/josealbertoap/TFM/runs/x160qp4z' target=\"_blank\">CLIP_ViT-B32_2pers_lr5e-04_bs16_20ep_DA</a></strong> to <a href='https://wandb.ai/josealbertoap/TFM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/josealbertoap/TFM' target=\"_blank\">https://wandb.ai/josealbertoap/TFM</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/josealbertoap/TFM/runs/x160qp4z' target=\"_blank\">https://wandb.ai/josealbertoap/TFM/runs/x160qp4z</a>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1724868094180
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 3. Definición de la función de pérdida"
      ],
      "metadata": {
        "id": "nNDygGUIgCuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función de pérdida\n",
        "def loss_fn(logits, labels):\n",
        "    \"\"\"\n",
        "    logits: Las salidas del modelo (predicciones) para cada clase.\n",
        "    labels: Las etiquetas verdaderas (números enteros) para cada ejemplo.\n",
        "    \"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()  # Función de pérdida de entropía cruzada\n",
        "    return criterion(logits, labels)\n",
        "\n",
        "# Ejemplo de cómo usar la función de pérdida\n",
        "logits = torch.tensor([[0.8, 0.1, 0.1], [0.2, 0.7, 0.1], [0.3, 0.2, 0.5]])\n",
        "labels = torch.tensor([0, 1, 2])\n",
        "\n",
        "loss = loss_fn(logits, labels)\n",
        "print(\"Pérdida:\", loss.item())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Pérdida: 0.7991690635681152\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 15,
          "status": "ok",
          "timestamp": 1721325782974,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          },
          "user_tz": -120
        },
        "id": "_HsDPC3xgCuZ",
        "outputId": "9b86853e-9616-4fc2-d3b4-ed18bc230732",
        "gather": {
          "logged": 1724868094412
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 4. Entrenamiento del modelo"
      ],
      "metadata": {
        "id": "45TIaiP6gCud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import Resize, Compose, ColorJitter, RandomHorizontalFlip, \\\n",
        "                                   RandomResizedCrop, RandomRotation, Normalize, ToTensor\n",
        "\n",
        "def train_test_dataloaders(database_df, model, num_classes, data_augmentation=False, BATCH_SIZE=32, test_split=0.2):\n",
        "\n",
        "    dataset = ImageDataset(database_df, num_classes, image_transform = model.preprocess)\n",
        "\n",
        "    train_idx, test_idx = train_test_split(list(range(len(dataset))), test_size=test_split)\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "\n",
        "    # test_subset = Subset(dataset, test_idx) # En caso de que quisiéramos un Dataset y no un Dataloader\n",
        "    test_sampler = SubsetRandomSampler(test_idx)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset, batch_size=512, sampler=test_sampler)\n",
        "\n",
        "    # En caso de tener data augmentation, cambiamos el dataset para el Dataloader de train\n",
        "    if data_augmentation:\n",
        "\n",
        "      augmentation = Compose([\n",
        "            RandomHorizontalFlip(p=0.3),\n",
        "            RandomRotation(degrees=(0, 45), fill=0),\n",
        "            RandomResizedCrop(size=(224, 224), scale=(0.2, 1.0), ratio=(0.8, 1.2)),\n",
        "            ToTensor(),\n",
        "            Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
        "        ])\n",
        "\n",
        "      dataset = ImageDataset(database_df, num_classes, image_transform = augmentation)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
        "\n",
        "    return train_loader, test_loader, dataset.labelencoder.classes_"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "executionInfo": {
          "elapsed": 13,
          "status": "ok",
          "timestamp": 1721325782975,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          },
          "user_tz": -120
        },
        "id": "YyBDPCxxMy3R",
        "gather": {
          "logged": 1724868094667
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargamos el modelo pre-entrenado y procesador de CLIP\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = CLIP_adapted(selected_model, device, output_dim)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "train_loader, test_loader, classes = train_test_dataloaders(pd.read_csv(f'{folder_path}/imagesDB_train.csv'),\n",
        "                                                            model, output_dim, data_augmentation, BATCH_SIZE, 0.2)"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 18978,
          "status": "ok",
          "timestamp": 1721325801941,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          },
          "user_tz": -120
        },
        "id": "7PleTEKgVQwN",
        "outputId": "213078f3-192e-49c5-adac-cf10c5cc2b59",
        "gather": {
          "logged": 1724868101857
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializa el optimizador\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience = 3)\n",
        "\n",
        "train_loss = {}\n",
        "test_loss = {}\n",
        "train_acc = {}\n",
        "test_acc = {}\n",
        "\n",
        "# Creamos la lista de descripciones para evaluar el modelo\n",
        "print(f\"People:{classes}\\n\")\n",
        "eval_descriptions = torch.cat([clip.tokenize(f\"a photo of {c}\") for c in classes])\n",
        "\n",
        "early_stopping = EarlyStopping(patience=5, verbose=True, delta=0.01, path=model_parameters_file)\n",
        "\n",
        "wandb.watch(model, log=\"all\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # Entrena el modelo\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    train_steps = tqdm(train_loader, unit=\"batch\")\n",
        "\n",
        "    for images, labels in train_steps:\n",
        "\n",
        "        train_steps.set_description(f\"Epoch [{epoch+1}/{num_epochs}]. Training\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        texts = eval_descriptions.to(device)\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        output = model(images, texts)\n",
        "\n",
        "        # Cálculo de accuracy\n",
        "        predictions = output.argmax(dim=1, keepdim=True).squeeze()\n",
        "        correct = (predictions == labels).sum().item()\n",
        "\n",
        "        total_samples += labels.size(0)\n",
        "        total_correct += correct\n",
        "\n",
        "        # Cálculo de la función de pérdida y actualización del modelo\n",
        "        loss = loss_fn(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        train_steps.set_postfix(mean_loss=epoch_loss/total_samples, mean_accuracy = total_correct / total_samples)\n",
        "\n",
        "    train_loss[epoch+1] = epoch_loss / len(train_loader)\n",
        "    train_acc[epoch+1] = total_correct / total_samples\n",
        "\n",
        "    # Evaluación en el conjunto de prueba\n",
        "    model.eval()  # Cambiamos al modo de evaluación\n",
        "    epoch_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    test_steps = tqdm(test_loader, unit=\"batch\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_steps:  # Itera sobre los datos de prueba\n",
        "\n",
        "            test_steps.set_description(f\"Epoch [{epoch+1}/{num_epochs}]. Evaluation\")\n",
        "\n",
        "            texts = eval_descriptions.to(device)\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            output = model(images, texts)\n",
        "\n",
        "            # Cálculo de la accuracy\n",
        "            predictions = output.argmax(dim=1, keepdim=True).squeeze()\n",
        "            correct = (predictions == labels).sum().item()\n",
        "\n",
        "            total_samples += labels.size(0)\n",
        "            total_correct += correct\n",
        "\n",
        "            # Cálculo de la función de pérdida y actualización del modelo\n",
        "            loss = loss_fn(output, labels)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            test_steps.set_postfix(mean_loss=epoch_loss/total_samples, mean_accuracy = total_correct / total_samples)\n",
        "\n",
        "        test_loss[epoch+1] = epoch_loss / len(test_loader)\n",
        "        test_acc[epoch+1] = total_correct / total_samples\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}]:')\n",
        "        print(f'- Training. Loss = {train_loss[epoch+1]}; Accuracy = {train_acc[epoch+1]}.')\n",
        "        print(f'- Evaluation. Loss = {test_loss[epoch+1]}; Accuracy = {test_acc[epoch+1]}.')\n",
        "\n",
        "        wandb.log({\n",
        "                    'Epoch': epoch+1,\n",
        "                    'Training Loss': train_loss[epoch+1],\n",
        "                    'Training Accuracy': train_acc[epoch+1],\n",
        "                    'Evaluation Loss': test_loss[epoch+1],\n",
        "                    'Evaluation Accuracy': test_acc[epoch+1],\n",
        "                })\n",
        "\n",
        "        # Llamar a early_stopping con la pérdida de validación actual y el modelo\n",
        "        early_stopping(test_loss[epoch+1], model)\n",
        "        print('')\n",
        "\n",
        "        # Si se alcanza el criterio de early stopping, romper el bucle\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "        # Reducir el learning rate en caso de que no esté mejorando la pérdida\n",
        "        scheduler.step(test_loss[epoch+1])\n",
        "\n",
        "print({'train_acc': train_acc, 'train_loss': train_loss, 'val_acc': test_acc, 'val_loss': test_loss})\n",
        "\n",
        "wandb.save(model_parameters_file)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "People:['Alba Azorin Zafrilla' 'Jose Alberto Azorin Puche']\n\nEpoch [1/20]:\n- Training. Loss = 0.5583237511190501; Accuracy = 0.7777777777777778.\n- Evaluation. Loss = 0.01938702166080475; Accuracy = 1.0.\nValidation loss decreased (inf --> 0.019387).  Saving model ...\n\nEpoch [2/20]:\n- Training. Loss = 0.012862671420655468; Accuracy = 1.0.\n- Evaluation. Loss = 0.005323350895196199; Accuracy = 1.0.\nEarlyStopping counter: validation loss is under the value of delta (0.01).\nValidation loss decreased (0.019387 --> 0.005323).  Saving model ...\n\nEarly stopping\n{'train_acc': {1: 0.7777777777777778, 2: 1.0}, 'train_loss': {1: 0.5583237511190501, 2: 0.012862671420655468}, 'val_acc': {1: 1.0, 2: 1.0}, 'val_loss': {1: 0.01938702166080475, 2: 0.005323350895196199}}\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Epoch [1/20]. Training: 100%|██████████| 11/11 [01:01<00:00,  5.59s/batch, mean_accuracy=0.778, mean_loss=0.0359]\nEpoch [1/20]. Evaluation: 100%|██████████| 1/1 [00:08<00:00,  8.03s/batch, mean_accuracy=1, mean_loss=0.000451]\nEpoch [2/20]. Training: 100%|██████████| 11/11 [00:58<00:00,  5.28s/batch, mean_accuracy=1, mean_loss=0.000827]\nEpoch [2/20]. Evaluation: 100%|██████████| 1/1 [00:07<00:00,  7.97s/batch, mean_accuracy=1, mean_loss=0.000124]\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "['/mnt/batch/tasks/shared/LS_root/mounts/clusters/tfm-cpu/code/Users/jose.puche/Scripts/wandb/run-20240828_180122-x160qp4z/files/modelos/clip/CLIP_ViT-B32_2pers_lr5e-04_bs16_20ep_DA.pt']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1550078,
          "status": "ok",
          "timestamp": 1721327352002,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          },
          "user_tz": -120
        },
        "id": "cfnL-Wak6-UN",
        "gather": {
          "logged": 1724868252195
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 5. Evaluación del modelo entrenado"
      ],
      "metadata": {
        "id": "nYkyndl3gCug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = ImageDataset(pd.read_csv(f'{folder_path}/imagesDB_test.csv'), \n",
        "                            output_dim, image_transform = model.preprocess)\n",
        "test_dl = torch.utils.data.DataLoader(test_dataset, batch_size=1048, shuffle=True)"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1724868252820
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inference (model, test_dl):\n",
        "  correct_prediction = 0\n",
        "  total_prediction = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    predictions = []\n",
        "    label_list = []\n",
        "    for data in test_dl:\n",
        "\n",
        "      inputs, labels = data[0].to(device), data[1].to(device)\n",
        "      texts = eval_descriptions.to(device)\n",
        "\n",
        "      outputs = model(inputs, texts)\n",
        "\n",
        "      _, prediction = torch.max(outputs,1)\n",
        "\n",
        "      correct_prediction += (prediction == labels).sum().item()\n",
        "      total_prediction += prediction.shape[0]\n",
        "\n",
        "      predictions.extend(prediction)\n",
        "      label_list.extend(data[1])\n",
        "\n",
        "  acc = correct_prediction/total_prediction\n",
        "  print(f'Accuracy: {acc:.2f}, Total items: {total_prediction}')\n",
        "\n",
        "  return predictions, label_list\n",
        "\n",
        "model.load_state_dict(torch.load(model_parameters_file, map_location=torch.device('cpu')))\n",
        "result = inference(model, test_dl)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Accuracy: 1.00, Total items: 48\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1724868288182
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, f1_score, confusion_matrix\n",
        "import seaborn as sn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "def extraer_iniciales(name):\n",
        "    name_words = name.split(' ')\n",
        "    r = re.compile(\"^[A-Z][A-z]*\")\n",
        "    valid_words = list(filter(r.match, name_words))\n",
        "    if len(valid_words) <=3:\n",
        "        name = valid_words[0]\n",
        "        valid_words.remove(valid_words[0])\n",
        "    else:\n",
        "        name = f'{valid_words[0]} {valid_words[1]}'\n",
        "        valid_words.remove(valid_words[0])\n",
        "        valid_words.remove(valid_words[1])\n",
        "    surname = re.sub('(?<=[A-Z])[A-z]+', '.', ' '.join(valid_words))\n",
        "    return f'{name} {surname}'\n",
        "\n",
        "def font_scale(num_classes):\n",
        "    if num_classes <= 10:\n",
        "        return 1.0\n",
        "    elif num_classes <= 20:\n",
        "        return 0.75\n",
        "    elif num_classes <= 30:\n",
        "        return 0.65\n",
        "    else:\n",
        "        return 0.45\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    people = list(map(extraer_iniciales, test_dataset.labelencoder.classes_))\n",
        "\n",
        "    df_cm = pd.DataFrame((cf_matrix / np.sum(cf_matrix, axis=1)[:, None]).round(3), index=people, columns=people)\n",
        "    \n",
        "    plt.figure(figsize=(8, 6))  \n",
        "    sn.set(font_scale = font_scale(df_cm.shape[0]))  \n",
        "    heatmap = sn.heatmap(df_cm, annot=True, cbar=False, cmap='Purples', fmt='g', xticklabels=False)\n",
        "\n",
        "    # Ajusta la rotación y alineación de los ticks de los ejes\n",
        "    heatmap.set_yticklabels(heatmap.get_yticklabels(), rotation=0, ha='right')\n",
        "\n",
        "    plt.tight_layout()  # Asegura que todo se ajuste bien en la figura\n",
        "    plt.savefig(model_parameters_file.replace('/modelos/', '/results/').replace('.pt', '.png'))\n",
        "\n",
        "    return plt.gcf()\n",
        "\n",
        "def get_metrics(result):\n",
        "    accuracy = accuracy_score(result[1], result[0])\n",
        "    precision = precision_score(result[1], result[0], average='macro')\n",
        "    recall = recall_score(result[1], result[0], average='macro')\n",
        "    f1 = f1_score(result[1], result[0], average='macro')\n",
        "\n",
        "    metrics = {\n",
        "        'Test accuracy': accuracy,\n",
        "        'Test precision': precision,\n",
        "        'Test recall': recall,\n",
        "        'F1-score': f1\n",
        "    }\n",
        "\n",
        "    print(metrics)\n",
        "\n",
        "    metrics['Confusion Matrix'] = wandb.Image(plot_confusion_matrix(result[1],result[0]))\n",
        "    metrics['Test metrics'] = wandb.Table(columns=[\"Metric name\", \"Value\"], \n",
        "                                          data=[[\"Test accuracy\", accuracy], [\"Test precision\", precision],\n",
        "                                                [\"Test recall\", recall], [\"Test F1-Score\", f1]])\n",
        "\n",
        "    return metrics\n",
        "\n",
        "metrics = get_metrics(result)\n",
        "wandb.log(metrics)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{'Test accuracy': 1.0, 'Test precision': 1.0, 'Test recall': 1.0, 'F1-score': 1.0}\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 800x600 with 1 Axes>",
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxAAAAJICAYAAADxUwLTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAgWklEQVR4nO3de7TVdZ3/8RcQ4gXTEfCG10yPhh4gLZTwhzdADUxzEspFv0zzkpkjXkYcx9vk5C1HRZxKzRvZz8YlxjkSitcuOt6apKl0nNFS0xQPIiJyO57fH+SZ8IC8RfGkPR5rsRbn+937s99nrwVrP/f3+927S1tbW1sAAAAKunb2AAAAwPuHgAAAAMoEBAAAUCYgAACAMgEBAACUCQgAAKBMQAAAAGUCAgAAKBMQAABA2Yc6ewA+2HbrclpnjwDwrrtj0RmdPQLAu67bh2rHFhyBAAAAygQEAABQJiAAAIAyAQEAAJQJCAAAoExAAAAAZQICAAAoExAAAECZgAAAAMoEBAAAUCYgAACAMgEBAACUCQgAAKBMQAAAAGUCAgAAKBMQAABAmYAAAADKBAQAAFAmIAAAgDIBAQAAlAkIAACgTEAAAABlAgIAACgTEAAAQJmAAAAAygQEAABQJiAAAIAyAQEAAJQJCAAAoExAAAAAZQICAAAoExAAAECZgAAAAMoEBAAAUCYgAACAMgEBAACUCQgAAKBMQAAAAGUCAgAAKBMQAABAmYAAAADKBAQAAFAmIAAAgDIBAQAAlAkIAACgTEAAAABlAgIAACgTEAAAQJmAAAAAygQEAABQJiAAAIAyAQEAAJQJCAAAoExAAAAAZQICAAAoExAAAECZgAAAAMoEBAAAUCYgAACAMgEBAACUCQgAAKBMQAAAAGUCAgAAKBMQAABAmYAAAADKBAQAAFAmIAAAgDIBAQAAlAkIAACgTEAAAABlAgIAACgTEAAAQJmAAAAAygQEAABQJiAAAIAyAQEAAJQJCAAAoExAAAAAZQICAAAoExAAAECZgAAAAMoEBAAAUCYgAACAMgEBAACUCQgAAKBMQAAAAGUCAgAAKBMQAABAmYAAAADKBAQAAFAmIAAAgDIBAQAAlAkIAACgTEAAAABlAgIAACgTEAAAQJmAAAAAygQEAABQJiAAAIAyAQEAAJQJCAAAoExAAAAAZQICAAAoExAAAECZgAAAAMoEBAAAUCYgAACAMgEBAACUCQgAAKBMQAAAAGUCAgAAKBMQAABAmYAAAADKBAQAAFAmIAAAgDIBAQAAlAkIAACgTEAAAABlAgIAACgTEAAAQJmAAAAAygQEAABQJiAAAIAyAQEAAJQJCAAAoExAAAAAZQICAAAoExAAAECZgAAAAMoEBAAAUCYgAACAMgEBAACUfaizBwBYWWustVpGn/ipfGzQJtn2k33z4fXWzDlfuinTrvllZ48GsNIWLlyYCRMuyZSmKZkzZ0622aYhx3796xk8+FOdPRokcQRimfbbb780NDTkoYce6rDv/vvvT0NDQ371q1+1b2toaMiVV165yuf6zW9+k4aGhgwbNmyl1xg7dmwaGhre8g+8X6zTe8186fTds9l2ffI/jzzf2eMAvCtOOWV8rrn2mowcOSrjTz4l3bp1zZFHHZmHH364s0eDJI5AdPD444/nscceS5I0NTVlp5126uSJ/ldTU1OS5KmnnsojjzyS/v37v+01Tj/99MydO7fD9qeffjonnXRSdt1113c8J7xXWp57JZ/d8LzMen5uGnbcON956MjOHgngHZkxY0am/nhqTjjhxHz5kC8nST7zmc9kv8/sl29deEGu//4POnlCcASig6ampnTt2jWDBg3KtGnTsmjRos4eKUny+uuvZ+rUqdlxxx3To0eP9ph4uz760Y9mwIABS/3Zfvvtc91116VXr14555xz3uXJYdVZtLA1s57vGMQA71e33XZrunXrloM+d1D7th49euTAAw/ML3/5yzz33HOdOB0sISD+TFtbW5qbm7PzzjvnkEMOyezZs/PTn/60dN/W1tacd9552XnnnTNw4MCcfPLJS73TP2/evJx11lkZMWJE+vfvnz322COnnXZaXnnlldL6Dz74YP74xz9mzJgx2W233TJ16tS0trau1O/5ZhdffHF+9atf5fzzz8966633rqwJALx9v330t9l88y3Ss2fPpbbvsMMOSZJHH320M8aCpQiIP/OLX/wif/jDHzJy5MgMGTIk6667bpqbm0v3ve666/LEE0/k3HPPzQknnJBbb701//iP/9i+f/78+Wltbc1xxx2Xyy+/PMcee2wefPDBfPWrXy2t39TUlDXWWCN77bVXRo4cmZaWltx7770r9Xv+ufvuuy9XXHFFDjvssOyyyy7veD0AYOXNnDkzffr06bC9T+8l216Y+cJ7PRJ04BqIP9Pc3JwePXpk+PDh6d69e0aMGJEpU6bk1VdfzVprrfWW911ttdUyceLEdOvWLcmSw42nnnpqvva1r2WrrbbKeuutlzPPPLP99osXL84mm2ySL3zhC3nyySez5ZZbLnfthQsX5rbbbssee+yRNddcM7vttlvWXnvtNDU1vaNrFl566aWcdNJJaWxszLHHHrvS6wAA744FCxZktdW6d9jeo0ePJfvnz3+vR4IOHIH4k8WLF2fatGkZOnRo1l577STJqFGj8tprr2X69OkrvP/uu+/eHg9Jsvfee6etrW2pT2u6+eabs//++2fgwIHp169fvvCFLyRJfve7373l2j/5yU/y8ssvZ+TIkUmWxMqwYcMyffr0zH8H/5GMHz8+r732Wi644IJ86ENaEgA6W48ePbJwYcfrLxcsWLBk/+qrv9cjQQcC4k9+/vOfZ9asWdl9990zZ86cP33u8jbp06dP6TSmXr16LfVzz54906NHj7zwwpJDjdOnT8/f//3fp7GxMRdddFF++MMfZuLEiUn+9z+F5Wlqasraa6+dAQMGtM+2++67Z968ebnzzjtX6vedNGlS7rrrrpx11lnZdNNNV2oNAODd1adPn8ycObPD9pkvLtm2fp/13+uRoANvO//JG59qNH78+IwfP36pfS+99FJaWlo6RMKfa2lpWernuXPnZsGCBVl//SX/0KdNm5btttsuZ511VvttHnjggRXONXfu3Nx9992ZP3/+Mq9RmDJlSvbdd98VrvPnHnvssZx33nk58MAD3/Z9AYBVZ9ttt8sDDzyQuXPnLnUh9YwZM/60f9vOGg3aCYgkr732Wu64447stdde+eIXv7jUvhdffDHjxo3L1KlTM3bs2OWucdddd2X8+PHtpzFNmzYtXbp0af/UhPnz56d796XPaax8FOvtt9+e+fPn58wzz+xwncTkyZPT3Nyc2bNnZ9111638qpk/f36OP/749O3bN6eeemrpPgDAe2P48OG56qrv5Yf/9sP274FYuHBhJk++KY2Njdloo406eUIQEEmSO+64I/PmzcvYsWMzaNCgDvuvuOKKNDc3v2VALFy4MEcffXQ+//nP55lnnskFF1yQESNGZKuttkqSDB48OGeddVYmTpyYgQMH5p577sl99923wtmamprSt2/fjB49Ol26dFlq3zrrrJPJkydn2rRpGTNmTE455ZTcfPPN+c1vfrPc9c4555w8/vjjOfPMM/Nf//Vfy7zNRz/60fTs2TOXXnppLrvsskyfPj19+/Zd4azQGQ44+pPpue4a6bXxkmuXdhnVkD6brJMkuWnCv+fVOW99iiDAX5L+jf0zYsTeueiif8mslpZsttnm+dGPbs6zzz6bb/zTNzp7PEgiIJIs+fSljTfeeJnxkCT7779//vmf/zlPPfXUctcYO3ZsZs2alZNOOikLFy7MsGHDctppp7XvHzNmTJ555plMmjQpV155ZYYMGZJvfetbOeigg5a7ZktLS+67774cfvjhHeIhWXIYc7vttktTU1PGjBmT119/fYXfDfGTn/wkyZJvpF6ea6+9NoMGDUpbW1taW1vT1tb2lmtCZxp9wqey4RZ/0/7z0AP7ZeiB/ZIk0yc9IiCA951zvnlOLplwSaY0TcmcOXPSsE1DLpv4r9lpp0909miQJOnS5tUhq9BuXU5b8Y0A3mfuWHRGZ48A8K7r9qHa5yv5FCYAAKBMQAAAAGUCAgAAKBMQAABAmYAAAADKBAQAAFAmIAAAgDIBAQAAlAkIAACgTEAAAABlAgIAACgTEAAAQJmAAAAAygQEAABQJiAAAIAyAQEAAJQJCAAAoExAAAAAZQICAAAoExAAAECZgAAAAMoEBAAAUCYgAACAMgEBAACUCQgAAKBMQAAAAGUCAgAAKBMQAABAmYAAAADKBAQAAFAmIAAAgDIBAQAAlAkIAACgTEAAAABlAgIAACgTEAAAQJmAAAAAygQEAABQJiAAAIAyAQEAAJQJCAAAoExAAAAAZQICAAAoExAAAECZgAAAAMoEBAAAUCYgAACAMgEBAACUCQgAAKBMQAAAAGUCAgAAKBMQAABAmYAAAADKBAQAAFAmIAAAgDIBAQAAlAkIAACgTEAAAABlAgIAACgTEAAAQJmAAAAAygQEAABQJiAAAIAyAQEAAJQJCAAAoExAAAAAZQICAAAoExAAAECZgAAAAMoEBAAAUCYgAACAMgEBAACUCQgAAKBMQAAAAGUCAgAAKBMQAABAmYAAAADKBAQAAFAmIAAAgDIBAQAAlAkIAACgTEAAAABlAgIAACgTEAAAQJmAAAAAygQEAABQJiAAAIAyAQEAAJQJCAAAoExAAAAAZQICAAAoExAAAECZgAAAAMoEBAAAUCYgAACAMgEBAACUCQgAAKBMQAAAAGUCAgAAKBMQAABAmYAAAADKBAQAAFAmIAAAgDIBAQAAlAkIAACgTEAAAABlAgIAACgTEAAAQJmAAAAAygQEAABQJiAAAIAyAQEAAJQJCAAAoExAAAAAZQICAAAoExAAAECZgAAAAMoEBAAAUCYgAACAMgEBAACUCQgAAKBMQAAAAGUCAgAAKBMQAABAmYAAAADKBAQAAFAmIAAAgDIBAQAAlAkIAACgTEAAAABlAgIAACgTEAAAQJmAAAAAygQEAABQJiAAAICyLm1tbW2dPQQfXK2LX+/sEQDedXt2P6OzRwB4193ddlbpdo5AAAAAZQICAAAoExAAAECZgAAAAMoEBAAAUCYgAACAMgEBAACUCQgAAKBMQAAAAGUCAgAAKBMQAABAmYAAAADKBAQAAFAmIAAAgDIBAQAAlAkIAACgTEAAAABlAgIAACgTEAAAQJmAAAAAygQEAABQJiAAAIAyAQEAAJQJCAAAoExAAAAAZQICAAAoExAAAECZgAAAAMoEBAAAUCYgAACAMgEBAACUCQgAAKBMQAAAAGUCAgAAKBMQAABAmYAAAADKBAQAAFAmIAAAgDIBAQAAlAkIAACgTEAAAABlAgIAACgTEAAAQJmAAAAAygQEAABQJiAAAIAyAQEAAJQJCAAAoExAAAAAZQICAAAoExAAAECZgAAAAMoEBAAAUCYgAACAMgEBAACUCQgAAKBMQAAAAGUCAgAAKBMQAABAmYAAAADKBAQAAFAmIAAAgDIBAQAAlAkIAACgTEAAAABlAgIAACgTEAAAQJmAAAAAygQEAABQJiAAAIAyAQEAAJQJCAAAoExAAAAAZQICAAAoExAAAECZgAAAAMoEBAAAUCYgAACAMgEBAACUCQgAAKBMQAAAAGUCAgAAKBMQAABAmYAAAADKBAQAAFAmIAAAgDIBAQAAlAkIAACgTEAAAABlAgIAACgTEAAAQJmAAAAAygQEAABQJiAAAIAyAQEAAJQJCAAAoExAAAAAZQICAAAoExAAAECZgAAAAMoEBAAAUCYgAACAMgEBAACUCQgAAKBMQAAAAGUCAgAAKBMQAABAmYAAAADKBAQAAFAmIAAAgDIBAQAAlAkIAACgTEAAAABlAgIAACgTEAAAQJmAAAAAygQEAABQJiAAAIAyAQEAAJQJCAAAoExAAAAAZQICAAAoExAAAECZgAAAAMoEBAAAUCYgAACAMgEBAACUCQgAAKBMQAAAAGUCAgAAKBMQAABAmYAAAADK3lZATJgwIQMHDlxVs6y0b3zjG2loaMjEiROXub+hoSFXXnll+89jx47NEUccsUpnmjNnTiZMmJD//u//ftfXnjVrVvr165eBAwdm/vz5K73OHnvskYaGhjQ0NORjH/tY9txzz5x++umZNWvWuzgtrDoLFy7Mt751QYbu9n8y8OMDMnrM6Nx77887eyyAd2SNtVbLl87YPef9eGymtJycu9vOyt7/d0BnjwXt3vdHIFpbW/PjH/84SdLc3NzJ0/yvOXPm5NJLL10lATF16tQsXrw48+bNy5133vmO1hoxYkRuuOGGXHvttfn85z+fH/3oRzn66KPz+uuvv0vTwqpzyinjc82112TkyFEZf/Ip6data4486sg8/PDDnT0awEpbp/ea+dLpu2ez7frkfx55vrPHgQ7e9wFx33335cUXX8zgwYPzxBNP5Ne//nVnj/SOjgpUNDc3Z6uttsoGG2yQKVOmvKO1evfunQEDBmSnnXbKYYcdlq985Sv5xS9+8RfxPMJbmTFjRqb+eGr+7u+Oy4knnJiDDjooV33v6my00cb51oUXdPZ4ACut5blX8tkNz8uYLS7Mt0+8tbPHgQ7eUUDMnj0748ePz6BBg9LY2JgxY8bkwQcfXOo2Dz/8cA4++ODsuOOOGThwYEaNGpXJkycvdZu77747n/vc59LY2Jidd945p59+eubNm1eaobm5OWuttVbOOeecdO/ePU1NTeX5b7755uy1115pbGzM2LFj88QTTyy1v62tLVdeeWVGjBiR7bffPnvuuWeuvvrqpW7zxmldM2bMyOjRo7PDDjvk+9//fvbcc88kybHHHtt+mtAzzzxTft6W5+mnn85//Md/ZNSoUfn0pz+dn/3sZ5k9e3b5d16R7bffPknaZ4W/VLfddmu6deuWgz53UPu2Hj165MADD8wvf/nLPPfcc504HcDKW7SwNbOen9vZY8ByrXRAtLa25itf+UruuuuunHDCCbn44ouz5ppr5pBDDsl//ud/Jknmzp2bI444Ij179syFF16Yyy67LAcddFDmzJnTvs60adNy1FFHZZtttsmll16aE088MdOnT88//MM/rHCGBQsW5LbbbsuwYcOywQYbZMiQIbnllltKp9/8+te/zne+850cf/zxOffcc/PCCy/ksMMOy8KFC9tvc/bZZ+eSSy7J/vvvn+9+97s54IADcsEFF+QHP/jBUmstWrQoxx9/fPbbb79cfvnl+dSnPpVLL700STJu3LjccMMNueGGG7L++uuXnre38sZpWiNHjszIkSOzaNGiTJs2bYX3q3ojHNZff/13bU1YFX776G+z+eZbpGfPnktt32GHHZIkjz76aGeMBQAfeB9a2TvefffdmTFjRq644orsuuuuSZIhQ4Zk+PDh+c53vpMJEybkySefzCuvvJJx48aloaEhSbLLLru0r9HW1pbzzjsv++67b84+++z27X369Mnhhx+er371q9l6662XO8Odd96ZV199NSNHjkySjBo1KnfddVfuv//+pR5nWVpaWjJp0qRsscUWSZKPfexj2XvvvXPTTTdlzJgxeeqppzJp0qSceeaZGT16dJJk8ODBmT9/fiZOnJjRo0ena9cl/bVo0aIcd9xx2XfffdvXf+NFzeabb54BAwa0b7/jjjtW+Ly9lVtuuSUDBgzIpptumiT5yEc+kqampowZM+Yt77c8bW1tWbx4cRYvXpxHHnkk3/72t7PpppumX79+K7UevFdmzpyZPn36dNjep/eSbS/MfOG9HgkA/iqs9BGIhx56KD179mx/EZwk3bt3z7Bhw9ovYNxss83Ss2fPnHHGGZk6dWqHT/d58skn84c//CH77LNP+4vYxYsX55Of/GS6du26wnfkm5ub06tXrwwePDjJkk8VWnPNNUunMW299dbt8ZAseaG/7bbb5pFHHkmS3HvvvUmS4cOHLzXb4MGDM3PmzA6nRwwdOnSFj5nUnrflefTRR/P444+3B1OSfPrTn87DDz+cZ599tvT4b3b99denX79+6d+/f774xS9mgw02yIQJE7L66quv1HrwXlmwYEFWW617h+09evRYsn8VX4sEAH+tVvoIxJw5c9KrV68O23v37p2XX345SbLOOuvkqquuyiWXXJKTTjopra2t2WmnnXLqqaemoaEhL730UpLk6KOPXuZjvNU5zHPmzMk999yTz3zmM3n11Vfbt++6666ZPn16zjjjjKy22mrLvf+yZu/Vq1dmzpyZJHnppZfS1taWnXfeebmz9e3bN0myxhprZK211lruY7157hU9b8szZcqUdO3aNUOGDGk/DWzo0KGZMGFCmpubc/jhh5dm+HP77LNPDj300HTv3j0bbrhh1l133be9BnSGHj16ZOHCRR22L1iwYMl+EQwAq8RKB8Q666yTlpaWDttffPHFrLPOOu0/NzY25oorrsj8+fNz//3359xzz83RRx+d22+/vf3F6mmnnZbGxsYOa73Vefi33nprFi1alBtvvDE33nhjh/133313hg8fvtz7L2v2lpaWbLvttu2/X5cuXXL99dene/eO73JuueWW7X/v0qXLch/nzarP25u1tbVl6tSpef3117P33nt32N/U1LRSAbHeeuu1nzMO7yd9+vTJ8893PE1p5otL3gRYv4/reABgVVjpgNhxxx1z5ZVX5mc/+1mGDBmSJFm8eHFuv/327Ljjjh1uv/rqq2fo0KF56qmncvbZZ2fBggX5yEc+kg033DBPP/10Dj744Lf1+E1NTenbt2+++c1vdtg3bty4NDU1vWVAPP744/n973+fzTffPEny+9//Po8++mj79Q5vXEMxe/bs7LHHHm9rtiTt0fHGu6FveLvP2xseeuihPPfccznmmGPyiU98Yql9P/3pT3P55Zfnsccea7/WBD7ott12uzzwwAOZO3fuUhdSz5gx40/7t+2s0QDgA22lA2K33XZLY2NjTjzxxBx//PHp3bt3rrvuurzwwgu55JJLkiw5CnDjjTdmr732ysYbb5wXX3wxkyZNysc//vH285RPPvnknHDCCZk3b1522223rLHGGnn22Wdzzz335Ljjjlvqnf43PP/883nwwQdz1FFHZdCgQR32jxw5Mtdff31eeeWVrL322sucv1evXjnyyCPz9a9/PUly8cUXZ4MNNshnP/vZJEuOMBx88ME56aSTcuihh6Z///5ZtGhRfve73+X+++/PZZdd9pbPT58+ffLhD384t9xySzbZZJOsttpqaWhoKD1vy9LU1NT+aU1vPl1q6623ztVXX53m5uY0NDTk0ksvzWWXXZbp06e3n2b1dg0bNiwbb7xxrrnmmpW6P6xqw4cPz1VXfS8//Lcf5suHfDnJkm+mnjz5pjQ2NmajjTbq5AkB4IPpbQXE/Pnz268r6NatW7773e/mvPPOy/nnn5958+alX79++d73vtf+XQKbbbZZunbtmosuuigtLS1Zd911M2TIkIwbN659zX322Scf/vCH8+1vf7v94ue+fftm1113Te/evZc5xxsf1br//vsvc/8BBxyQq6++Orfeemv+9m//dpm36devX4YPH57zzz8/M2fOTP/+/XPmmWcudd3Eqaeemi233DI33HBDJk6cmLXWWitbbrnlMk8herOuXbvmm9/8Zi688MJ86UtfysKFC3PHHXdkk002WeHz9maLFi3Krbfemr322muZ11qst956GTp0aJqbmzNu3Li0tbWltbU1bW1tK5xzeVpbW30bNX/R+jf2z4gRe+eii/4ls1pastlmm+dHP7o5zz77bL7xT9/o7PEA3pEDjv5keq67RnptvOSN0F1GNaTPJktOdb5pwr/n1TkL3urusEp1aXsbrzK/9rWv5dlnn81NN920KmfiA6R1sQhh1VmwYEEumXBJmpqmZM6cOWnYpiHHHPP19tMDYVXZs/sZnT0CH3D/78njsuEWf7PMfWO2uDB//P3s93Yg/irc3XZW6XalgPjtb3+bBx54IOeff36OOeaYHHHEEe94QP46CAjgg0hAAB9E1YAoncJ0yimn5OWXX84hhxySQw899B0NBgAAvH+VAmLy5Mmreg4AAOB9YKW/iRoAAPjrIyAAAIAyAQEAAJQJCAAAoExAAAAAZQICAAAoExAAAECZgAAAAMoEBAAAUCYgAACAMgEBAACUCQgAAKBMQAAAAGUCAgAAKBMQAABAmYAAAADKBAQAAFAmIAAAgDIBAQAAlAkIAACgTEAAAABlAgIAACgTEAAAQJmAAAAAygQEAABQJiAAAIAyAQEAAJQJCAAAoExAAAAAZQICAAAoExAAAECZgAAAAMoEBAAAUCYgAACAMgEBAACUCQgAAKBMQAAAAGUCAgAAKBMQAABAmYAAAADKBAQAAFAmIAAAgDIBAQAAlAkIAACgTEAAAABlAgIAACgTEAAAQJmAAAAAygQEAABQJiAAAIAyAQEAAJQJCAAAoExAAAAAZQICAAAoExAAAECZgAAAAMoEBAAAUCYgAACAMgEBAACUCQgAAKBMQAAAAGUCAgAAKBMQAABAmYAAAADKBAQAAFAmIAAAgDIBAQAAlAkIAACgTEAAAABlAgIAACgTEAAAQJmAAAAAygQEAABQJiAAAIAyAQEAAJQJCAAAoExAAAAAZQICAAAoExAAAECZgAAAAMoEBAAAUCYgAACAMgEBAACUCQgAAKBMQAAAAGUCAgAAKBMQAABAmYAAAADKBAQAAFAmIAAAgDIBAQAAlAkIAACgTEAAAABlAgIAACgTEAAAQJmAAAAAygQEAABQJiAAAIAyAQEAAJQJCAAAoExAAAAAZQICAAAoExAAAECZgAAAAMoEBAAAUCYgAACAMgEBAACUCQgAAKBMQAAAAGUCAgAAKBMQAABAmYAAAADKBAQAAFAmIAAAgDIBAQAAlAkIAACgTEAAAABlAgIAACgTEAAAQJmAAAAAygQEAABQJiAAAIAyAQEAAJQJCAAAoExAAAAAZQICAAAoExAAAECZgAAAAMoEBAAAUCYgAACAMgEBAACUCQgAAKBMQAAAAGVd2tra2jp7CAAA4P3BEQgAAKBMQAAAAGUCAgAAKBMQAABAmYAAAADKBAQAAFAmIAAAgDIBAQAAlAkIAACg7P8DGnD7G4/iRc4AAAAASUVORK5CYII="
          },
          "metadata": {}
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1724868292068
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_results = []\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    for i in range(4):\n",
        "\n",
        "        read_image = Image.open(f'./../Test_images/IMG_000{i}.jpg')\n",
        "        image = model.preprocess(read_image).unsqueeze(0).to(device)\n",
        "\n",
        "        output = model(image, eval_descriptions)\n",
        "        probs = torch.round(output.softmax(dim=-1), decimals=4)\n",
        "        pred_prob = torch.max(probs).item()\n",
        "        pred_person = classes[torch.argmax(probs)]\n",
        "        my_prob = probs.squeeze()[list(classes).index('Jose Alberto Azorin Puche')].item()\n",
        "\n",
        "        image_results.append([f'Imagen {i+1}', pred_person, pred_prob, my_prob])\n",
        "print(image_results)\n",
        "wandb.log({\"Test images results\": wandb.Table(columns=[\"Imagen\", \"Persona\", \"Probabilidad\", \"Prob (Joseal)\"], data=image_results)})        \n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[['Imagen 1', 'Jose Alberto Azorin Puche', 0.6251999735832214, 0.6251999735832214], ['Imagen 2', 'Jose Alberto Azorin Puche', 0.9674999713897705, 0.9674999713897705], ['Imagen 3', 'Jose Alberto Azorin Puche', 0.9225000143051147, 0.9225000143051147], ['Imagen 4', 'Jose Alberto Azorin Puche', 0.9814000129699707, 0.9814000129699707]]\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 522,
          "status": "ok",
          "timestamp": 1721327397900,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          },
          "user_tz": -120
        },
        "id": "wl5Ev629E56E",
        "outputId": "6b688c6f-60fd-4e88-d005-96297860291f",
        "gather": {
          "logged": 1724868294616
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁█</td></tr><tr><td>Evaluation Accuracy</td><td>▁▁</td></tr><tr><td>Evaluation Loss</td><td>█▁</td></tr><tr><td>F1-score</td><td>▁</td></tr><tr><td>Test accuracy</td><td>▁</td></tr><tr><td>Test precision</td><td>▁</td></tr><tr><td>Test recall</td><td>▁</td></tr><tr><td>Training Accuracy</td><td>▁█</td></tr><tr><td>Training Loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>2</td></tr><tr><td>Evaluation Accuracy</td><td>1.0</td></tr><tr><td>Evaluation Loss</td><td>0.00532</td></tr><tr><td>F1-score</td><td>1.0</td></tr><tr><td>Test accuracy</td><td>1.0</td></tr><tr><td>Test precision</td><td>1.0</td></tr><tr><td>Test recall</td><td>1.0</td></tr><tr><td>Training Accuracy</td><td>1.0</td></tr><tr><td>Training Loss</td><td>0.01286</td></tr></table><br/></div></div>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run <strong style=\"color:#cdcd00\">CLIP_ViT-B32_2pers_lr5e-04_bs16_20ep_DA</strong> at: <a href='https://wandb.ai/josealbertoap/TFM/runs/x160qp4z' target=\"_blank\">https://wandb.ai/josealbertoap/TFM/runs/x160qp4z</a><br/> View project at: <a href='https://wandb.ai/josealbertoap/TFM' target=\"_blank\">https://wandb.ai/josealbertoap/TFM</a><br/>Synced 6 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Find logs at: <code>./wandb/run-20240828_180122-x160qp4z/logs</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
          },
          "metadata": {}
        }
      ],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1724868306060
        }
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "",
      "version": ""
    },
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}