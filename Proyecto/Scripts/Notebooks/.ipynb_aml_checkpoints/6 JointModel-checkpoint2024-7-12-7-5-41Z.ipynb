{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Trabajo Fin de Máster <br/> Diseño de una arquitectura multimodal para descripción textual de pares imagen-audio\n",
        "\n",
        "## Script 6. Entrenamiento del modelo conjunto con inputs de imagen, texto y audio\n",
        "\n",
        "En este notebook, usamos la base de datos que hemos definido en el Script 5 para entrenar un modelo que acepta imágenes, piezas de texto y audios como inputs. Este modelo pretende diferenciar las distintas personas que han participado en la creación de la misma."
      ],
      "metadata": {
        "id": "z3fMe8NQrjgk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 1. Montamos el almacenamiento\n",
        "\n",
        "Damos permiso a Colab para acceder a mi unidad de Drive y nos situamos en la carpeta donde tenemos los scripts y la librería que hemos creado con las clases propias."
      ],
      "metadata": {
        "id": "u5XhgzYSstMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(0)"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "id": "_wERV4ujAEvf",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721316867069,
          "user_tz": -120,
          "elapsed": 4966,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "gather": {
          "logged": 1723394530990
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('..')\n",
        "os.getcwd()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "'/mnt/batch/tasks/shared/LS_root/mounts/clusters/tfm-cpu/code/Users/jose.puche/Scripts'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "id": "20aib2kKsyqP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721316887311,
          "user_tz": -120,
          "elapsed": 20252,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "outputId": "d69fa3d0-832c-4210-9d64-a87d9a6f2afb",
        "gather": {
          "logged": 1723394531421
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 2. Iniciamos sesión para registrar los resultados en wandb\n"
      ],
      "metadata": {
        "id": "b3ym-_u8GQOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "!wandb login 1b8abaacf33b7b5812267384768c22a1eef3c11e"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/azureuser/.netrc\r\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "yC3Z84iGGbTX",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721316937562,
          "user_tz": -120,
          "elapsed": 36699,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "outputId": "1ff09cd9-b5bb-41de-830a-23ec8a983dff",
        "gather": {
          "logged": 1723394538437
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 2. Importación de paquetes\n",
        "\n",
        "Instalamos las librerías necesarias (entre ellas, necesitamos el modelo CLIP, que descargamos directamente desde github), e importamos otras necesarias.\n",
        "\n",
        "También importamos el dataset y el modelo que hemos definido para nuestro problema, y que se encuentran en"
      ],
      "metadata": {
        "id": "OJpJhpNRsvqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, Subset, SubsetRandomSampler, DataLoader\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from tfm_lib.audio_processing import AudioUtil, AudioAugmentation\n",
        "from tfm_lib.datasets import CustomDataset\n",
        "from tfm_lib.modelos import AudioCLIP\n",
        "from tfm_lib.EarlyStopping import EarlyStopping"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/jupyter_env/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n  warnings.warn(\"Can't initialize NVML\")\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "id": "_uyQrqZKz3Pt",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721317037454,
          "user_tz": -120,
          "elapsed": 3959,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "gather": {
          "logged": 1723394553378
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función de pérdida\n",
        "def loss_fn(logits, labels):\n",
        "    \"\"\"\n",
        "    logits: Las salidas del modelo (predicciones) para cada clase.\n",
        "    labels: Las etiquetas verdaderas (números enteros) para cada ejemplo.\n",
        "    \"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()  # Función de pérdida de entropía cruzada\n",
        "    return criterion(logits, labels)\n",
        "\n",
        "# Ejemplo de cómo usar la función de pérdida\n",
        "logits = torch.tensor([[0.8, 0.1, 0.1], [0.2, 0.7, 0.1], [0.3, 0.2, 0.5]])\n",
        "labels = torch.tensor([0, 1, 2])\n",
        "\n",
        "loss = loss_fn(logits, labels)\n",
        "print(\"Pérdida:\", loss.item())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Pérdida: 0.7991690635681152\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzHNQ6XCpXlf",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721317045748,
          "user_tz": -120,
          "elapsed": 29,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "outputId": "abc646c5-1cb7-4d88-a486-93eca6eaca56",
        "gather": {
          "logged": 1723394553644
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 3. Definición de parámetros y configuración"
      ],
      "metadata": {
        "id": "1PAe7ILdyoPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = './../Final_Database'\n",
        "num_epochs = 20\n",
        "BATCH_SIZE = 16\n",
        "data_augmentation = True\n",
        "da = \"_DA\" if data_augmentation else \"\"\n",
        "lr = 1e-4\n",
        "output_dim = 20\n",
        "selected_model = 'RN50'\n",
        "\n",
        "model_parameters_file = f\"./modelos/multimodal/FULL_{selected_model.replace('/','')}_{output_dim}pers_lr{f'{lr:.0e}'}_bs{BATCH_SIZE}_{num_epochs}ep{da}.pt\"\n",
        "print(model_parameters_file)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "./modelos/multimodal/FULL_RN50_20pers_lr1e-04_bs16_20ep_DA.pt\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4en3ajR6MTnW",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721320046475,
          "user_tz": -120,
          "elapsed": 495,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "outputId": "65318f54-e302-49a7-a3cc-fe0daeae10bf",
        "gather": {
          "logged": 1723394553900
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WandB – Initialize a new run\n",
        "run_name = model_parameters_file.split(\"/\")[-1].replace('.pt', '')\n",
        "wandb.init(entity=\"josealbertoap\", project='TFM', name = run_name, tags=[\"multimodal\"])\n",
        "\n",
        "# WandB – Config is a variable that holds and saves hyperparameters and inputs\n",
        "config = wandb.config          # Initialize config\n",
        "config.batch_size = BATCH_SIZE          # input batch size for training (default: 64)\n",
        "config.test_batch_size = BATCH_SIZE    # input batch size for testing (default: 1000)\n",
        "config.epochs = num_epochs             # number of epochs to train (default: 10)\n",
        "config.lr = lr              # learning rate (default: 0.01)\n",
        "config.momentum = 0          # SGD momentum (default: 0.5)\n",
        "config.no_cuda = True         # disables CUDA training\n",
        "config.seed = 0               # random seed (default: 42)\n",
        "config.log_interval = 1     # how many batches to wait before logging training status\n",
        "config.num_classes = output_dim"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjosealbertoap\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "wandb version 0.17.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.17.5"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/mnt/batch/tasks/shared/LS_root/mounts/clusters/tfm-cpu/code/Users/jose.puche/Scripts/wandb/run-20240811_164236-3iwxpz5f</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/josealbertoap/TFM/runs/3iwxpz5f' target=\"_blank\">FULL_RN50_20pers_lr1e-04_bs16_20ep_DA</a></strong> to <a href='https://wandb.ai/josealbertoap/TFM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/josealbertoap/TFM' target=\"_blank\">https://wandb.ai/josealbertoap/TFM</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/josealbertoap/TFM/runs/3iwxpz5f' target=\"_blank\">https://wandb.ai/josealbertoap/TFM/runs/3iwxpz5f</a>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "id": "Z1N6kbdL2kpi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721317355610,
          "user_tz": -120,
          "elapsed": 2541,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "outputId": "05b53740-587e-46d8-aa38-5fedff721557",
        "gather": {
          "logged": 1723394568145
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 4. Definición de modelo y base de datos"
      ],
      "metadata": {
        "id": "8KjZYpM-zoWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import Resize, Compose, ColorJitter, RandomHorizontalFlip, \\\n",
        "                                   RandomResizedCrop, RandomRotation, Normalize, ToTensor\n",
        "\n",
        "def train_test_dataloaders(database_df, model, num_classes, data_augmentation=False, BATCH_SIZE=32, test_split=0.2):\n",
        "\n",
        "    dataset = CustomDataset(database_df, num_classes, image_transform = model.preprocess)\n",
        "\n",
        "    train_idx, test_idx = train_test_split(list(range(len(dataset))), test_size=test_split,\n",
        "                                           stratify=dataset.database_info.classID, random_state=42)\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "\n",
        "    # test_subset = Subset(dataset, test_idx) # En caso de que quisiéramos un Dataset y no un Dataloader\n",
        "    test_sampler = SubsetRandomSampler(test_idx)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=test_sampler)\n",
        "\n",
        "    # En caso de tener data augmentation, cambiamos el dataset para el Dataloader de train\n",
        "    if data_augmentation:\n",
        "\n",
        "      augmentation = Compose([\n",
        "            RandomHorizontalFlip(p=0.3),\n",
        "            RandomRotation(degrees=(0, 45), fill=0),\n",
        "            RandomResizedCrop(size=(224, 224), scale=(0.2, 1.0), ratio=(0.8, 1.2)),\n",
        "            # ColorJitter(brightness=.3, contrast=.1, saturation=.1, hue=.1),\n",
        "            ToTensor(),\n",
        "            Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
        "        ])\n",
        "\n",
        "      dataset = CustomDataset(database_df, num_classes, image_transform = augmentation)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
        "\n",
        "    return train_loader, test_loader, dataset.labelencoder.classes_\n",
        "\n",
        "# Por si hay que meter la data augmentation para los audios\n",
        "# aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "id": "q-bNbcxUz9xm",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721320931417,
          "user_tz": -120,
          "elapsed": 5,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "gather": {
          "logged": 1723394568443
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargamos el modelo pre-entrenado y procesador de CLIP\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "model = AudioCLIP(selected_model, device, output_dim).to(device)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "train_loader, test_loader, classes = train_test_dataloaders(pd.read_csv(f'{folder_path}/finalDB_train.csv'),\n",
        "                                                            model, output_dim, data_augmentation, BATCH_SIZE, 0.2)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Device: cpu\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "id": "yhcgIsqp-HBB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721320941710,
          "user_tz": -120,
          "elapsed": 4570,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "outputId": "bd8cc195-90e5-4b90-b254-bacbf1004ef8",
        "gather": {
          "logged": 1723394574025
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 5. Entrenamiento del modelo"
      ],
      "metadata": {
        "id": "Dsqfl1eWziyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializa el optimizador\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience = 3)\n",
        "\n",
        "train_loss = {}\n",
        "test_loss = {}\n",
        "train_acc = {}\n",
        "test_acc = {}\n",
        "\n",
        "# Creamos la lista de descripciones para evaluar el modelo\n",
        "print(f\"People:{classes}\\n\")\n",
        "eval_descriptions = torch.cat([clip.tokenize(f\"a photo of {c}\") for c in classes])\n",
        "\n",
        "early_stopping = EarlyStopping(patience=5, verbose=True, delta=0.01, path=model_parameters_file)\n",
        "\n",
        "wandb.watch(model, log=\"all\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    train_steps = tqdm(train_loader, unit=\"batch\")\n",
        "\n",
        "    for images, audios, labels in train_steps:\n",
        "\n",
        "        train_steps.set_description(f\"Epoch [{epoch+1}/{num_epochs}]. Training\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        text_desc = eval_descriptions.to(device)\n",
        "        audios = audios.to(device)\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        output = model(images, text_desc, audios)\n",
        "\n",
        "        # Cálculo de la accuracy\n",
        "        predictions = output.argmax(dim=1, keepdim=True).squeeze()\n",
        "        correct = (predictions == labels).sum().item()\n",
        "\n",
        "        total_samples += labels.size(0)\n",
        "        total_correct += correct\n",
        "\n",
        "        # Cálculo de la función de pérdida y actualización del modelo\n",
        "        loss = loss_fn(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        train_steps.set_postfix(mean_loss=epoch_loss/total_samples, mean_accuracy = total_correct / total_samples)\n",
        "\n",
        "    train_loss[epoch+1] = epoch_loss / len(train_loader)\n",
        "    train_acc[epoch+1] = total_correct / total_samples\n",
        "\n",
        "    # Evaluación en el conjunto de prueba\n",
        "    model.eval()  # Cambiamos al modo de evaluación\n",
        "    epoch_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    test_steps = tqdm(test_loader, unit=\"batch\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, audios, labels in test_steps:  # Itera sobre los datos de prueba\n",
        "\n",
        "            test_steps.set_description(f\"Epoch [{epoch+1}/{num_epochs}]. Validation\")\n",
        "\n",
        "            text_desc = eval_descriptions.to(device)\n",
        "            audios = audios.to(device)\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            output = model(images, text_desc, audios)\n",
        "\n",
        "            # Cálculo de la accuracy\n",
        "            predictions = output.argmax(dim=1, keepdim=True).squeeze()\n",
        "            correct = (predictions == labels).sum().item()\n",
        "\n",
        "            total_samples += labels.size(0)\n",
        "            total_correct += correct\n",
        "\n",
        "            # Cálculo de la función de pérdida y actualización del modelo\n",
        "            loss = loss_fn(output, labels)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            test_steps.set_postfix(mean_loss=epoch_loss/total_samples, mean_accuracy = total_correct / total_samples)\n",
        "\n",
        "        test_loss[epoch+1] = epoch_loss / len(test_loader)\n",
        "        test_acc[epoch+1] = total_correct / total_samples\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}]:')\n",
        "        print(f'- Training. Loss = {train_loss[epoch+1]}; Accuracy = {train_acc[epoch+1]}.')\n",
        "        print(f'- Validation. Loss = {test_loss[epoch+1]}; Accuracy = {test_acc[epoch+1]}.')\n",
        "        print()\n",
        "\n",
        "        wandb.log({\n",
        "                        'Epoch': epoch+1,\n",
        "                        'Training Loss': train_loss[epoch+1],\n",
        "                        'Training Accuracy': train_acc[epoch+1],\n",
        "                        'Evaluation Loss': test_loss[epoch+1],\n",
        "                        'Evaluation Accuracy': test_acc[epoch+1],\n",
        "                    })\n",
        "\n",
        "        # Llamar a early_stopping con la pérdida de validación actual y el modelo\n",
        "        early_stopping(test_loss[epoch+1], model)\n",
        "        print('')\n",
        "\n",
        "        # Si se alcanza el criterio de early stopping, romper el bucle\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "        # Reducir el learning rate en caso de que no esté mejorando la pérdida\n",
        "        scheduler.step(test_loss[epoch+1])\n",
        "\n",
        "print({'train_acc': train_acc, 'train_loss': train_loss, 'val_acc': test_acc, 'val_loss': test_loss})\n",
        "\n",
        "wandb.save(model_parameters_file)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "People:['Alba Azorin Zafrilla' 'Alfonso Girona Palao' 'Alfonso Vidal Lopez'\n 'Ana Azorin Puche' 'Ana Puche Palao' 'Angela Espinosa Martinez'\n 'Clara Hidalgo Lopez' 'Cristina Carpena Ortiz' 'David Azorin Soriano'\n 'Diego Molina Puche' 'Eva Jimenez Mariscal'\n 'Francisco Jose Maldonado Montiel' 'Genesis Reyes Arteaga'\n 'Irene Gutierrez Perez' 'Irene Molina Puche' 'Irene Ponte Ibanez'\n 'Iria Alonso Alves' 'Javier Lopez Martinez' 'Jonathan Gonzalez Lopez'\n 'Jose Alberto Azorin Puche']\n\nEpoch [1/20]:\n- Training. Loss = 2.8522090510680127; Accuracy = 0.147005444646098.\n- Validation. Loss = 3.1212102266458364; Accuracy = 0.0893719806763285.\n\nValidation loss decreased (inf --> 3.121210).  Saving model ...\n\nEpoch [2/20]:\n- Training. Loss = 2.3500846211726847; Accuracy = 0.2909860859044162.\n- Validation. Loss = 4.037777277139517; Accuracy = 0.07246376811594203.\n\nEarlyStopping counter: 1 out of 5\n\nEpoch [3/20]:\n- Training. Loss = 2.010368389578966; Accuracy = 0.3877797943133696.\n- Validation. Loss = 3.132055566861079; Accuracy = 0.10869565217391304.\n\nEarlyStopping counter: 2 out of 5\n\nEpoch [4/20]:\n- Training. Loss = 1.4449971851248007; Accuracy = 0.5644283121597096.\n- Validation. Loss = 1.1977131366729736; Accuracy = 0.644927536231884.\n\nValidation loss decreased (3.121210 --> 1.197713).  Saving model ...\n\nEpoch [5/20]:\n- Training. Loss = 0.9191526471135708; Accuracy = 0.7441016333938294.\n- Validation. Loss = 1.0675157033480132; Accuracy = 0.6739130434782609.\n\nValidation loss decreased (1.197713 --> 1.067516).  Saving model ...\n\nEpoch [6/20]:\n- Training. Loss = 0.7335418654749026; Accuracy = 0.7979431336963098.\n- Validation. Loss = 1.107335782968081; Accuracy = 0.7004830917874396.\n\nEarlyStopping counter: 1 out of 5\n\nEpoch [7/20]:\n- Training. Loss = 0.48865711646011245; Accuracy = 0.8802177858439202.\n- Validation. Loss = 0.7062505506552182; Accuracy = 0.7608695652173914.\n\nValidation loss decreased (1.067516 --> 0.706251).  Saving model ...\n\nEpoch [8/20]:\n- Training. Loss = 0.36435540729703814; Accuracy = 0.9092558983666061.\n- Validation. Loss = 0.6294379165539374; Accuracy = 0.8164251207729468.\n\nValidation loss decreased (0.706251 --> 0.629438).  Saving model ...\n\nEpoch [9/20]:\n- Training. Loss = 0.3529909989581658; Accuracy = 0.9134906231094979.\n- Validation. Loss = 0.35645811746899897; Accuracy = 0.9202898550724637.\n\nValidation loss decreased (0.629438 --> 0.356458).  Saving model ...\n\nEpoch [10/20]:\n- Training. Loss = 0.2722889199638023; Accuracy = 0.9310344827586207.\n- Validation. Loss = 0.22442007766893277; Accuracy = 0.9516908212560387.\n\nValidation loss decreased (0.356458 --> 0.224420).  Saving model ...\n\nEpoch [11/20]:\n- Training. Loss = 0.2582805878482759; Accuracy = 0.9382940108892922.\n- Validation. Loss = 0.1570516490878967; Accuracy = 0.9806763285024155.\n\nValidation loss decreased (0.224420 --> 0.157052).  Saving model ...\n\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Epoch [1/20]. Training: 100%|██████████| 104/104 [18:40<00:00, 10.78s/batch, mean_accuracy=0.147, mean_loss=0.179]\nEpoch [1/20]. Validation: 100%|██████████| 26/26 [03:25<00:00,  7.92s/batch, mean_accuracy=0.0894, mean_loss=0.196]\nEpoch [2/20]. Training: 100%|██████████| 104/104 [18:01<00:00, 10.40s/batch, mean_accuracy=0.291, mean_loss=0.148]\nEpoch [2/20]. Validation: 100%|██████████| 26/26 [03:19<00:00,  7.67s/batch, mean_accuracy=0.0725, mean_loss=0.254]\nEpoch [3/20]. Training: 100%|██████████| 104/104 [18:22<00:00, 10.60s/batch, mean_accuracy=0.388, mean_loss=0.126]\nEpoch [3/20]. Validation: 100%|██████████| 26/26 [03:18<00:00,  7.63s/batch, mean_accuracy=0.109, mean_loss=0.197]\nEpoch [4/20]. Training: 100%|██████████| 104/104 [17:48<00:00, 10.27s/batch, mean_accuracy=0.564, mean_loss=0.0909]\nEpoch [4/20]. Validation: 100%|██████████| 26/26 [03:26<00:00,  7.93s/batch, mean_accuracy=0.645, mean_loss=0.0752]\nEpoch [5/20]. Training: 100%|██████████| 104/104 [17:34<00:00, 10.14s/batch, mean_accuracy=0.744, mean_loss=0.0578]\nEpoch [5/20]. Validation: 100%|██████████| 26/26 [03:14<00:00,  7.47s/batch, mean_accuracy=0.674, mean_loss=0.067] \nEpoch [6/20]. Training: 100%|██████████| 104/104 [17:22<00:00, 10.02s/batch, mean_accuracy=0.798, mean_loss=0.0462]\nEpoch [6/20]. Validation: 100%|██████████| 26/26 [03:14<00:00,  7.49s/batch, mean_accuracy=0.7, mean_loss=0.0695]  \nEpoch [7/20]. Training: 100%|██████████| 104/104 [17:16<00:00,  9.96s/batch, mean_accuracy=0.88, mean_loss=0.0307] \nEpoch [7/20]. Validation: 100%|██████████| 26/26 [03:17<00:00,  7.59s/batch, mean_accuracy=0.761, mean_loss=0.0444]\nEpoch [8/20]. Training: 100%|██████████| 104/104 [17:16<00:00,  9.97s/batch, mean_accuracy=0.909, mean_loss=0.0229]\nEpoch [8/20]. Validation: 100%|██████████| 26/26 [03:11<00:00,  7.38s/batch, mean_accuracy=0.816, mean_loss=0.0395]\nEpoch [9/20]. Training: 100%|██████████| 104/104 [17:24<00:00, 10.05s/batch, mean_accuracy=0.913, mean_loss=0.0222]\nEpoch [9/20]. Validation: 100%|██████████| 26/26 [03:13<00:00,  7.46s/batch, mean_accuracy=0.92, mean_loss=0.0224] \nEpoch [10/20]. Training: 100%|██████████| 104/104 [17:28<00:00, 10.08s/batch, mean_accuracy=0.931, mean_loss=0.0171]\nEpoch [10/20]. Validation: 100%|██████████| 26/26 [03:12<00:00,  7.41s/batch, mean_accuracy=0.952, mean_loss=0.0141]\nEpoch [11/20]. Training: 100%|██████████| 104/104 [17:18<00:00,  9.99s/batch, mean_accuracy=0.938, mean_loss=0.0162]\nEpoch [11/20]. Validation: 100%|██████████| 26/26 [03:16<00:00,  7.55s/batch, mean_accuracy=0.981, mean_loss=0.00986]\nEpoch [12/20]. Training:  44%|████▍     | 46/104 [07:48<09:39, 10.00s/batch, mean_accuracy=0.965, mean_loss=0.0105] \rEpoch [12/20]. Training:  80%|███████▉  | 83/104 [13:48<03:28,  9.94s/batch, mean_accuracy=0.956, mean_loss=0.0116]"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 946
        },
        "id": "jL5OGRZhBXR0",
        "executionInfo": {
          "status": "error",
          "timestamp": 1721321030627,
          "user_tz": -120,
          "elapsed": 82805,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "outputId": "644a3909-3a4e-4a31-8178-2d575aab801c",
        "gather": {
          "logged": 1723392396611
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluación del modelo entrenado"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = CustomDataset(pd.read_csv(f'{folder_path}/finalDB_test.csv'), \n",
        "                            output_dim, image_transform = model.preprocess)\n",
        "test_dl = torch.utils.data.DataLoader(test_dataset, batch_size=1048, shuffle=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723392396961
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Inference\n",
        "# ----------------------------\n",
        "def inference (model, test_dl):\n",
        "  correct_prediction = 0\n",
        "  total_prediction = 0\n",
        "\n",
        "  # Disable gradient updates\n",
        "  with torch.no_grad():\n",
        "\n",
        "    predictions = []\n",
        "    label_list = []\n",
        "    for data in test_dl:\n",
        "      # Get the input features and target labels, and put them on the GPU\n",
        "      images, audios, labels = data[0].to(device), data[1].to(device), data[2].to(device)\n",
        "      texts = eval_descriptions.to(device)\n",
        "\n",
        "      # Get predictions\n",
        "      outputs = model(images, texts, audios)\n",
        "\n",
        "      # Get the predicted class with the highest score\n",
        "      _, prediction = torch.max(outputs,1)\n",
        "      # Count of predictions that matched the target label\n",
        "      correct_prediction += (prediction == labels).sum().item()\n",
        "      total_prediction += prediction.shape[0]\n",
        "\n",
        "      predictions.extend(prediction)\n",
        "      label_list.extend(data[2])\n",
        "\n",
        "  acc = correct_prediction/total_prediction\n",
        "  print(f'Accuracy: {acc:.2f}, Total items: {total_prediction}')\n",
        "\n",
        "  return predictions, label_list\n",
        "\n",
        "# Run inference on trained model with the validation set\n",
        "model.load_state_dict(torch.load(model_parameters_file, map_location=torch.device('cpu')))\n",
        "result = inference(model, test_dl)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723392534433
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, f1_score, confusion_matrix\n",
        "import seaborn as sn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "def extraer_iniciales(name):\n",
        "    name_words = name.split(' ')\n",
        "    r = re.compile(\"^[A-Z][A-z]*\")\n",
        "    valid_words = list(filter(r.match, name_words))\n",
        "    if len(valid_words) <=3:\n",
        "        name = valid_words[0]\n",
        "        valid_words.remove(valid_words[0])\n",
        "    else:\n",
        "        name = f'{valid_words[0]} {valid_words[1]}'\n",
        "        valid_words.remove(valid_words[0])\n",
        "        valid_words.remove(valid_words[1])\n",
        "    surname = re.sub('(?<=[A-Z])[A-z]+', '.', ' '.join(valid_words))\n",
        "    return f'{name} {surname}'\n",
        "\n",
        "def font_scale(num_classes):\n",
        "    if num_classes <= 10:\n",
        "        return 1.0\n",
        "    elif num_classes <= 20:\n",
        "        return 0.75\n",
        "    elif num_classes <= 30:\n",
        "        return 0.65\n",
        "    else:\n",
        "        return 0.45\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    people = list(map(extraer_iniciales, test_dataset.labelencoder.classes_))\n",
        "\n",
        "    df_cm = pd.DataFrame((cf_matrix / np.sum(cf_matrix, axis=1)[:, None]).round(3), index=people, columns=people)\n",
        "    \n",
        "    plt.figure(figsize=(8, 6))  \n",
        "    sn.set(font_scale = font_scale(df_cm.shape[0]))  \n",
        "    heatmap = sn.heatmap(df_cm, annot=True, cbar=False, cmap='Purples', fmt='g', xticklabels=False)\n",
        "\n",
        "    # Ajusta la rotación y alineación de los ticks de los ejes\n",
        "    heatmap.set_yticklabels(heatmap.get_yticklabels(), rotation=0, ha='right')\n",
        "\n",
        "    plt.tight_layout()  # Asegura que todo se ajuste bien en la figura\n",
        "    plt.savefig(model_parameters_file.replace('/modelos/', '/results/').replace('.pt', '.png'))\n",
        "\n",
        "    return plt.gcf()\n",
        "\n",
        "def get_metrics(result):\n",
        "    accuracy = accuracy_score(result[1], result[0])\n",
        "    precision = precision_score(result[1], result[0], average='macro')\n",
        "    recall = recall_score(result[1], result[0], average='macro')\n",
        "    f1 = f1_score(result[1], result[0], average='macro')\n",
        "\n",
        "    metrics = {\n",
        "        'Test accuracy': accuracy,\n",
        "        'Test precision': precision,\n",
        "        'Test recall': recall,\n",
        "        'F1-score': f1\n",
        "    }\n",
        "\n",
        "    print(metrics)\n",
        "\n",
        "    metrics['Confusion Matrix'] = wandb.Image(plot_confusion_matrix(result[1],result[0]))\n",
        "    metrics['Test metrics'] = wandb.Table(columns=[\"Metric name\", \"Value\"], \n",
        "                                          data=[[\"Test accuracy\", accuracy], [\"Test precision\", precision],\n",
        "                                                [\"Test recall\", recall], [\"Test F1-Score\", f1]])\n",
        "\n",
        "    return metrics\n",
        "\n",
        "metrics = get_metrics(result)\n",
        "wandb.log(metrics)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723392538432
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "image_results = []\n",
        "\n",
        "audio_file = './../Final_Database/audio/Jose Alberto Azorin Puche/audio0000.ogg'\n",
        "aud = AudioUtil.open(audio_file)\n",
        "aud = AudioUtil.resample(aud, 16000)\n",
        "aud = AudioUtil.rechannel(aud, 1)\n",
        "aud = AudioAugmentation.pad_trunc(aud, 4)\n",
        "sgram_1 = AudioUtil.spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None).unsqueeze(0).to(device)\n",
        "\n",
        "audio_file = './../Final_Database/audio/Jose Alberto Azorin Puche/audio_prueba.ogg'\n",
        "aud = AudioUtil.open(audio_file)\n",
        "aud = AudioUtil.resample(aud, 16000)\n",
        "aud = AudioUtil.rechannel(aud, 1)\n",
        "aud = AudioAugmentation.pad_trunc(aud, 4)\n",
        "sgram_2 = AudioUtil.spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None).unsqueeze(0).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    prueba = False\n",
        "\n",
        "    for sgram in [sgram_1, sgram_2]:\n",
        "        audio_name = 'Prueba' if prueba else 'Original'\n",
        "\n",
        "        for i in range(4):\n",
        "\n",
        "            read_image = Image.open(f'./../Test_images/IMG_000{i}.jpg')\n",
        "            image = model.preprocess(read_image).unsqueeze(0).to(device)\n",
        "\n",
        "            output = model(image, eval_descriptions, sgram)\n",
        "            probs = torch.round(output.softmax(dim=-1), decimals=4)\n",
        "            pred_prob = torch.max(probs).item()\n",
        "            pred_person = classes[torch.argmax(probs)]\n",
        "            my_prob = probs.squeeze()[list(classes).index('Jose Alberto Azorin Puche')].item()\n",
        "\n",
        "            image_results.append([f'Imagen {i+1}', audio_name, pred_person, pred_prob, my_prob])\n",
        "\n",
        "        prueba = True\n",
        "        \n",
        "print(image_results)\n",
        "wandb.log({\"Test images results\": wandb.Table(columns=[\"Imagen\", \"Audio\", \"Persona\", \"Probabilidad\", \"Prob (Joseal)\"], data=image_results)})        \n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723392543405
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "w5374GuwbV2Z",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721319542390,
          "user_tz": -120,
          "elapsed": 4285,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "170b3beef2d44b0b88a34e0b1f4222b3",
            "ff72256106fb48cb9ab1927eed418326",
            "be3df9b759d340279bdcfd3936ecf88e",
            "880e7f5d493b49b0874b42644cb53e94",
            "2f7f50e6edd24047b6ddb1aec2ddc61a",
            "f6656948665744d39b587f1fcf5643a3",
            "e1fbf550fe864c559af5522d12a4b050",
            "69ee02a8a33141ef9f324193fd3a5361"
          ]
        },
        "outputId": "ec3a01a7-ffad-486d-e94f-0d2a9918928b",
        "gather": {
          "logged": 1723392558413
        }
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "u5XhgzYSstMq",
        "OJpJhpNRsvqj",
        "1PAe7ILdyoPt",
        "8KjZYpM-zoWn",
        "Dsqfl1eWziyb"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNdoDSWLx0t06VSUizuGF13"
    },
    "accelerator": "GPU",
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "170b3beef2d44b0b88a34e0b1f4222b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_model_module": "@jupyter-widgets/controls",
            "_view_name": "VBoxView",
            "_view_module": "@jupyter-widgets/controls",
            "_dom_classes": [],
            "layout": "IPY_MODEL_880e7f5d493b49b0874b42644cb53e94",
            "_view_count": null,
            "_model_module_version": "1.5.0",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff72256106fb48cb9ab1927eed418326",
              "IPY_MODEL_be3df9b759d340279bdcfd3936ecf88e"
            ]
          }
        },
        "ff72256106fb48cb9ab1927eed418326": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_name": "LabelModel",
            "_model_module": "@jupyter-widgets/controls",
            "_view_name": "LabelView",
            "_view_module": "@jupyter-widgets/controls",
            "_dom_classes": [],
            "layout": "IPY_MODEL_2f7f50e6edd24047b6ddb1aec2ddc61a",
            "value": "0.017 MB of 0.017 MB uploaded\r",
            "style": "IPY_MODEL_f6656948665744d39b587f1fcf5643a3",
            "placeholder": "​",
            "_view_count": null,
            "_model_module_version": "1.5.0",
            "description": ""
          }
        },
        "be3df9b759d340279bdcfd3936ecf88e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_name": "FloatProgressModel",
            "_model_module": "@jupyter-widgets/controls",
            "max": 1,
            "bar_style": "",
            "_view_name": "ProgressView",
            "_view_module": "@jupyter-widgets/controls",
            "_dom_classes": [],
            "layout": "IPY_MODEL_e1fbf550fe864c559af5522d12a4b050",
            "orientation": "horizontal",
            "value": 1,
            "style": "IPY_MODEL_69ee02a8a33141ef9f324193fd3a5361",
            "min": 0,
            "_view_count": null,
            "_model_module_version": "1.5.0",
            "description": ""
          }
        },
        "880e7f5d493b49b0874b42644cb53e94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "grid_row": null,
            "_model_module": "@jupyter-widgets/base",
            "overflow": null,
            "max_height": null,
            "display": null,
            "grid_auto_flow": null,
            "grid_template_rows": null,
            "align_self": null,
            "grid_auto_columns": null,
            "width": null,
            "grid_area": null,
            "align_items": null,
            "_view_name": "LayoutView",
            "left": null,
            "height": null,
            "_view_module": "@jupyter-widgets/base",
            "object_position": null,
            "justify_content": null,
            "bottom": null,
            "max_width": null,
            "border": null,
            "margin": null,
            "order": null,
            "grid_column": null,
            "grid_auto_rows": null,
            "padding": null,
            "grid_template_columns": null,
            "justify_items": null,
            "object_fit": null,
            "visibility": null,
            "_view_count": null,
            "flex_flow": null,
            "min_height": null,
            "top": null,
            "min_width": null,
            "flex": null,
            "_model_module_version": "1.2.0",
            "grid_template_areas": null,
            "overflow_x": null,
            "right": null,
            "overflow_y": null,
            "grid_gap": null,
            "align_content": null
          }
        },
        "2f7f50e6edd24047b6ddb1aec2ddc61a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "grid_row": null,
            "_model_module": "@jupyter-widgets/base",
            "overflow": null,
            "max_height": null,
            "display": null,
            "grid_auto_flow": null,
            "grid_template_rows": null,
            "align_self": null,
            "grid_auto_columns": null,
            "width": null,
            "grid_area": null,
            "align_items": null,
            "_view_name": "LayoutView",
            "left": null,
            "height": null,
            "_view_module": "@jupyter-widgets/base",
            "object_position": null,
            "justify_content": null,
            "bottom": null,
            "max_width": null,
            "border": null,
            "margin": null,
            "order": null,
            "grid_column": null,
            "grid_auto_rows": null,
            "padding": null,
            "grid_template_columns": null,
            "justify_items": null,
            "object_fit": null,
            "visibility": null,
            "_view_count": null,
            "flex_flow": null,
            "min_height": null,
            "top": null,
            "min_width": null,
            "flex": null,
            "_model_module_version": "1.2.0",
            "grid_template_areas": null,
            "overflow_x": null,
            "right": null,
            "overflow_y": null,
            "grid_gap": null,
            "align_content": null
          }
        },
        "f6656948665744d39b587f1fcf5643a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1fbf550fe864c559af5522d12a4b050": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "grid_row": null,
            "_model_module": "@jupyter-widgets/base",
            "overflow": null,
            "max_height": null,
            "display": null,
            "grid_auto_flow": null,
            "grid_template_rows": null,
            "align_self": null,
            "grid_auto_columns": null,
            "width": null,
            "grid_area": null,
            "align_items": null,
            "_view_name": "LayoutView",
            "left": null,
            "height": null,
            "_view_module": "@jupyter-widgets/base",
            "object_position": null,
            "justify_content": null,
            "bottom": null,
            "max_width": null,
            "border": null,
            "margin": null,
            "order": null,
            "grid_column": null,
            "grid_auto_rows": null,
            "padding": null,
            "grid_template_columns": null,
            "justify_items": null,
            "object_fit": null,
            "visibility": null,
            "_view_count": null,
            "flex_flow": null,
            "min_height": null,
            "top": null,
            "min_width": null,
            "flex": null,
            "_model_module_version": "1.2.0",
            "grid_template_areas": null,
            "overflow_x": null,
            "right": null,
            "overflow_y": null,
            "grid_gap": null,
            "align_content": null
          }
        },
        "69ee02a8a33141ef9f324193fd3a5361": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "ProgressStyleModel",
            "_model_module": "@jupyter-widgets/controls",
            "description_width": "",
            "_view_name": "StyleView",
            "_view_module": "@jupyter-widgets/base",
            "_view_count": null,
            "bar_color": null,
            "_model_module_version": "1.5.0"
          }
        }
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.18",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}