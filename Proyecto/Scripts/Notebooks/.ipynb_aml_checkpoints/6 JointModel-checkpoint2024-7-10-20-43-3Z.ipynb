{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Trabajo Fin de Máster <br/> Diseño de una arquitectura multimodal para descripción textual de pares imagen-audio\n",
        "\n",
        "## Script 6. Entrenamiento del modelo conjunto con inputs de imagen, texto y audio\n",
        "\n",
        "En este notebook, usamos la base de datos que hemos definido en el Script 5 para entrenar un modelo que acepta imágenes, piezas de texto y audios como inputs. Este modelo pretende diferenciar las distintas personas que han participado en la creación de la misma."
      ],
      "metadata": {
        "id": "z3fMe8NQrjgk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 1. Montamos el almacenamiento\n",
        "\n",
        "Damos permiso a Colab para acceder a mi unidad de Drive y nos situamos en la carpeta donde tenemos los scripts y la librería que hemos creado con las clases propias."
      ],
      "metadata": {
        "id": "u5XhgzYSstMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(0)"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "id": "_wERV4ujAEvf",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721316867069,
          "user_tz": -120,
          "elapsed": 4966,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "gather": {
          "logged": 1723238217514
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('..')\n",
        "os.getcwd()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "'/mnt/batch/tasks/shared/LS_root/mounts/clusters/tfm-cpu/code/Users/jose.puche/Scripts'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "id": "20aib2kKsyqP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721316887311,
          "user_tz": -120,
          "elapsed": 20252,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "outputId": "d69fa3d0-832c-4210-9d64-a87d9a6f2afb",
        "gather": {
          "logged": 1723238217900
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 2. Iniciamos sesión para registrar los resultados en wandb\n"
      ],
      "metadata": {
        "id": "b3ym-_u8GQOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "!wandb login 1b8abaacf33b7b5812267384768c22a1eef3c11e"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/azureuser/.netrc\r\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "yC3Z84iGGbTX",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721316937562,
          "user_tz": -120,
          "elapsed": 36699,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "outputId": "1ff09cd9-b5bb-41de-830a-23ec8a983dff",
        "gather": {
          "logged": 1723238223170
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 2. Importación de paquetes\n",
        "\n",
        "Instalamos las librerías necesarias (entre ellas, necesitamos el modelo CLIP, que descargamos directamente desde github), e importamos otras necesarias.\n",
        "\n",
        "También importamos el dataset y el modelo que hemos definido para nuestro problema, y que se encuentran en"
      ],
      "metadata": {
        "id": "OJpJhpNRsvqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, Subset, SubsetRandomSampler, DataLoader\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from tfm_lib.audio_processing import AudioUtil, AudioAugmentation\n",
        "from tfm_lib.datasets import CustomDataset\n",
        "from tfm_lib.modelos import AudioCLIP\n",
        "from tfm_lib.EarlyStopping import EarlyStopping"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "id": "_uyQrqZKz3Pt",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721317037454,
          "user_tz": -120,
          "elapsed": 3959,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "gather": {
          "logged": 1723239523929
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función de pérdida\n",
        "def loss_fn(logits, labels):\n",
        "    \"\"\"\n",
        "    logits: Las salidas del modelo (predicciones) para cada clase.\n",
        "    labels: Las etiquetas verdaderas (números enteros) para cada ejemplo.\n",
        "    \"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()  # Función de pérdida de entropía cruzada\n",
        "    return criterion(logits, labels)\n",
        "\n",
        "# Ejemplo de cómo usar la función de pérdida\n",
        "logits = torch.tensor([[0.8, 0.1, 0.1], [0.2, 0.7, 0.1], [0.3, 0.2, 0.5]])\n",
        "labels = torch.tensor([0, 1, 2])\n",
        "\n",
        "loss = loss_fn(logits, labels)\n",
        "print(\"Pérdida:\", loss.item())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Pérdida: 0.7991690635681152\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzHNQ6XCpXlf",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721317045748,
          "user_tz": -120,
          "elapsed": 29,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "outputId": "abc646c5-1cb7-4d88-a486-93eca6eaca56",
        "gather": {
          "logged": 1723238229647
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 3. Definición de parámetros y configuración"
      ],
      "metadata": {
        "id": "1PAe7ILdyoPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = './../Final_Database'\n",
        "num_epochs = 20\n",
        "BATCH_SIZE = 16\n",
        "data_augmentation = True\n",
        "da = \"_DA\" if data_augmentation else \"\"\n",
        "lr = 1e-4\n",
        "output_dim = 2\n",
        "selected_model = 'RN50'\n",
        "\n",
        "model_parameters_file = f\"./modelos/multimodal/FULL_{selected_model.replace('/','')}_{output_dim}pers_lr{f'{lr:.0e}'}_bs{BATCH_SIZE}_{num_epochs}ep{da}.pt\"\n",
        "print(model_parameters_file)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "./modelos/multimodal/FULL_RN50_2pers_lr1e-04_bs16_20ep_DA.pt\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4en3ajR6MTnW",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721320046475,
          "user_tz": -120,
          "elapsed": 495,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "outputId": "65318f54-e302-49a7-a3cc-fe0daeae10bf",
        "gather": {
          "logged": 1723238230080
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WandB – Initialize a new run\n",
        "run_name = model_parameters_file.split(\"/\")[-1].replace('.pt', '')\n",
        "wandb.init(entity=\"josealbertoap\", project='TFM', name = run_name, tags=[\"multimodal\"])\n",
        "\n",
        "# WandB – Config is a variable that holds and saves hyperparameters and inputs\n",
        "config = wandb.config          # Initialize config\n",
        "config.batch_size = BATCH_SIZE          # input batch size for training (default: 64)\n",
        "config.test_batch_size = BATCH_SIZE    # input batch size for testing (default: 1000)\n",
        "config.epochs = num_epochs             # number of epochs to train (default: 10)\n",
        "config.lr = lr              # learning rate (default: 0.01)\n",
        "config.momentum = 0          # SGD momentum (default: 0.5)\n",
        "config.no_cuda = True         # disables CUDA training\n",
        "config.seed = 0               # random seed (default: 42)\n",
        "config.log_interval = 1     # how many batches to wait before logging training status\n",
        "config.num_classes = output_dim"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjosealbertoap\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "wandb version 0.17.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.17.5"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/mnt/batch/tasks/shared/LS_root/mounts/clusters/tfm-cpu/code/Users/jose.puche/Scripts/wandb/run-20240809_211712-lbqu4b42</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/josealbertoap/TFM/runs/lbqu4b42' target=\"_blank\">FULL_RN50_2pers_lr1e-04_bs16_20ep_DA</a></strong> to <a href='https://wandb.ai/josealbertoap/TFM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/josealbertoap/TFM' target=\"_blank\">https://wandb.ai/josealbertoap/TFM</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/josealbertoap/TFM/runs/lbqu4b42' target=\"_blank\">https://wandb.ai/josealbertoap/TFM/runs/lbqu4b42</a>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "id": "Z1N6kbdL2kpi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721317355610,
          "user_tz": -120,
          "elapsed": 2541,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "outputId": "05b53740-587e-46d8-aa38-5fedff721557",
        "gather": {
          "logged": 1723238244763
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 4. Definición de modelo y base de datos"
      ],
      "metadata": {
        "id": "8KjZYpM-zoWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import Resize, Compose, ColorJitter, RandomHorizontalFlip, \\\n",
        "                                   RandomResizedCrop, RandomRotation, Normalize, ToTensor\n",
        "\n",
        "def train_test_dataloaders(database_df, model, num_classes, data_augmentation=False, BATCH_SIZE=32, test_split=0.2):\n",
        "\n",
        "    dataset = CustomDataset(database_df, num_classes, image_transform = model.preprocess)\n",
        "\n",
        "    train_idx, test_idx = train_test_split(list(range(len(dataset))), test_size=test_split,\n",
        "                                           stratify=dataset.database_info.classID, random_state=42)\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "\n",
        "    # test_subset = Subset(dataset, test_idx) # En caso de que quisiéramos un Dataset y no un Dataloader\n",
        "    test_sampler = SubsetRandomSampler(test_idx)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=test_sampler)\n",
        "\n",
        "    # En caso de tener data augmentation, cambiamos el dataset para el Dataloader de train\n",
        "    if data_augmentation:\n",
        "\n",
        "      augmentation = Compose([\n",
        "            RandomHorizontalFlip(p=0.3),\n",
        "            RandomRotation(degrees=(0, 45), fill=0),\n",
        "            RandomResizedCrop(size=(224, 224), scale=(0.2, 1.0), ratio=(0.8, 1.2)),\n",
        "            # ColorJitter(brightness=.3, contrast=.1, saturation=.1, hue=.1),\n",
        "            ToTensor(),\n",
        "            Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
        "        ])\n",
        "\n",
        "      dataset = CustomDataset(database_df, num_classes, image_transform = augmentation)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
        "\n",
        "    return train_loader, test_loader, dataset.labelencoder.classes_\n",
        "\n",
        "# Por si hay que meter la data augmentation para los audios\n",
        "# aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "id": "q-bNbcxUz9xm",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721320931417,
          "user_tz": -120,
          "elapsed": 5,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "gather": {
          "logged": 1723238245225
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargamos el modelo pre-entrenado y procesador de CLIP\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "model = AudioCLIP(selected_model, device, output_dim).to(device)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "train_loader, test_loader, classes = train_test_dataloaders(pd.read_csv(f'{folder_path}/finalDB_train.csv'),\n",
        "                                                            model, output_dim, data_augmentation, BATCH_SIZE, 0.2)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Device: cpu\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "id": "yhcgIsqp-HBB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721320941710,
          "user_tz": -120,
          "elapsed": 4570,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "outputId": "bd8cc195-90e5-4b90-b254-bacbf1004ef8",
        "gather": {
          "logged": 1723238248513
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 5. Entrenamiento del modelo"
      ],
      "metadata": {
        "id": "Dsqfl1eWziyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializa el optimizador\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience = 3)\n",
        "\n",
        "train_loss = {}\n",
        "test_loss = {}\n",
        "train_acc = {}\n",
        "test_acc = {}\n",
        "\n",
        "# Creamos la lista de descripciones para evaluar el modelo\n",
        "print(f\"People:{classes}\\n\")\n",
        "eval_descriptions = torch.cat([clip.tokenize(f\"a photo of {c}\") for c in classes])\n",
        "\n",
        "early_stopping = EarlyStopping(patience=5, verbose=True, delta=0.01, path=model_parameters_file)\n",
        "\n",
        "wandb.watch(model, log=\"all\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    train_steps = tqdm(train_loader, unit=\"batch\")\n",
        "\n",
        "    for images, audios, labels in train_steps:\n",
        "\n",
        "        train_steps.set_description(f\"Epoch [{epoch+1}/{num_epochs}]. Training\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        text_desc = eval_descriptions.to(device)\n",
        "        audios = audios.to(device)\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        output = model(images, text_desc, audios)\n",
        "\n",
        "        # Cálculo de la accuracy\n",
        "        predictions = output.argmax(dim=1, keepdim=True).squeeze()\n",
        "        correct = (predictions == labels).sum().item()\n",
        "\n",
        "        total_samples += labels.size(0)\n",
        "        total_correct += correct\n",
        "\n",
        "        # Cálculo de la función de pérdida y actualización del modelo\n",
        "        loss = loss_fn(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        train_steps.set_postfix(mean_loss=epoch_loss/total_samples, mean_accuracy = total_correct / total_samples)\n",
        "\n",
        "    train_loss[epoch+1] = epoch_loss / len(train_loader)\n",
        "    train_acc[epoch+1] = total_correct / total_samples\n",
        "\n",
        "    # Evaluación en el conjunto de prueba\n",
        "    model.eval()  # Cambiamos al modo de evaluación\n",
        "    epoch_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    test_steps = tqdm(test_loader, unit=\"batch\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, audios, labels in test_steps:  # Itera sobre los datos de prueba\n",
        "\n",
        "            test_steps.set_description(f\"Epoch [{epoch+1}/{num_epochs}]. Validation\")\n",
        "\n",
        "            text_desc = eval_descriptions.to(device)\n",
        "            audios = audios.to(device)\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            output = model(images, text_desc, audios)\n",
        "\n",
        "            # Cálculo de la accuracy\n",
        "            predictions = output.argmax(dim=1, keepdim=True).squeeze()\n",
        "            correct = (predictions == labels).sum().item()\n",
        "\n",
        "            total_samples += labels.size(0)\n",
        "            total_correct += correct\n",
        "\n",
        "            # Cálculo de la función de pérdida y actualización del modelo\n",
        "            loss = loss_fn(output, labels)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            test_steps.set_postfix(mean_loss=epoch_loss/total_samples, mean_accuracy = total_correct / total_samples)\n",
        "\n",
        "        test_loss[epoch+1] = epoch_loss / len(test_loader)\n",
        "        test_acc[epoch+1] = total_correct / total_samples\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}]:')\n",
        "        print(f'- Training. Loss = {train_loss[epoch+1]}; Accuracy = {train_acc[epoch+1]}.')\n",
        "        print(f'- Validation. Loss = {test_loss[epoch+1]}; Accuracy = {test_acc[epoch+1]}.')\n",
        "        print()\n",
        "\n",
        "        wandb.log({\n",
        "                        'Epoch': epoch+1,\n",
        "                        'Training Loss': train_loss[epoch+1],\n",
        "                        'Training Accuracy': train_acc[epoch+1],\n",
        "                        'Validation Loss': test_loss[epoch+1],\n",
        "                        'Validation Accuracy': test_acc[epoch+1],\n",
        "                    })\n",
        "\n",
        "        # Llamar a early_stopping con la pérdida de validación actual y el modelo\n",
        "        early_stopping(test_loss[epoch+1], model)\n",
        "        print('')\n",
        "\n",
        "        # Si se alcanza el criterio de early stopping, romper el bucle\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "        # Reducir el learning rate en caso de que no esté mejorando la pérdida\n",
        "        scheduler.step(test_loss[epoch+1])\n",
        "\n",
        "print({'train_acc': train_acc, 'train_loss': train_loss, 'val_acc': test_acc, 'val_loss': test_loss})\n",
        "\n",
        "wandb.save(model_parameters_file)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "People:['Alba Azorin Zafrilla' 'Jose Alberto Azorin Puche']\n\nEpoch [1/20]:\n- Training. Loss = 0.6906289133158597; Accuracy = 0.5263157894736842.\n- Evaluation. Loss = 0.7260085542996725; Accuracy = 0.5116279069767442.\n\nValidation loss decreased (inf --> 0.726009).  Saving model ...\n\nEpoch [2/20]:\n- Training. Loss = 0.6538020751693032; Accuracy = 0.6666666666666666.\n- Evaluation. Loss = 0.670283834139506; Accuracy = 0.5581395348837209.\n\nValidation loss decreased (0.726009 --> 0.670284).  Saving model ...\n\nEpoch [3/20]:\n- Training. Loss = 0.582186368378726; Accuracy = 0.8128654970760234.\n- Evaluation. Loss = 0.5342688957850138; Accuracy = 0.9767441860465116.\n\nValidation loss decreased (0.670284 --> 0.534269).  Saving model ...\n\nEpoch [4/20]:\n- Training. Loss = 0.5028264766389673; Accuracy = 0.9590643274853801.\n- Evaluation. Loss = 0.48847247163454693; Accuracy = 1.0.\n\nValidation loss decreased (0.534269 --> 0.488472).  Saving model ...\n\nEpoch [5/20]:\n- Training. Loss = 0.38464538888497785; Accuracy = 0.9941520467836257.\n- Evaluation. Loss = 0.45555060108502704; Accuracy = 0.9069767441860465.\n\nValidation loss decreased (0.488472 --> 0.455551).  Saving model ...\n\nEpoch [6/20]:\n- Training. Loss = 0.32751660319891845; Accuracy = 0.9941520467836257.\n- Evaluation. Loss = 0.35227083166440326; Accuracy = 1.0.\n\nValidation loss decreased (0.455551 --> 0.352271).  Saving model ...\n\nEpoch [7/20]:\n- Training. Loss = 0.27345685931769287; Accuracy = 0.9883040935672515.\n- Evaluation. Loss = 0.3082507054011027; Accuracy = 1.0.\n\nValidation loss decreased (0.352271 --> 0.308251).  Saving model ...\n\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Epoch [1/20]. Training: 100%|██████████| 11/11 [01:52<00:00, 10.22s/batch, mean_accuracy=0.526, mean_loss=0.0444]\nEpoch [1/20]. Evaluation: 100%|██████████| 3/3 [00:24<00:00,  8.07s/batch, mean_accuracy=0.512, mean_loss=0.0507]\nEpoch [2/20]. Training: 100%|██████████| 11/11 [01:42<00:00,  9.28s/batch, mean_accuracy=0.667, mean_loss=0.0421]\nEpoch [2/20]. Evaluation: 100%|██████████| 3/3 [00:17<00:00,  5.73s/batch, mean_accuracy=0.558, mean_loss=0.0468]\nEpoch [3/20]. Training: 100%|██████████| 11/11 [01:28<00:00,  8.00s/batch, mean_accuracy=0.813, mean_loss=0.0375]\nEpoch [3/20]. Evaluation: 100%|██████████| 3/3 [00:17<00:00,  5.79s/batch, mean_accuracy=0.977, mean_loss=0.0373]\nEpoch [4/20]. Training: 100%|██████████| 11/11 [01:27<00:00,  7.93s/batch, mean_accuracy=0.959, mean_loss=0.0323]\nEpoch [4/20]. Evaluation: 100%|██████████| 3/3 [00:16<00:00,  5.57s/batch, mean_accuracy=1, mean_loss=0.0341]\nEpoch [5/20]. Training: 100%|██████████| 11/11 [01:28<00:00,  8.02s/batch, mean_accuracy=0.994, mean_loss=0.0247]\nEpoch [5/20]. Evaluation: 100%|██████████| 3/3 [00:17<00:00,  5.75s/batch, mean_accuracy=0.907, mean_loss=0.0318]\nEpoch [6/20]. Training: 100%|██████████| 11/11 [01:27<00:00,  7.95s/batch, mean_accuracy=0.994, mean_loss=0.0211]\nEpoch [6/20]. Evaluation: 100%|██████████| 3/3 [00:17<00:00,  5.86s/batch, mean_accuracy=1, mean_loss=0.0246]\nEpoch [7/20]. Training: 100%|██████████| 11/11 [01:29<00:00,  8.10s/batch, mean_accuracy=0.988, mean_loss=0.0176]\nEpoch [7/20]. Evaluation: 100%|██████████| 3/3 [00:16<00:00,  5.65s/batch, mean_accuracy=1, mean_loss=0.0215]\nEpoch [8/20]. Training:  64%|██████▎   | 7/11 [01:04<00:36,  9.24s/batch, mean_accuracy=0.982, mean_loss=0.015] \n"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     36\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 38\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_desc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudios\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Cálculo de la accuracy\u001b[39;00m\n\u001b[1;32m     41\u001b[0m predictions \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39msqueeze()\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1582\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1580\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1582\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1584\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1585\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1586\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1587\u001b[0m     ):\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
            "File \u001b[0;32m/mnt/batch/tasks/shared/LS_root/mounts/clusters/tfm-cpu/code/Users/jose.puche/Scripts/tfm_lib/modelos.py:106\u001b[0m, in \u001b[0;36mAudioCLIP.forward\u001b[0;34m(self, x1, x2, y1)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x1, x2, y1):\n\u001b[1;32m    104\u001b[0m \n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# Primero, obtenemos el output del modelo CLIP\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhstack((x[\u001b[38;5;241m0\u001b[39m], x[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mT))\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    108\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.9/site-packages/clip/model.py:359\u001b[0m, in \u001b[0;36mCLIP.forward\u001b[0;34m(self, image, text)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, text):\n\u001b[0;32m--> 359\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m     text_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_text(text)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;66;03m# normalized features\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.9/site-packages/clip/model.py:341\u001b[0m, in \u001b[0;36mCLIP.encode_image\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, image):\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.9/site-packages/clip/model.py:148\u001b[0m, in \u001b[0;36mModifiedResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    146\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    147\u001b[0m x \u001b[38;5;241m=\u001b[39m stem(x)\n\u001b[0;32m--> 148\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[1;32m    150\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.9/site-packages/clip/model.py:48\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(out)))\n\u001b[1;32m     47\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(out)\n\u001b[0;32m---> 48\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn3\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     identity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py:175\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    168\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/jupyter_env/lib/python3.9/site-packages/torch/nn/functional.py:2509\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2507\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2510\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2511\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 946
        },
        "id": "jL5OGRZhBXR0",
        "executionInfo": {
          "status": "error",
          "timestamp": 1721321030627,
          "user_tz": -120,
          "elapsed": 82805,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "outputId": "644a3909-3a4e-4a31-8178-2d575aab801c",
        "gather": {
          "logged": 1723239127254
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluación del modelo entrenado"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = CustomDataset(pd.read_csv(f'{folder_path}/finalDB_test.csv'), \n",
        "                            output_dim, image_transform = model.preprocess)\n",
        "test_dl = torch.utils.data.DataLoader(test_dataset, batch_size=1048, shuffle=True)"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723239155021
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Inference\n",
        "# ----------------------------\n",
        "def inference (model, test_dl):\n",
        "  correct_prediction = 0\n",
        "  total_prediction = 0\n",
        "\n",
        "  # Disable gradient updates\n",
        "  with torch.no_grad():\n",
        "\n",
        "    predictions = []\n",
        "    label_list = []\n",
        "    for data in test_dl:\n",
        "      # Get the input features and target labels, and put them on the GPU\n",
        "      images, audios, labels = data[0].to(device), data[1].to(device), data[2].to(device)\n",
        "      texts = eval_descriptions.to(device)\n",
        "\n",
        "      # Get predictions\n",
        "      outputs = model(images, texts, audios)\n",
        "\n",
        "      # Get the predicted class with the highest score\n",
        "      _, prediction = torch.max(outputs,1)\n",
        "      # Count of predictions that matched the target label\n",
        "      correct_prediction += (prediction == labels).sum().item()\n",
        "      total_prediction += prediction.shape[0]\n",
        "\n",
        "      predictions.extend(prediction)\n",
        "      label_list.extend(data[2])\n",
        "\n",
        "  acc = correct_prediction/total_prediction\n",
        "  print(f'Accuracy: {acc:.2f}, Total items: {total_prediction}')\n",
        "\n",
        "  return predictions, label_list\n",
        "\n",
        "# Run inference on trained model with the validation set\n",
        "model.load_state_dict(torch.load(model_parameters_file, map_location=torch.device('cpu')))\n",
        "result = inference(model, test_dl)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Accuracy: 1.00, Total items: 48\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723239296768
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, f1_score, confusion_matrix\n",
        "import seaborn as sn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "def extraer_iniciales(name):\n",
        "    name_words = name.split(' ')\n",
        "    r = re.compile(\"^[A-Z][A-z]*\")\n",
        "    valid_words = list(filter(r.match, name_words))\n",
        "    if len(valid_words) <=3:\n",
        "        name = valid_words[0]\n",
        "        valid_words.remove(valid_words[0])\n",
        "    else:\n",
        "        name = f'{valid_words[0]} {valid_words[1]}'\n",
        "        valid_words.remove(valid_words[0])\n",
        "        valid_words.remove(valid_words[1])\n",
        "    surname = re.sub('(?<=[A-Z])[A-z]+', '.', ' '.join(valid_words))\n",
        "    return f'{name} {surname}'\n",
        "\n",
        "def font_scale(num_classes):\n",
        "    if num_classes <= 10:\n",
        "        return 1.0\n",
        "    elif num_classes <= 20:\n",
        "        return 0.75\n",
        "    elif num_classes <= 30:\n",
        "        return 0.65\n",
        "    else:\n",
        "        return 0.45\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    people = list(map(extraer_iniciales, test_dataset.labelencoder.classes_))\n",
        "\n",
        "    df_cm = pd.DataFrame((cf_matrix / np.sum(cf_matrix, axis=1)[:, None]).round(3), index=people, columns=people)\n",
        "    \n",
        "    plt.figure(figsize=(8, 6))  \n",
        "    sn.set(font_scale = font_scale(df_cm.shape[0]))  \n",
        "    heatmap = sn.heatmap(df_cm, annot=True, cbar=False, cmap='Purples', fmt='g', xticklabels=False)\n",
        "\n",
        "    # Ajusta la rotación y alineación de los ticks de los ejes\n",
        "    heatmap.set_yticklabels(heatmap.get_yticklabels(), rotation=0, ha='right')\n",
        "\n",
        "    plt.tight_layout()  # Asegura que todo se ajuste bien en la figura\n",
        "    plt.savefig(model_parameters_file.replace('/modelos/', '/results/').replace('.pt', '.png'))\n",
        "\n",
        "    return plt.gcf()\n",
        "\n",
        "def get_metrics(result):\n",
        "    accuracy = accuracy_score(result[1], result[0])\n",
        "    precision = precision_score(result[1], result[0], average='macro')\n",
        "    recall = recall_score(result[1], result[0], average='macro')\n",
        "    f1 = f1_score(result[1], result[0], average='macro')\n",
        "\n",
        "    metrics = {\n",
        "        'Test accuracy': accuracy,\n",
        "        'Test precision': precision,\n",
        "        'Test recall': recall,\n",
        "        'F1-score': f1\n",
        "    }\n",
        "\n",
        "    print(metrics)\n",
        "\n",
        "    metrics['Confusion Matrix'] = wandb.Image(plot_confusion_matrix(result[1],result[0]))\n",
        "    metrics['Test metrics'] = wandb.Table(columns=[\"Metric name\", \"Value\"], \n",
        "                                          data=[[\"Test accuracy\", accuracy], [\"Test precision\", precision],\n",
        "                                                [\"Test recall\", recall], [\"Test F1-Score\", f1]])\n",
        "\n",
        "    return metrics\n",
        "\n",
        "metrics = get_metrics(result)\n",
        "wandb.log(metrics)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{'Test accuracy': 1.0, 'Test precision': 1.0, 'Test recall': 1.0, 'F1-score': 1.0}\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 800x600 with 1 Axes>",
            "image/png": "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"
          },
          "metadata": {}
        }
      ],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723239306865
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_descriptions.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 29,
          "data": {
            "text/plain": "torch.Size([2, 77])"
          },
          "metadata": {}
        }
      ],
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723240064759
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "image_results = []\n",
        "\n",
        "audio_file = './../Final_Database/audio/Jose Alberto Azorin Puche/audio0000.ogg'\n",
        "aud = AudioUtil.open(audio_file)\n",
        "aud = AudioUtil.resample(aud, 16000)\n",
        "aud = AudioUtil.rechannel(aud, 1)\n",
        "aud = AudioAugmentation.pad_trunc(aud, 4)\n",
        "sgram_1 = AudioUtil.spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None).unsqueeze(0).to(device)\n",
        "\n",
        "audio_file = './../Final_Database/audio/Jose Alberto Azorin Puche/audio_prueba.ogg'\n",
        "aud = AudioUtil.open(audio_file)\n",
        "aud = AudioUtil.resample(aud, 16000)\n",
        "aud = AudioUtil.rechannel(aud, 1)\n",
        "aud = AudioAugmentation.pad_trunc(aud, 4)\n",
        "sgram_2 = AudioUtil.spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None).unsqueeze(0).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    prueba = False\n",
        "\n",
        "    for sgram in [sgram_1, sgram_2]:\n",
        "        audio_name = 'Prueba' if prueba else 'Original'\n",
        "\n",
        "        for i in range(4):\n",
        "\n",
        "            read_image = Image.open(f'./../Test_images/IMG_000{i}.jpg')\n",
        "            image = model.preprocess(read_image).unsqueeze(0).to(device)\n",
        "\n",
        "            output = model(image, eval_descriptions, sgram)\n",
        "            probs = torch.round(output.softmax(dim=-1), decimals=4)\n",
        "            pred_prob = torch.max(probs).item()\n",
        "            pred_person = classes[torch.argmax(probs)]\n",
        "            my_prob = probs.squeeze()[list(classes).index('Jose Alberto Azorin Puche')].item()\n",
        "\n",
        "            image_results.append([f'Imagen {i+1}', audio_name, pred_person, pred_prob, my_prob])\n",
        "\n",
        "        prueba = True\n",
        "        \n",
        "print(image_results)\n",
        "wandb.log({\"Test images results\": wandb.Table(columns=[\"Imagen\", \"Audio\", \"Persona\", \"Probabilidad\", \"Prob (Joseal)\"], data=image_results)})        \n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[['Imagen 1', 'Alba Azorin Zafrilla', 0.6987000107765198, 0.3012999892234802], ['Imagen 2', 'Alba Azorin Zafrilla', 0.6215000152587891, 0.3785000145435333], ['Imagen 3', 'Alba Azorin Zafrilla', 0.6276999711990356, 0.37229999899864197], ['Imagen 4', 'Alba Azorin Zafrilla', 0.5371000170707703, 0.4629000127315521]]\n"
        }
      ],
      "execution_count": 51,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723240415471
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(Label(value='0.012 MB of 0.012 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf9348dc878a4b9b8b9ebf9d8d76a954"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▃▅▆▇█</td></tr><tr><td>Evaluation Accuracy</td><td>▁▂██▇██</td></tr><tr><td>Evaluation Loss</td><td>█▇▅▄▃▂▁</td></tr><tr><td>F1-score</td><td>▁</td></tr><tr><td>Test accuracy</td><td>▁</td></tr><tr><td>Test precision</td><td>▁</td></tr><tr><td>Test recall</td><td>▁</td></tr><tr><td>Training Accuracy</td><td>▁▃▅▇███</td></tr><tr><td>Training Loss</td><td>█▇▆▅▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>7</td></tr><tr><td>Evaluation Accuracy</td><td>1.0</td></tr><tr><td>Evaluation Loss</td><td>0.30825</td></tr><tr><td>F1-score</td><td>1.0</td></tr><tr><td>Test accuracy</td><td>1.0</td></tr><tr><td>Test precision</td><td>1.0</td></tr><tr><td>Test recall</td><td>1.0</td></tr><tr><td>Training Accuracy</td><td>0.9883</td></tr><tr><td>Training Loss</td><td>0.27346</td></tr></table><br/></div></div>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run <strong style=\"color:#cdcd00\">FULL_RN50_2pers_lr1e-04_bs16_20ep_DA</strong> at: <a href='https://wandb.ai/josealbertoap/TFM/runs/lbqu4b42' target=\"_blank\">https://wandb.ai/josealbertoap/TFM/runs/lbqu4b42</a><br/> View project at: <a href='https://wandb.ai/josealbertoap/TFM' target=\"_blank\">https://wandb.ai/josealbertoap/TFM</a><br/>Synced 6 W&B file(s), 3 media file(s), 2 artifact file(s) and 0 other file(s)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Find logs at: <code>./wandb/run-20240809_211712-lbqu4b42/logs</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
          },
          "metadata": {}
        }
      ],
      "execution_count": 52,
      "metadata": {
        "id": "w5374GuwbV2Z",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721319542390,
          "user_tz": -120,
          "elapsed": 4285,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "170b3beef2d44b0b88a34e0b1f4222b3",
            "ff72256106fb48cb9ab1927eed418326",
            "be3df9b759d340279bdcfd3936ecf88e",
            "880e7f5d493b49b0874b42644cb53e94",
            "2f7f50e6edd24047b6ddb1aec2ddc61a",
            "f6656948665744d39b587f1fcf5643a3",
            "e1fbf550fe864c559af5522d12a4b050",
            "69ee02a8a33141ef9f324193fd3a5361"
          ]
        },
        "outputId": "ec3a01a7-ffad-486d-e94f-0d2a9918928b",
        "gather": {
          "logged": 1723240469320
        }
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "u5XhgzYSstMq",
        "OJpJhpNRsvqj",
        "1PAe7ILdyoPt",
        "8KjZYpM-zoWn",
        "Dsqfl1eWziyb"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNdoDSWLx0t06VSUizuGF13"
    },
    "accelerator": "GPU",
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "170b3beef2d44b0b88a34e0b1f4222b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_model_module": "@jupyter-widgets/controls",
            "_view_name": "VBoxView",
            "_view_module": "@jupyter-widgets/controls",
            "_dom_classes": [],
            "layout": "IPY_MODEL_880e7f5d493b49b0874b42644cb53e94",
            "_view_count": null,
            "_model_module_version": "1.5.0",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff72256106fb48cb9ab1927eed418326",
              "IPY_MODEL_be3df9b759d340279bdcfd3936ecf88e"
            ]
          }
        },
        "ff72256106fb48cb9ab1927eed418326": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_name": "LabelModel",
            "_model_module": "@jupyter-widgets/controls",
            "_view_name": "LabelView",
            "_view_module": "@jupyter-widgets/controls",
            "_dom_classes": [],
            "layout": "IPY_MODEL_2f7f50e6edd24047b6ddb1aec2ddc61a",
            "value": "0.017 MB of 0.017 MB uploaded\r",
            "style": "IPY_MODEL_f6656948665744d39b587f1fcf5643a3",
            "placeholder": "​",
            "_view_count": null,
            "_model_module_version": "1.5.0",
            "description": ""
          }
        },
        "be3df9b759d340279bdcfd3936ecf88e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_name": "FloatProgressModel",
            "_model_module": "@jupyter-widgets/controls",
            "max": 1,
            "bar_style": "",
            "_view_name": "ProgressView",
            "_view_module": "@jupyter-widgets/controls",
            "_dom_classes": [],
            "layout": "IPY_MODEL_e1fbf550fe864c559af5522d12a4b050",
            "orientation": "horizontal",
            "value": 1,
            "style": "IPY_MODEL_69ee02a8a33141ef9f324193fd3a5361",
            "min": 0,
            "_view_count": null,
            "_model_module_version": "1.5.0",
            "description": ""
          }
        },
        "880e7f5d493b49b0874b42644cb53e94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "grid_row": null,
            "_model_module": "@jupyter-widgets/base",
            "overflow": null,
            "max_height": null,
            "display": null,
            "grid_auto_flow": null,
            "grid_template_rows": null,
            "align_self": null,
            "grid_auto_columns": null,
            "width": null,
            "grid_area": null,
            "align_items": null,
            "_view_name": "LayoutView",
            "left": null,
            "height": null,
            "_view_module": "@jupyter-widgets/base",
            "object_position": null,
            "justify_content": null,
            "bottom": null,
            "max_width": null,
            "border": null,
            "margin": null,
            "order": null,
            "grid_column": null,
            "grid_auto_rows": null,
            "padding": null,
            "grid_template_columns": null,
            "justify_items": null,
            "object_fit": null,
            "visibility": null,
            "_view_count": null,
            "flex_flow": null,
            "min_height": null,
            "top": null,
            "min_width": null,
            "flex": null,
            "_model_module_version": "1.2.0",
            "grid_template_areas": null,
            "overflow_x": null,
            "right": null,
            "overflow_y": null,
            "grid_gap": null,
            "align_content": null
          }
        },
        "2f7f50e6edd24047b6ddb1aec2ddc61a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "grid_row": null,
            "_model_module": "@jupyter-widgets/base",
            "overflow": null,
            "max_height": null,
            "display": null,
            "grid_auto_flow": null,
            "grid_template_rows": null,
            "align_self": null,
            "grid_auto_columns": null,
            "width": null,
            "grid_area": null,
            "align_items": null,
            "_view_name": "LayoutView",
            "left": null,
            "height": null,
            "_view_module": "@jupyter-widgets/base",
            "object_position": null,
            "justify_content": null,
            "bottom": null,
            "max_width": null,
            "border": null,
            "margin": null,
            "order": null,
            "grid_column": null,
            "grid_auto_rows": null,
            "padding": null,
            "grid_template_columns": null,
            "justify_items": null,
            "object_fit": null,
            "visibility": null,
            "_view_count": null,
            "flex_flow": null,
            "min_height": null,
            "top": null,
            "min_width": null,
            "flex": null,
            "_model_module_version": "1.2.0",
            "grid_template_areas": null,
            "overflow_x": null,
            "right": null,
            "overflow_y": null,
            "grid_gap": null,
            "align_content": null
          }
        },
        "f6656948665744d39b587f1fcf5643a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1fbf550fe864c559af5522d12a4b050": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "grid_row": null,
            "_model_module": "@jupyter-widgets/base",
            "overflow": null,
            "max_height": null,
            "display": null,
            "grid_auto_flow": null,
            "grid_template_rows": null,
            "align_self": null,
            "grid_auto_columns": null,
            "width": null,
            "grid_area": null,
            "align_items": null,
            "_view_name": "LayoutView",
            "left": null,
            "height": null,
            "_view_module": "@jupyter-widgets/base",
            "object_position": null,
            "justify_content": null,
            "bottom": null,
            "max_width": null,
            "border": null,
            "margin": null,
            "order": null,
            "grid_column": null,
            "grid_auto_rows": null,
            "padding": null,
            "grid_template_columns": null,
            "justify_items": null,
            "object_fit": null,
            "visibility": null,
            "_view_count": null,
            "flex_flow": null,
            "min_height": null,
            "top": null,
            "min_width": null,
            "flex": null,
            "_model_module_version": "1.2.0",
            "grid_template_areas": null,
            "overflow_x": null,
            "right": null,
            "overflow_y": null,
            "grid_gap": null,
            "align_content": null
          }
        },
        "69ee02a8a33141ef9f324193fd3a5361": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "ProgressStyleModel",
            "_model_module": "@jupyter-widgets/controls",
            "description_width": "",
            "_view_name": "StyleView",
            "_view_module": "@jupyter-widgets/base",
            "_view_count": null,
            "bar_color": null,
            "_model_module_version": "1.5.0"
          }
        }
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.18",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}