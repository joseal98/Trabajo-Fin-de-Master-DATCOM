{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Trabajo Fin de Máster <br/> Diseño de una arquitectura multimodal para descripción textual de pares imagen-audio\n",
        "\n",
        "## Script 6. Entrenamiento del modelo conjunto con inputs de imagen, texto y audio\n",
        "\n",
        "En este notebook, usamos la base de datos que hemos definido en el Script 5 para entrenar un modelo que acepta imágenes, piezas de texto y audios como inputs. Este modelo pretende diferenciar las distintas personas que han participado en la creación de la misma."
      ],
      "metadata": {
        "id": "z3fMe8NQrjgk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 1. Montamos el almacenamiento\n",
        "\n",
        "Damos permiso a Colab para acceder a mi unidad de Drive y nos situamos en la carpeta donde tenemos los scripts y la librería que hemos creado con las clases propias."
      ],
      "metadata": {
        "id": "u5XhgzYSstMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(0)"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "id": "_wERV4ujAEvf",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721316867069,
          "user_tz": -120,
          "elapsed": 4966,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "gather": {
          "logged": 1723657702985
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('..')\n",
        "os.getcwd()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "'/mnt/batch/tasks/shared/LS_root/mounts/clusters/tfm-cpu/code/Users/jose.puche/Scripts'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "id": "20aib2kKsyqP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721316887311,
          "user_tz": -120,
          "elapsed": 20252,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "outputId": "d69fa3d0-832c-4210-9d64-a87d9a6f2afb",
        "gather": {
          "logged": 1723657703565
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 2. Iniciamos sesión para registrar los resultados en wandb\n"
      ],
      "metadata": {
        "id": "b3ym-_u8GQOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "!wandb login 1b8abaacf33b7b5812267384768c22a1eef3c11e"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/azureuser/.netrc\r\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "yC3Z84iGGbTX",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721316937562,
          "user_tz": -120,
          "elapsed": 36699,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "outputId": "1ff09cd9-b5bb-41de-830a-23ec8a983dff",
        "gather": {
          "logged": 1723657707114
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 2. Importación de paquetes\n",
        "\n",
        "Instalamos las librerías necesarias (entre ellas, necesitamos el modelo CLIP, que descargamos directamente desde github), e importamos otras necesarias.\n",
        "\n",
        "También importamos el dataset y el modelo que hemos definido para nuestro problema, y que se encuentran en"
      ],
      "metadata": {
        "id": "OJpJhpNRsvqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, Subset, SubsetRandomSampler, DataLoader\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from tfm_lib.audio_processing import AudioUtil, AudioAugmentation\n",
        "from tfm_lib.datasets import CustomDataset\n",
        "from tfm_lib.modelos import AudioCLIP\n",
        "from tfm_lib.EarlyStopping import EarlyStopping"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/jupyter_env/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n  warnings.warn(\"Can't initialize NVML\")\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "id": "_uyQrqZKz3Pt",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721317037454,
          "user_tz": -120,
          "elapsed": 3959,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "gather": {
          "logged": 1723657711509
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función de pérdida\n",
        "def loss_fn(logits, labels):\n",
        "    \"\"\"\n",
        "    logits: Las salidas del modelo (predicciones) para cada clase.\n",
        "    labels: Las etiquetas verdaderas (números enteros) para cada ejemplo.\n",
        "    \"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()  # Función de pérdida de entropía cruzada\n",
        "    return criterion(logits, labels)\n",
        "\n",
        "# Ejemplo de cómo usar la función de pérdida\n",
        "logits = torch.tensor([[0.8, 0.1, 0.1], [0.2, 0.7, 0.1], [0.3, 0.2, 0.5]])\n",
        "labels = torch.tensor([0, 1, 2])\n",
        "\n",
        "loss = loss_fn(logits, labels)\n",
        "print(\"Pérdida:\", loss.item())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Pérdida: 0.7991690635681152\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzHNQ6XCpXlf",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721317045748,
          "user_tz": -120,
          "elapsed": 29,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "outputId": "abc646c5-1cb7-4d88-a486-93eca6eaca56",
        "gather": {
          "logged": 1723657711877
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 3. Definición de parámetros y configuración"
      ],
      "metadata": {
        "id": "1PAe7ILdyoPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = './../Final_Database'\n",
        "num_epochs = 20\n",
        "BATCH_SIZE = 32\n",
        "data_augmentation = True\n",
        "da = \"_DA\" if data_augmentation else \"\"\n",
        "lr = 1e-4\n",
        "output_dim = 2\n",
        "selected_model = 'ViT-B/32'\n",
        "\n",
        "model_parameters_file = f\"./modelos/multimodal/FULL_{selected_model.replace('/','')}_{output_dim}pers_lr{f'{lr:.0e}'}_bs{BATCH_SIZE}_{num_epochs}ep{da}.pt\"\n",
        "print(model_parameters_file)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "./modelos/multimodal/FULL_ViT-B32_2pers_lr1e-04_bs32_20ep_DA.pt\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4en3ajR6MTnW",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721320046475,
          "user_tz": -120,
          "elapsed": 495,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "outputId": "65318f54-e302-49a7-a3cc-fe0daeae10bf",
        "gather": {
          "logged": 1723657712200
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WandB – Initialize a new run\n",
        "run_name = model_parameters_file.split(\"/\")[-1].replace('.pt', '')\n",
        "wandb.init(entity=\"josealbertoap\", project='TFM', name = run_name, tags=[\"multimodal\"])\n",
        "\n",
        "# WandB – Config is a variable that holds and saves hyperparameters and inputs\n",
        "config = wandb.config          # Initialize config\n",
        "config.batch_size = BATCH_SIZE          # input batch size for training (default: 64)\n",
        "config.test_batch_size = BATCH_SIZE    # input batch size for testing (default: 1000)\n",
        "config.epochs = num_epochs             # number of epochs to train (default: 10)\n",
        "config.lr = lr              # learning rate (default: 0.01)\n",
        "config.momentum = 0          # SGD momentum (default: 0.5)\n",
        "config.no_cuda = True         # disables CUDA training\n",
        "config.seed = 0               # random seed (default: 42)\n",
        "config.log_interval = 1     # how many batches to wait before logging training status\n",
        "config.num_classes = output_dim"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjosealbertoap\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "wandb version 0.17.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.17.5"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/mnt/batch/tasks/shared/LS_root/mounts/clusters/tfm-cpu/code/Users/jose.puche/Scripts/wandb/run-20240814_174833-lt4oe8xz</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/josealbertoap/TFM/runs/lt4oe8xz' target=\"_blank\">FULL_ViT-B32_2pers_lr1e-04_bs32_20ep_DA</a></strong> to <a href='https://wandb.ai/josealbertoap/TFM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/josealbertoap/TFM' target=\"_blank\">https://wandb.ai/josealbertoap/TFM</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/josealbertoap/TFM/runs/lt4oe8xz' target=\"_blank\">https://wandb.ai/josealbertoap/TFM/runs/lt4oe8xz</a>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "id": "Z1N6kbdL2kpi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721317355610,
          "user_tz": -120,
          "elapsed": 2541,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "outputId": "05b53740-587e-46d8-aa38-5fedff721557",
        "gather": {
          "logged": 1723657721658
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 4. Definición de modelo y base de datos"
      ],
      "metadata": {
        "id": "8KjZYpM-zoWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import Resize, Compose, ColorJitter, RandomHorizontalFlip, \\\n",
        "                                   RandomResizedCrop, RandomRotation, Normalize, ToTensor\n",
        "\n",
        "def train_test_dataloaders(database_df, model, num_classes, data_augmentation=False, BATCH_SIZE=32, test_split=0.2):\n",
        "\n",
        "    dataset = CustomDataset(database_df, num_classes, image_transform = model.preprocess)\n",
        "\n",
        "    train_idx, test_idx = train_test_split(list(range(len(dataset))), test_size=test_split,\n",
        "                                           stratify=dataset.database_info.classID, random_state=42)\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "\n",
        "    # test_subset = Subset(dataset, test_idx) # En caso de que quisiéramos un Dataset y no un Dataloader\n",
        "    test_sampler = SubsetRandomSampler(test_idx)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=test_sampler)\n",
        "\n",
        "    # En caso de tener data augmentation, cambiamos el dataset para el Dataloader de train\n",
        "    if data_augmentation:\n",
        "\n",
        "      augmentation = Compose([\n",
        "            RandomHorizontalFlip(p=0.3),\n",
        "            RandomRotation(degrees=(0, 45), fill=0),\n",
        "            RandomResizedCrop(size=(224, 224), scale=(0.2, 1.0), ratio=(0.8, 1.2)),\n",
        "            # ColorJitter(brightness=.3, contrast=.1, saturation=.1, hue=.1),\n",
        "            ToTensor(),\n",
        "            Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
        "        ])\n",
        "\n",
        "      dataset = CustomDataset(database_df, num_classes, image_transform = augmentation)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
        "\n",
        "    return train_loader, test_loader, dataset.labelencoder.classes_\n",
        "\n",
        "# Por si hay que meter la data augmentation para los audios\n",
        "# aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "id": "q-bNbcxUz9xm",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721320931417,
          "user_tz": -120,
          "elapsed": 5,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "gather": {
          "logged": 1723657722001
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargamos el modelo pre-entrenado y procesador de CLIP\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "model = AudioCLIP(selected_model, device, output_dim).to(device)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "train_loader, test_loader, classes = train_test_dataloaders(pd.read_csv(f'{folder_path}/finalDB_train.csv'),\n",
        "                                                            model, output_dim, data_augmentation, BATCH_SIZE, 0.2)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Device: cpu\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "id": "yhcgIsqp-HBB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721320941710,
          "user_tz": -120,
          "elapsed": 4570,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "outputId": "bd8cc195-90e5-4b90-b254-bacbf1004ef8",
        "gather": {
          "logged": 1723657725627
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 5. Entrenamiento del modelo"
      ],
      "metadata": {
        "id": "Dsqfl1eWziyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializa el optimizador\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience = 3)\n",
        "\n",
        "train_loss = {}\n",
        "test_loss = {}\n",
        "train_acc = {}\n",
        "test_acc = {}\n",
        "\n",
        "# Creamos la lista de descripciones para evaluar el modelo\n",
        "print(f\"People:{classes}\\n\")\n",
        "eval_descriptions = torch.cat([clip.tokenize(f\"a photo of {c}\") for c in classes])\n",
        "\n",
        "early_stopping = EarlyStopping(patience=5, verbose=True, delta=0.01, path=model_parameters_file)\n",
        "\n",
        "wandb.watch(model, log=\"all\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    train_steps = tqdm(train_loader, unit=\"batch\")\n",
        "\n",
        "    for images, audios, labels in train_steps:\n",
        "\n",
        "        train_steps.set_description(f\"Epoch [{epoch+1}/{num_epochs}]. Training\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        text_desc = eval_descriptions.to(device)\n",
        "        audios = audios.to(device)\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        output = model(images, text_desc, audios)\n",
        "\n",
        "        # Cálculo de la accuracy\n",
        "        predictions = output.argmax(dim=1, keepdim=True).squeeze()\n",
        "        correct = (predictions == labels).sum().item()\n",
        "\n",
        "        total_samples += labels.size(0)\n",
        "        total_correct += correct\n",
        "\n",
        "        # Cálculo de la función de pérdida y actualización del modelo\n",
        "        loss = loss_fn(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        train_steps.set_postfix(mean_loss=epoch_loss/total_samples, mean_accuracy = total_correct / total_samples)\n",
        "\n",
        "    train_loss[epoch+1] = epoch_loss / len(train_loader)\n",
        "    train_acc[epoch+1] = total_correct / total_samples\n",
        "\n",
        "    # Evaluación en el conjunto de prueba\n",
        "    model.eval()  # Cambiamos al modo de evaluación\n",
        "    epoch_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    test_steps = tqdm(test_loader, unit=\"batch\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, audios, labels in test_steps:  # Itera sobre los datos de prueba\n",
        "\n",
        "            test_steps.set_description(f\"Epoch [{epoch+1}/{num_epochs}]. Validation\")\n",
        "\n",
        "            text_desc = eval_descriptions.to(device)\n",
        "            audios = audios.to(device)\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            output = model(images, text_desc, audios)\n",
        "\n",
        "            # Cálculo de la accuracy\n",
        "            predictions = output.argmax(dim=1, keepdim=True).squeeze()\n",
        "            correct = (predictions == labels).sum().item()\n",
        "\n",
        "            total_samples += labels.size(0)\n",
        "            total_correct += correct\n",
        "\n",
        "            # Cálculo de la función de pérdida y actualización del modelo\n",
        "            loss = loss_fn(output, labels)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            test_steps.set_postfix(mean_loss=epoch_loss/total_samples, mean_accuracy = total_correct / total_samples)\n",
        "\n",
        "        test_loss[epoch+1] = epoch_loss / len(test_loader)\n",
        "        test_acc[epoch+1] = total_correct / total_samples\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}]:')\n",
        "        print(f'- Training. Loss = {train_loss[epoch+1]}; Accuracy = {train_acc[epoch+1]}.')\n",
        "        print(f'- Validation. Loss = {test_loss[epoch+1]}; Accuracy = {test_acc[epoch+1]}.')\n",
        "        print()\n",
        "\n",
        "        wandb.log({\n",
        "                        'Epoch': epoch+1,\n",
        "                        'Training Loss': train_loss[epoch+1],\n",
        "                        'Training Accuracy': train_acc[epoch+1],\n",
        "                        'Evaluation Loss': test_loss[epoch+1],\n",
        "                        'Evaluation Accuracy': test_acc[epoch+1],\n",
        "                    })\n",
        "\n",
        "        # Llamar a early_stopping con la pérdida de validación actual y el modelo\n",
        "        early_stopping(test_loss[epoch+1], model)\n",
        "        print('')\n",
        "\n",
        "        # Si se alcanza el criterio de early stopping, romper el bucle\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "        # Reducir el learning rate en caso de que no esté mejorando la pérdida\n",
        "        scheduler.step(test_loss[epoch+1])\n",
        "\n",
        "print({'train_acc': train_acc, 'train_loss': train_loss, 'val_acc': test_acc, 'val_loss': test_loss})\n",
        "\n",
        "wandb.save(model_parameters_file)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "People:['Alba Azorin Zafrilla' 'Jose Alberto Azorin Puche']\n\nEpoch [1/20]:\n- Training. Loss = 0.703787624835968; Accuracy = 0.5263157894736842.\n- Validation. Loss = 0.6900548040866852; Accuracy = 0.5348837209302325.\n\nValidation loss decreased (inf --> 0.690055).  Saving model ...\n\nEpoch [2/20]:\n- Training. Loss = 0.6999651988347372; Accuracy = 0.5263157894736842.\n- Validation. Loss = 0.6983155012130737; Accuracy = 0.5348837209302325.\n\nEarlyStopping counter: 1 out of 5\n\nEpoch [3/20]:\n- Training. Loss = 0.6933479011058807; Accuracy = 0.5263157894736842.\n- Validation. Loss = 0.7056849598884583; Accuracy = 0.5348837209302325.\n\nEarlyStopping counter: 2 out of 5\n\nEpoch [4/20]:\n- Training. Loss = 0.7061681846777598; Accuracy = 0.5263157894736842.\n- Validation. Loss = 0.6926553249359131; Accuracy = 0.5348837209302325.\n\nEarlyStopping counter: 3 out of 5\n\nEpoch [5/20]:\n- Training. Loss = 0.704667737086614; Accuracy = 0.5263157894736842.\n- Validation. Loss = 0.7140788435935974; Accuracy = 0.5348837209302325.\n\nEarlyStopping counter: 4 out of 5\n\nEpoch [6/20]:\n- Training. Loss = 0.7017974158128103; Accuracy = 0.5263157894736842.\n- Validation. Loss = 0.6813008785247803; Accuracy = 0.5348837209302325.\n\nEarlyStopping counter: 5 out of 5\n\nEarly stopping\n{'train_acc': {1: 0.5263157894736842, 2: 0.5263157894736842, 3: 0.5263157894736842, 4: 0.5263157894736842, 5: 0.5263157894736842, 6: 0.5263157894736842}, 'train_loss': {1: 0.703787624835968, 2: 0.6999651988347372, 3: 0.6933479011058807, 4: 0.7061681846777598, 5: 0.704667737086614, 6: 0.7017974158128103}, 'val_acc': {1: 0.5348837209302325, 2: 0.5348837209302325, 3: 0.5348837209302325, 4: 0.5348837209302325, 5: 0.5348837209302325, 6: 0.5348837209302325}, 'val_loss': {1: 0.6900548040866852, 2: 0.6983155012130737, 3: 0.7056849598884583, 4: 0.6926553249359131, 5: 0.7140788435935974, 6: 0.6813008785247803}}\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Epoch [1/20]. Training: 100%|██████████| 6/6 [01:12<00:00, 12.06s/batch, mean_accuracy=0.526, mean_loss=0.0247]\nEpoch [1/20]. Validation: 100%|██████████| 2/2 [00:15<00:00,  7.71s/batch, mean_accuracy=0.535, mean_loss=0.0321]\nEpoch [2/20]. Training: 100%|██████████| 6/6 [01:10<00:00, 11.83s/batch, mean_accuracy=0.526, mean_loss=0.0246]\nEpoch [2/20]. Validation: 100%|██████████| 2/2 [00:16<00:00,  8.16s/batch, mean_accuracy=0.535, mean_loss=0.0325]\nEpoch [3/20]. Training: 100%|██████████| 6/6 [01:11<00:00, 11.97s/batch, mean_accuracy=0.526, mean_loss=0.0243]\nEpoch [3/20]. Validation: 100%|██████████| 2/2 [00:15<00:00,  7.93s/batch, mean_accuracy=0.535, mean_loss=0.0328]\nEpoch [4/20]. Training: 100%|██████████| 6/6 [01:12<00:00, 12.04s/batch, mean_accuracy=0.526, mean_loss=0.0248]\nEpoch [4/20]. Validation: 100%|██████████| 2/2 [00:15<00:00,  7.81s/batch, mean_accuracy=0.535, mean_loss=0.0322]\nEpoch [5/20]. Training: 100%|██████████| 6/6 [01:12<00:00, 12.04s/batch, mean_accuracy=0.526, mean_loss=0.0247]\nEpoch [5/20]. Validation: 100%|██████████| 2/2 [00:15<00:00,  7.99s/batch, mean_accuracy=0.535, mean_loss=0.0332]\nEpoch [6/20]. Training: 100%|██████████| 6/6 [01:11<00:00, 11.94s/batch, mean_accuracy=0.526, mean_loss=0.0246]\nEpoch [6/20]. Validation: 100%|██████████| 2/2 [00:15<00:00,  7.91s/batch, mean_accuracy=0.535, mean_loss=0.0317]\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "['/mnt/batch/tasks/shared/LS_root/mounts/clusters/tfm-cpu/code/Users/jose.puche/Scripts/wandb/run-20240814_174833-lt4oe8xz/files/modelos/multimodal/FULL_ViT-B32_2pers_lr1e-04_bs32_20ep_DA.pt']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 946
        },
        "id": "jL5OGRZhBXR0",
        "executionInfo": {
          "status": "error",
          "timestamp": 1721321030627,
          "user_tz": -120,
          "elapsed": 82805,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "outputId": "644a3909-3a4e-4a31-8178-2d575aab801c",
        "gather": {
          "logged": 1723658259664
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluación del modelo entrenado"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = CustomDataset(pd.read_csv(f'{folder_path}/finalDB_test.csv'), \n",
        "                            output_dim, image_transform = model.preprocess)\n",
        "test_dl = torch.utils.data.DataLoader(test_dataset, batch_size=1048, shuffle=True)"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723658260027
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Inference\n",
        "# ----------------------------\n",
        "def inference (model, test_dl):\n",
        "  correct_prediction = 0\n",
        "  total_prediction = 0\n",
        "\n",
        "  # Disable gradient updates\n",
        "  with torch.no_grad():\n",
        "\n",
        "    predictions = []\n",
        "    label_list = []\n",
        "    for data in test_dl:\n",
        "      # Get the input features and target labels, and put them on the GPU\n",
        "      images, audios, labels = data[0].to(device), data[1].to(device), data[2].to(device)\n",
        "      texts = eval_descriptions.to(device)\n",
        "\n",
        "      # Get predictions\n",
        "      outputs = model(images, texts, audios)\n",
        "\n",
        "      # Get the predicted class with the highest score\n",
        "      _, prediction = torch.max(outputs,1)\n",
        "      # Count of predictions that matched the target label\n",
        "      correct_prediction += (prediction == labels).sum().item()\n",
        "      total_prediction += prediction.shape[0]\n",
        "\n",
        "      predictions.extend(prediction)\n",
        "      label_list.extend(data[2])\n",
        "\n",
        "  acc = correct_prediction/total_prediction\n",
        "  print(f'Accuracy: {acc:.2f}, Total items: {total_prediction}')\n",
        "\n",
        "  return predictions, label_list\n",
        "\n",
        "# Run inference on trained model with the validation set\n",
        "model.load_state_dict(torch.load(model_parameters_file, map_location=torch.device('cpu')))\n",
        "result = inference(model, test_dl)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Accuracy: 0.46, Total items: 48\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723658290223
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, f1_score, confusion_matrix\n",
        "import seaborn as sn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "def extraer_iniciales(name):\n",
        "    name_words = name.split(' ')\n",
        "    r = re.compile(\"^[A-Z][A-z]*\")\n",
        "    valid_words = list(filter(r.match, name_words))\n",
        "    if len(valid_words) <=3:\n",
        "        name = valid_words[0]\n",
        "        valid_words.remove(valid_words[0])\n",
        "    else:\n",
        "        name = f'{valid_words[0]} {valid_words[1]}'\n",
        "        valid_words.remove(valid_words[0])\n",
        "        valid_words.remove(valid_words[1])\n",
        "    surname = re.sub('(?<=[A-Z])[A-z]+', '.', ' '.join(valid_words))\n",
        "    return f'{name} {surname}'\n",
        "\n",
        "def font_scale(num_classes):\n",
        "    if num_classes <= 10:\n",
        "        return 1.0\n",
        "    elif num_classes <= 20:\n",
        "        return 0.75\n",
        "    elif num_classes <= 30:\n",
        "        return 0.65\n",
        "    else:\n",
        "        return 0.45\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred):\n",
        "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    people = list(map(extraer_iniciales, test_dataset.labelencoder.classes_))\n",
        "\n",
        "    df_cm = pd.DataFrame((cf_matrix / np.sum(cf_matrix, axis=1)[:, None]).round(3), index=people, columns=people)\n",
        "    \n",
        "    plt.figure(figsize=(8, 6))  \n",
        "    sn.set(font_scale = font_scale(df_cm.shape[0]))  \n",
        "    heatmap = sn.heatmap(df_cm, annot=True, cbar=False, cmap='Purples', fmt='g', xticklabels=False)\n",
        "\n",
        "    # Ajusta la rotación y alineación de los ticks de los ejes\n",
        "    heatmap.set_yticklabels(heatmap.get_yticklabels(), rotation=0, ha='right')\n",
        "\n",
        "    plt.tight_layout()  # Asegura que todo se ajuste bien en la figura\n",
        "    plt.savefig(model_parameters_file.replace('/modelos/', '/results/').replace('.pt', '.png'))\n",
        "\n",
        "    return plt.gcf()\n",
        "\n",
        "def get_metrics(result):\n",
        "    accuracy = accuracy_score(result[1], result[0])\n",
        "    precision = precision_score(result[1], result[0], average='macro')\n",
        "    recall = recall_score(result[1], result[0], average='macro')\n",
        "    f1 = f1_score(result[1], result[0], average='macro')\n",
        "\n",
        "    metrics = {\n",
        "        'Test accuracy': accuracy,\n",
        "        'Test precision': precision,\n",
        "        'Test recall': recall,\n",
        "        'F1-score': f1\n",
        "    }\n",
        "\n",
        "    print(metrics)\n",
        "\n",
        "    metrics['Confusion Matrix'] = wandb.Image(plot_confusion_matrix(result[1],result[0]))\n",
        "    metrics['Test metrics'] = wandb.Table(columns=[\"Metric name\", \"Value\"], \n",
        "                                          data=[[\"Test accuracy\", accuracy], [\"Test precision\", precision],\n",
        "                                                [\"Test recall\", recall], [\"Test F1-Score\", f1]])\n",
        "\n",
        "    return metrics\n",
        "\n",
        "metrics = get_metrics(result)\n",
        "wandb.log(metrics)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{'Test accuracy': 0.4583333333333333, 'Test precision': 0.22916666666666666, 'Test recall': 0.5, 'F1-score': 0.3142857142857143}\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/jupyter_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 800x600 with 1 Axes>",
            "image/png": "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"
          },
          "metadata": {}
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723658293432
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "image_results = []\n",
        "\n",
        "audio_file = './../Final_Database/audio/Jose Alberto Azorin Puche/audio0000.ogg'\n",
        "aud = AudioUtil.open(audio_file)\n",
        "aud = AudioUtil.resample(aud, 16000)\n",
        "aud = AudioUtil.rechannel(aud, 1)\n",
        "aud = AudioAugmentation.pad_trunc(aud, 4)\n",
        "sgram_1 = AudioUtil.spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None).unsqueeze(0).to(device)\n",
        "\n",
        "audio_file = './../Final_Database/audio/Jose Alberto Azorin Puche/audio_prueba.ogg'\n",
        "aud = AudioUtil.open(audio_file)\n",
        "aud = AudioUtil.resample(aud, 16000)\n",
        "aud = AudioUtil.rechannel(aud, 1)\n",
        "aud = AudioAugmentation.pad_trunc(aud, 4)\n",
        "sgram_2 = AudioUtil.spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None).unsqueeze(0).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    prueba = False\n",
        "\n",
        "    for sgram in [sgram_1, sgram_2]:\n",
        "        audio_name = 'Prueba' if prueba else 'Original'\n",
        "\n",
        "        for i in range(4):\n",
        "\n",
        "            read_image = Image.open(f'./../Test_images/IMG_000{i}.jpg')\n",
        "            image = model.preprocess(read_image).unsqueeze(0).to(device)\n",
        "\n",
        "            output = model(image, eval_descriptions, sgram)\n",
        "            probs = torch.round(output.softmax(dim=-1), decimals=4)\n",
        "            pred_prob = torch.max(probs).item()\n",
        "            pred_person = classes[torch.argmax(probs)]\n",
        "            my_prob = probs.squeeze()[list(classes).index('Jose Alberto Azorin Puche')].item()\n",
        "\n",
        "            image_results.append([f'Imagen {i+1}', audio_name, pred_person, pred_prob, my_prob])\n",
        "\n",
        "        prueba = True\n",
        "        \n",
        "print(image_results)\n",
        "wandb.log({\"Test images results\": wandb.Table(columns=[\"Imagen\", \"Audio\", \"Persona\", \"Probabilidad\", \"Prob (Joseal)\"], data=image_results)})        \n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[['Imagen 1', 'Original', 'Alba Azorin Zafrilla', 0.6026999950408936, 0.39730000495910645], ['Imagen 2', 'Original', 'Alba Azorin Zafrilla', 0.5971999764442444, 0.4027999937534332], ['Imagen 3', 'Original', 'Alba Azorin Zafrilla', 0.5997999906539917, 0.4002000093460083], ['Imagen 4', 'Original', 'Alba Azorin Zafrilla', 0.5981000065803528, 0.4018999934196472], ['Imagen 1', 'Prueba', 'Alba Azorin Zafrilla', 0.5875999927520752, 0.4124000072479248], ['Imagen 2', 'Prueba', 'Alba Azorin Zafrilla', 0.5819000005722046, 0.4180999994277954], ['Imagen 3', 'Prueba', 'Alba Azorin Zafrilla', 0.5845999717712402, 0.4153999984264374], ['Imagen 4', 'Prueba', 'Alba Azorin Zafrilla', 0.5828999876976013, 0.4171000123023987]]\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1723658296316
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(Label(value='577.319 MB of 577.319 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "901035a54f794c9488edcfed240e8c6d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▄▅▇█</td></tr><tr><td>Evaluation Accuracy</td><td>▁▁▁▁▁▁</td></tr><tr><td>Evaluation Loss</td><td>▃▅▆▃█▁</td></tr><tr><td>F1-score</td><td>▁</td></tr><tr><td>Test accuracy</td><td>▁</td></tr><tr><td>Test precision</td><td>▁</td></tr><tr><td>Test recall</td><td>▁</td></tr><tr><td>Training Accuracy</td><td>▁▁▁▁▁▁</td></tr><tr><td>Training Loss</td><td>▇▅▁█▇▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>6</td></tr><tr><td>Evaluation Accuracy</td><td>0.53488</td></tr><tr><td>Evaluation Loss</td><td>0.6813</td></tr><tr><td>F1-score</td><td>0.31429</td></tr><tr><td>Test accuracy</td><td>0.45833</td></tr><tr><td>Test precision</td><td>0.22917</td></tr><tr><td>Test recall</td><td>0.5</td></tr><tr><td>Training Accuracy</td><td>0.52632</td></tr><tr><td>Training Loss</td><td>0.7018</td></tr></table><br/></div></div>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run <strong style=\"color:#cdcd00\">FULL_ViT-B32_2pers_lr1e-04_bs32_20ep_DA</strong> at: <a href='https://wandb.ai/josealbertoap/TFM/runs/lt4oe8xz' target=\"_blank\">https://wandb.ai/josealbertoap/TFM/runs/lt4oe8xz</a><br/> View project at: <a href='https://wandb.ai/josealbertoap/TFM' target=\"_blank\">https://wandb.ai/josealbertoap/TFM</a><br/>Synced 6 W&B file(s), 3 media file(s), 2 artifact file(s) and 1 other file(s)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Find logs at: <code>./wandb/run-20240814_174833-lt4oe8xz/logs</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
          },
          "metadata": {}
        }
      ],
      "execution_count": 15,
      "metadata": {
        "id": "w5374GuwbV2Z",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721319542390,
          "user_tz": -120,
          "elapsed": 4285,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "170b3beef2d44b0b88a34e0b1f4222b3",
            "ff72256106fb48cb9ab1927eed418326",
            "be3df9b759d340279bdcfd3936ecf88e",
            "880e7f5d493b49b0874b42644cb53e94",
            "2f7f50e6edd24047b6ddb1aec2ddc61a",
            "f6656948665744d39b587f1fcf5643a3",
            "e1fbf550fe864c559af5522d12a4b050",
            "69ee02a8a33141ef9f324193fd3a5361"
          ]
        },
        "outputId": "ec3a01a7-ffad-486d-e94f-0d2a9918928b",
        "gather": {
          "logged": 1723658306566
        }
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "u5XhgzYSstMq",
        "OJpJhpNRsvqj",
        "1PAe7ILdyoPt",
        "8KjZYpM-zoWn",
        "Dsqfl1eWziyb"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNdoDSWLx0t06VSUizuGF13"
    },
    "accelerator": "GPU",
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "170b3beef2d44b0b88a34e0b1f4222b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_model_module": "@jupyter-widgets/controls",
            "_view_name": "VBoxView",
            "_view_module": "@jupyter-widgets/controls",
            "_dom_classes": [],
            "layout": "IPY_MODEL_880e7f5d493b49b0874b42644cb53e94",
            "_view_count": null,
            "_model_module_version": "1.5.0",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff72256106fb48cb9ab1927eed418326",
              "IPY_MODEL_be3df9b759d340279bdcfd3936ecf88e"
            ]
          }
        },
        "ff72256106fb48cb9ab1927eed418326": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_name": "LabelModel",
            "_model_module": "@jupyter-widgets/controls",
            "_view_name": "LabelView",
            "_view_module": "@jupyter-widgets/controls",
            "_dom_classes": [],
            "layout": "IPY_MODEL_2f7f50e6edd24047b6ddb1aec2ddc61a",
            "value": "0.017 MB of 0.017 MB uploaded\r",
            "style": "IPY_MODEL_f6656948665744d39b587f1fcf5643a3",
            "placeholder": "​",
            "_view_count": null,
            "_model_module_version": "1.5.0",
            "description": ""
          }
        },
        "be3df9b759d340279bdcfd3936ecf88e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_name": "FloatProgressModel",
            "_model_module": "@jupyter-widgets/controls",
            "max": 1,
            "bar_style": "",
            "_view_name": "ProgressView",
            "_view_module": "@jupyter-widgets/controls",
            "_dom_classes": [],
            "layout": "IPY_MODEL_e1fbf550fe864c559af5522d12a4b050",
            "orientation": "horizontal",
            "value": 1,
            "style": "IPY_MODEL_69ee02a8a33141ef9f324193fd3a5361",
            "min": 0,
            "_view_count": null,
            "_model_module_version": "1.5.0",
            "description": ""
          }
        },
        "880e7f5d493b49b0874b42644cb53e94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "grid_row": null,
            "_model_module": "@jupyter-widgets/base",
            "overflow": null,
            "max_height": null,
            "display": null,
            "grid_auto_flow": null,
            "grid_template_rows": null,
            "align_self": null,
            "grid_auto_columns": null,
            "width": null,
            "grid_area": null,
            "align_items": null,
            "_view_name": "LayoutView",
            "left": null,
            "height": null,
            "_view_module": "@jupyter-widgets/base",
            "object_position": null,
            "justify_content": null,
            "bottom": null,
            "max_width": null,
            "border": null,
            "margin": null,
            "order": null,
            "grid_column": null,
            "grid_auto_rows": null,
            "padding": null,
            "grid_template_columns": null,
            "justify_items": null,
            "object_fit": null,
            "visibility": null,
            "_view_count": null,
            "flex_flow": null,
            "min_height": null,
            "top": null,
            "min_width": null,
            "flex": null,
            "_model_module_version": "1.2.0",
            "grid_template_areas": null,
            "overflow_x": null,
            "right": null,
            "overflow_y": null,
            "grid_gap": null,
            "align_content": null
          }
        },
        "2f7f50e6edd24047b6ddb1aec2ddc61a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "grid_row": null,
            "_model_module": "@jupyter-widgets/base",
            "overflow": null,
            "max_height": null,
            "display": null,
            "grid_auto_flow": null,
            "grid_template_rows": null,
            "align_self": null,
            "grid_auto_columns": null,
            "width": null,
            "grid_area": null,
            "align_items": null,
            "_view_name": "LayoutView",
            "left": null,
            "height": null,
            "_view_module": "@jupyter-widgets/base",
            "object_position": null,
            "justify_content": null,
            "bottom": null,
            "max_width": null,
            "border": null,
            "margin": null,
            "order": null,
            "grid_column": null,
            "grid_auto_rows": null,
            "padding": null,
            "grid_template_columns": null,
            "justify_items": null,
            "object_fit": null,
            "visibility": null,
            "_view_count": null,
            "flex_flow": null,
            "min_height": null,
            "top": null,
            "min_width": null,
            "flex": null,
            "_model_module_version": "1.2.0",
            "grid_template_areas": null,
            "overflow_x": null,
            "right": null,
            "overflow_y": null,
            "grid_gap": null,
            "align_content": null
          }
        },
        "f6656948665744d39b587f1fcf5643a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1fbf550fe864c559af5522d12a4b050": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "grid_row": null,
            "_model_module": "@jupyter-widgets/base",
            "overflow": null,
            "max_height": null,
            "display": null,
            "grid_auto_flow": null,
            "grid_template_rows": null,
            "align_self": null,
            "grid_auto_columns": null,
            "width": null,
            "grid_area": null,
            "align_items": null,
            "_view_name": "LayoutView",
            "left": null,
            "height": null,
            "_view_module": "@jupyter-widgets/base",
            "object_position": null,
            "justify_content": null,
            "bottom": null,
            "max_width": null,
            "border": null,
            "margin": null,
            "order": null,
            "grid_column": null,
            "grid_auto_rows": null,
            "padding": null,
            "grid_template_columns": null,
            "justify_items": null,
            "object_fit": null,
            "visibility": null,
            "_view_count": null,
            "flex_flow": null,
            "min_height": null,
            "top": null,
            "min_width": null,
            "flex": null,
            "_model_module_version": "1.2.0",
            "grid_template_areas": null,
            "overflow_x": null,
            "right": null,
            "overflow_y": null,
            "grid_gap": null,
            "align_content": null
          }
        },
        "69ee02a8a33141ef9f324193fd3a5361": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "ProgressStyleModel",
            "_model_module": "@jupyter-widgets/controls",
            "description_width": "",
            "_view_name": "StyleView",
            "_view_module": "@jupyter-widgets/base",
            "_view_count": null,
            "bar_color": null,
            "_model_module_version": "1.5.0"
          }
        }
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.18",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}