{"cells":[{"cell_type":"markdown","metadata":{"id":"2smUTVGegCuP"},"source":["### Importación de paquetes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":108087,"status":"ok","timestamp":1715533037092,"user":{"displayName":"JOSÉ ALBERTO AZORIN PUCHE","userId":"07780853208545474625"},"user_tz":-120},"id":"JoZjHL63gLmK","outputId":"22c013b4-2dc1-4022-ef6c-6e48f92714a9"},"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: line 1: conda: command not found\n","Collecting ftfy\n","  Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m968.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n","Installing collected packages: ftfy\n","Successfully installed ftfy-6.2.0\n","Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-xdiipqc8\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-xdiipqc8\n","  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.2.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.4)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.2.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.17.1+cu121)\n","Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->clip==1.0)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->clip==1.0)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->clip==1.0)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->clip==1.0)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->clip==1.0)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->clip==1.0)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch->clip==1.0)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->clip==1.0)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->clip==1.0)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch->clip==1.0)\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch->clip==1.0)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->clip==1.0)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.25.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n","Building wheels for collected packages: clip\n","  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369499 sha256=b7604fc4be4f4317e4a49bae10523945f62684fcc9875995f44d7a77ed474f52\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-87q9z_f6/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n","Successfully built clip\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, clip\n","Successfully installed clip-1.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"]}],"source":["!conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\n","!pip install ftfy regex tqdm\n","!pip install git+https://github.com/openai/CLIP.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xTCFxiapgCuV"},"outputs":[],"source":["import os\n","\n","import torch\n","import torch.nn as nn\n","\n","import clip\n","import torch\n","from torchvision.datasets import ImageFolder\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset, Subset, SubsetRandomSampler, DataLoader\n","\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1715533047238,"user":{"displayName":"JOSÉ ALBERTO AZORIN PUCHE","userId":"07780853208545474625"},"user_tz":-120},"id":"bSvjOYHgw44U","outputId":"ff7bb4c9-8363-490d-f752-70b471279ca2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7ac614c213f0>"]},"metadata":{},"execution_count":3}],"source":["torch.autograd.set_detect_anomaly(True)"]},{"cell_type":"markdown","metadata":{"id":"nNDygGUIgCuX"},"source":["### Definición de clases y funciones"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZNHtJz7bgCuX"},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, image_folder_path, transform=None):\n","        if transform:\n","            self.image_folder = ImageFolder(root=image_folder_path, transform=transform)\n","        else:\n","            self.image_folder = ImageFolder(root=image_folder_path)\n","\n","        self.label_mapping = dict((v, k) for k, v in self.image_folder.class_to_idx.items())\n","        self.text_data = list(map(self.label_mapping.get, self.image_folder.targets))\n","        self.encoded_text = torch.cat([clip.tokenize(text) for text in self.text_data])\n","\n","    def __len__(self):\n","        return len(self.image_folder)\n","\n","    def __getitem__(self, index):\n","        # Obtén la imagen y la etiqueta del ImageFolder\n","        image, label = self.image_folder[index]\n","\n","        # Obtén el texto correspondiente al índice\n","        text = self.encoded_text[index]\n","\n","        # Devuelve la imagen, el texto y la etiqueta\n","        return image, text, label"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1715533047238,"user":{"displayName":"JOSÉ ALBERTO AZORIN PUCHE","userId":"07780853208545474625"},"user_tz":-120},"id":"_HsDPC3xgCuZ","outputId":"e8477505-6b91-4814-b24a-c53942384919"},"outputs":[{"output_type":"stream","name":"stdout","text":["Pérdida: 0.7991690635681152\n"]}],"source":["# Función de pérdida\n","def loss_logits(logits, labels):\n","    \"\"\"\n","    logits: Las salidas del modelo (predicciones) para cada clase.\n","    labels: Las etiquetas verdaderas (números enteros) para cada ejemplo.\n","    \"\"\"\n","    criterion = nn.CrossEntropyLoss()  # Función de pérdida de entropía cruzada\n","    return criterion(logits, labels)\n","\n","# Ejemplo de cómo usar la función de pérdida\n","logits = torch.tensor([[0.8, 0.1, 0.1], [0.2, 0.7, 0.1], [0.3, 0.2, 0.5]])\n","labels = torch.tensor([0, 1, 2])\n","\n","loss = loss_logits(logits, labels)\n","print(\"Pérdida:\", loss.item())"]},{"cell_type":"markdown","metadata":{"id":"45TIaiP6gCud"},"source":["### Entrenamiento del modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26835,"status":"ok","timestamp":1715533074066,"user":{"displayName":"JOSÉ ALBERTO AZORIN PUCHE","userId":"07780853208545474625"},"user_tz":-120},"id":"h4KAkxnGgu7g","outputId":"880d155c-ceec-4a35-af8c-84f8f038aa45"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AKqrgq3AgCud"},"outputs":[],"source":["folder_path = '/content/drive/MyDrive/TFM/Proyecto/Final_Database_mini_prueba/image'\n","num_epochs = 30\n","BATCH_SIZE = 32\n","data_augmentation = True\n","da = \"DA\" if data_augmentation else \"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cU8tjcSTgCue"},"outputs":[],"source":["from torchvision.transforms import Resize, Compose, ColorJitter, RandomHorizontalFlip, \\\n","                                   RandomResizedCrop, RandomRotation, Normalize, ToTensor\n","\n","\n","augmentation = Compose([\n","    RandomHorizontalFlip(p=0.3),\n","    RandomRotation(degrees=(0, 45), fill=0),\n","    RandomResizedCrop(size=(224, 224), scale=(0.2, 1.0), ratio=(0.8, 1.2)),\n","    ToTensor(),\n","    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n","])"]},{"cell_type":"code","source":["def train_test_dataloaders(folder_path, data_augmentation=False, test_split=0.1):\n","\n","    dataset = CustomDataset(folder_path,  transform=preprocess)\n","\n","    train_idx, test_idx = train_test_split(list(range(len(dataset))), test_size=test_split)\n","    train_sampler = SubsetRandomSampler(train_idx)\n","\n","    # test_subset = Subset(dataset, test_idx) # En caso de que quisiéramos un Dataset y no un Dataloader\n","    test_sampler = SubsetRandomSampler(test_idx)\n","    test_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=test_sampler)\n","\n","    # En caso de tener data augmentation, cambiamos el dataset para el Dataloader de train\n","    if data_augmentation:\n","      dataset = CustomDataset(folder_path,  transform=augmentation)\n","\n","    train_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n","\n","    return train_loader, test_loader"],"metadata":{"id":"YyBDPCxxMy3R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Descarga el modelo pre-entrenado y procesador de CLIP\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","selected_model = \"RN50\" # Otros modelos: \"ViT-B/32\"\n","model, preprocess = clip.load(selected_model, device)\n","\n","train_loader, test_loader = train_test_dataloaders(folder_path, data_augmentation, 0.1)"],"metadata":{"id":"7PleTEKgVQwN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715533087077,"user_tz":-120,"elapsed":13020,"user":{"displayName":"JOSÉ ALBERTO AZORIN PUCHE","userId":"07780853208545474625"}},"outputId":"e928efbe-bb6a-4af9-9a2e-a83eaf78ab3a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|████████████████████████████████████████| 244M/244M [00:02<00:00, 120MiB/s]\n"]}]},{"cell_type":"code","source":["lr = 1e-6\n","model_parameters_file = f\"/content/drive/MyDrive/TFM/Proyecto/Scripts/{selected_model}_2pers_lr{f'{lr:.0e}'}_bs{BATCH_SIZE}_{num_epochs}ep{da}.pt\"\n","model_parameters_file"],"metadata":{"id":"6TXpyPD9VJUw","executionInfo":{"status":"ok","timestamp":1715533087078,"user_tz":-120,"elapsed":16,"user":{"displayName":"JOSÉ ALBERTO AZORIN PUCHE","userId":"07780853208545474625"}},"colab":{"base_uri":"https://localhost:8080/","height":36},"outputId":"4bd7075f-4425-4860-96ed-3403e96fc55f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/TFM/Proyecto/Scripts/RN50_2pers_lr1e-06_bs32_30epDA.pt'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# Inicializa el optimizador\n","optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n","\n","train_loss = {}\n","test_loss = {}\n","\n","# Creamos la lista de descripciones para evaluar el modelo\n","people_list = [name for name in os.listdir(folder_path) if os.path.isdir(f'{folder_path}/{name}')]\n","# eval_descriptions = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in people_list]).to(device)\n","eval_descriptions = torch.cat([clip.tokenize(f\"{c}\") for c in people_list]).to(device)\n","\n","for epoch in range(num_epochs):\n","\n","    # Entrena el modelo\n","    model.train()\n","    epoch_loss = 0.0\n","    for images, texts, labels in train_loader:\n","        optimizer.zero_grad()\n","        # texts = texts.to(device)\n","        texts = eval_descriptions.to(device)\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        logits_per_image, logits_per_text = model(images, texts)\n","        loss = loss_logits(logits_per_image, labels) + loss_logits(logits_per_text.T, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    train_loss[epoch] = epoch_loss\n","\n","    # Evaluación en el conjunto de prueba\n","    model.eval()  # Cambiamos al modo de evaluación\n","    epoch_loss = 0.0\n","    with torch.no_grad():\n","        total_correct = 0\n","        total_samples = 0\n","        for images, texts, labels in test_loader:  # Itera sobre los datos de prueba\n","            # texts = texts.to(device)\n","            texts = eval_descriptions.to(device)\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            logits_per_image, logits_per_text = model(images, texts)\n","            loss = loss_logits(logits_per_image, labels) + loss_logits(logits_per_text.T, labels)\n","            epoch_loss += loss.item()\n","\n","            image_features = model.encode_image(images).float()\n","            image_features /= image_features.norm(dim=-1, keepdim=True)\n","            text_features = model.encode_text(eval_descriptions).float()\n","            text_features /= text_features.norm(dim=-1, keepdim=True)\n","\n","            probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n","\n","            predicted = torch.max(probs, 1).indices\n","            total_samples += labels.size(0)\n","            total_correct += (predicted == labels).sum().item()\n","\n","        test_loss[epoch] = epoch_loss\n","\n","        accuracy = total_correct / total_samples\n","\n","        print(f'Epoch [{epoch+1}/{num_epochs}]:')\n","        print(f'- Loss (training):   {train_loss[epoch]}')\n","        print(f'- Loss (evaluation): {test_loss[epoch]}')\n","        print(f'- Accuracy:          {accuracy}')\n","        print()\n","\n","# Guarda el modelo entrenado\n","torch.save(model.state_dict(), model_parameters_file)"],"metadata":{"id":"cfnL-Wak6-UN","colab":{"base_uri":"https://localhost:8080/","height":425},"executionInfo":{"status":"error","timestamp":1715534330011,"user_tz":-120,"elapsed":1242945,"user":{"displayName":"JOSÉ ALBERTO AZORIN PUCHE","userId":"07780853208545474625"}},"outputId":"003c74f2-e99a-4c2f-9ba4-3ee72d39c652"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Expected input batch_size (2) to match target batch_size (32).","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-8c73e3ad828e>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mlogits_per_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_per_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_per_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_per_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-8e702b0deafb>\u001b[0m in \u001b[0;36mloss_logits\u001b[0;34m(logits, labels)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \"\"\"\n\u001b[1;32m      7\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Función de pérdida de entropía cruzada\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Ejemplo de cómo usar la función de pérdida\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1180\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                                label_smoothing=self.label_smoothing)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3058\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3059\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Expected input batch_size (2) to match target batch_size (32)."]}]},{"cell_type":"code","source":["plt.plot(*zip(*sorted(train_loss.items())))\n","plt.plot(*zip(*sorted(test_loss.items())))\n","plt.show()"],"metadata":{"id":"T56NoxGZYRHg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nYkyndl3gCug"},"source":["### Uso del modelo ya entrenado"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3649,"status":"ok","timestamp":1714304011621,"user":{"displayName":"JOSÉ ALBERTO AZORIN PUCHE","userId":"07780853208545474625"},"user_tz":-120},"id":"xdZTAVghgCuh","outputId":"221c869e-b763-43d2-c1b1-301f7f61ed23"},"outputs":[{"output_type":"stream","name":"stdout","text":["Label probs: [[0.18 0.82]]\n","tensor([[18.0000],\n","        [19.5156]], device='cuda:0', dtype=torch.float16)\n"]}],"source":["import clip\n","import torch\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import DataLoader, TensorDataset\n","from torchvision import transforms\n","from PIL import Image\n","import numpy as np\n","\n","# Descarga el modelo pre-entrenado y procesador de CLIP\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model, preprocess = clip.load(selected_model, device)\n","\n","model.load_state_dict(torch.load(model_parameters_file))\n","model.eval()\n","\n","read_image = Image.open('/content/drive/MyDrive/TFM/Proyecto/Final_Database_mini_prueba/image/Juan/frame00000.jpg')\n","image = preprocess(read_image).unsqueeze(0).to(device)\n","text = clip.tokenize([\"Genesis\", \"Juan\"]).to(device)\n","\n","with torch.no_grad():\n","    image_features = model.encode_image(image).float()\n","    text_features = model.encode_text(text).float()\n","\n","    logits_per_image, logits_per_text = model(image, text)\n","    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n","\n","#read_image.show()\n","print(\"Label probs:\", probs)\n","print(logits_per_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1714304011622,"user":{"displayName":"JOSÉ ALBERTO AZORIN PUCHE","userId":"07780853208545474625"},"user_tz":-120},"id":"jPsgcgbtgCuh","colab":{"base_uri":"https://localhost:8080/"},"outputId":"29055c64-79c4-40ed-ec89-f9eb4b9c4fdd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Label probs: [[0.2069 0.793 ]]\n","tensor([[18.1719],\n","        [19.5156]], device='cuda:0', dtype=torch.float16)\n"]}],"source":["text = clip.tokenize([\"Gene\", \"Juan\"]).to(device)\n","\n","with torch.no_grad():\n","    image_features = model.encode_image(image)\n","    text_features = model.encode_text(text)\n","\n","    logits_per_image, logits_per_text = model(image, text)\n","    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n","\n","#read_image.show()\n","print(\"Label probs:\", probs)\n","print(logits_per_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1714304011622,"user":{"displayName":"JOSÉ ALBERTO AZORIN PUCHE","userId":"07780853208545474625"},"user_tz":-120},"id":"Drz8WpVTgCui","colab":{"base_uri":"https://localhost:8080/"},"outputId":"247a74b4-e2a8-4d99-cd23-5bac74dc8eee"},"outputs":[{"output_type":"stream","name":"stdout","text":["Label probs: [[0.4377 0.562 ]]\n","tensor([[18.4531],\n","        [18.7031]], device='cuda:0', dtype=torch.float16)\n"]}],"source":["text = clip.tokenize([\"Genesi\", \"Jun\"]).to(device)\n","\n","with torch.no_grad():\n","    image_features = model.encode_image(image)\n","    text_features = model.encode_text(text)\n","\n","    logits_per_image, logits_per_text = model(image, text)\n","    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n","\n","#read_image.show()\n","print(\"Label probs:\", probs)\n","print(logits_per_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1714304011623,"user":{"displayName":"JOSÉ ALBERTO AZORIN PUCHE","userId":"07780853208545474625"},"user_tz":-120},"id":"JfkrZFM1gCui","colab":{"base_uri":"https://localhost:8080/"},"outputId":"eeda11f4-18ee-4004-9d0a-94c972d604a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Label probs: [[0.07477 0.9253 ]]\n","tensor([[16.6719],\n","        [19.1875]], device='cuda:0', dtype=torch.float16)\n"]}],"source":["text = clip.tokenize([\"Génesis\", \"Juán\"]).to(device)\n","\n","with torch.no_grad():\n","    image_features = model.encode_image(image)\n","    text_features = model.encode_text(text)\n","\n","    logits_per_image, logits_per_text = model(image, text)\n","    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n","\n","print(\"Label probs:\", probs)\n","print(logits_per_text)"]},{"cell_type":"code","source":[],"metadata":{"id":"Mf-n09OSMxMN"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":0}