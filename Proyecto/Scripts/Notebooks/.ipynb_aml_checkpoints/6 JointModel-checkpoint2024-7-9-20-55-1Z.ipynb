{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Trabajo Fin de Máster <br/> Diseño de una arquitectura multimodal para descripción textual de pares imagen-audio\n",
        "\n",
        "## Script 6. Entrenamiento del modelo conjunto con inputs de imagen, texto y audio\n",
        "\n",
        "En este notebook, usamos la base de datos que hemos definido en el Script 5 para entrenar un modelo que acepta imágenes, piezas de texto y audios como inputs. Este modelo pretende diferenciar las distintas personas que han participado en la creación de la misma."
      ],
      "metadata": {
        "id": "z3fMe8NQrjgk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 1. Montamos el almacenamiento\n",
        "\n",
        "Damos permiso a Colab para acceder a mi unidad de Drive y nos situamos en la carpeta donde tenemos los scripts y la librería que hemos creado con las clases propias."
      ],
      "metadata": {
        "id": "u5XhgzYSstMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(0)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "id": "_wERV4ujAEvf",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721316867069,
          "user_tz": -120,
          "elapsed": 4966,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "gather": {
          "logged": 1723230132562
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('..')\n",
        "os.getcwd()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "'/mnt/batch/tasks/shared/LS_root/mounts/clusters/tfm-cpu/code/Users/jose.puche/Scripts'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "id": "20aib2kKsyqP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721316887311,
          "user_tz": -120,
          "elapsed": 20252,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "outputId": "d69fa3d0-832c-4210-9d64-a87d9a6f2afb",
        "gather": {
          "logged": 1723230133124
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 2. Iniciamos sesión para registrar los resultados en wandb\n"
      ],
      "metadata": {
        "id": "b3ym-_u8GQOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "!wandb login 1b8abaacf33b7b5812267384768c22a1eef3c11e"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/azureuser/.netrc\r\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "yC3Z84iGGbTX",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721316937562,
          "user_tz": -120,
          "elapsed": 36699,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "outputId": "1ff09cd9-b5bb-41de-830a-23ec8a983dff",
        "gather": {
          "logged": 1723230064854
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 2. Importación de paquetes\n",
        "\n",
        "Instalamos las librerías necesarias (entre ellas, necesitamos el modelo CLIP, que descargamos directamente desde github), e importamos otras necesarias.\n",
        "\n",
        "También importamos el dataset y el modelo que hemos definido para nuestro problema, y que se encuentran en"
      ],
      "metadata": {
        "id": "OJpJhpNRsvqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, Subset, SubsetRandomSampler, DataLoader\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from tfm_lib.datasets import CustomDataset\n",
        "from tfm_lib.modelos import AudioCLIP\n",
        "from tfm_lib.EarlyStopping import EarlyStopping"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "id": "_uyQrqZKz3Pt",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721317037454,
          "user_tz": -120,
          "elapsed": 3959,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "gather": {
          "logged": 1723230460639
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función de pérdida\n",
        "def loss_fn(logits, labels):\n",
        "    \"\"\"\n",
        "    logits: Las salidas del modelo (predicciones) para cada clase.\n",
        "    labels: Las etiquetas verdaderas (números enteros) para cada ejemplo.\n",
        "    \"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()  # Función de pérdida de entropía cruzada\n",
        "    return criterion(logits, labels)\n",
        "\n",
        "# Ejemplo de cómo usar la función de pérdida\n",
        "logits = torch.tensor([[0.8, 0.1, 0.1], [0.2, 0.7, 0.1], [0.3, 0.2, 0.5]])\n",
        "labels = torch.tensor([0, 1, 2])\n",
        "\n",
        "loss = loss_fn(logits, labels)\n",
        "print(\"Pérdida:\", loss.item())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Pérdida: 0.7991690635681152\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzHNQ6XCpXlf",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721317045748,
          "user_tz": -120,
          "elapsed": 29,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "outputId": "abc646c5-1cb7-4d88-a486-93eca6eaca56",
        "gather": {
          "logged": 1723230148579
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 3. Definición de parámetros y configuración"
      ],
      "metadata": {
        "id": "1PAe7ILdyoPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = './../Final_Database'\n",
        "num_epochs = 20\n",
        "BATCH_SIZE = 16\n",
        "data_augmentation = True\n",
        "da = \"_DA\" if data_augmentation else \"\"\n",
        "lr = 1e-4\n",
        "output_dim = 2\n",
        "selected_model = 'RN50'\n",
        "\n",
        "model_parameters_file = f\"./modelos/multimodal/FULL_{selected_model.replace('/','')}_{output_dim}pers_lr{f'{lr:.0e}'}_bs{BATCH_SIZE}_{num_epochs}ep{da}.pt\"\n",
        "print(model_parameters_file)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "./modelos/multimodal/FULL_RN50_2pers_lr1e-04_bs16_20ep_DA.pt\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4en3ajR6MTnW",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721320046475,
          "user_tz": -120,
          "elapsed": 495,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "outputId": "65318f54-e302-49a7-a3cc-fe0daeae10bf",
        "gather": {
          "logged": 1723230262555
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WandB – Initialize a new run\n",
        "run_name = model_parameters_file.split(\"/\")[-1].replace('.pt', '')\n",
        "wandb.init(entity=\"josealbertoap\", project='TFM', name = run_name, tags=[\"multimodal\"])\n",
        "\n",
        "# WandB – Config is a variable that holds and saves hyperparameters and inputs\n",
        "config = wandb.config          # Initialize config\n",
        "config.batch_size = BATCH_SIZE          # input batch size for training (default: 64)\n",
        "config.test_batch_size = BATCH_SIZE    # input batch size for testing (default: 1000)\n",
        "config.epochs = num_epochs             # number of epochs to train (default: 10)\n",
        "config.lr = lr              # learning rate (default: 0.01)\n",
        "config.momentum = 0          # SGD momentum (default: 0.5)\n",
        "config.no_cuda = True         # disables CUDA training\n",
        "config.seed = 0               # random seed (default: 42)\n",
        "config.log_interval = 1     # how many batches to wait before logging training status\n",
        "config.num_classes = output_dim"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjosealbertoap\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "wandb version 0.17.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.17.5"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/mnt/batch/tasks/shared/LS_root/mounts/clusters/tfm-cpu/code/Users/jose.puche/Scripts/wandb/run-20240809_190518-ma2wfrsd</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/josealbertoap/TFM/runs/ma2wfrsd' target=\"_blank\">FULL_RN50_2pers_lr1e-04_bs16_20ep_DA</a></strong> to <a href='https://wandb.ai/josealbertoap/TFM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/josealbertoap/TFM' target=\"_blank\">https://wandb.ai/josealbertoap/TFM</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/josealbertoap/TFM/runs/ma2wfrsd' target=\"_blank\">https://wandb.ai/josealbertoap/TFM/runs/ma2wfrsd</a>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "id": "Z1N6kbdL2kpi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721317355610,
          "user_tz": -120,
          "elapsed": 2541,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "outputId": "05b53740-587e-46d8-aa38-5fedff721557",
        "gather": {
          "logged": 1723230330228
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 4. Definición de modelo y base de datos"
      ],
      "metadata": {
        "id": "8KjZYpM-zoWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import Resize, Compose, ColorJitter, RandomHorizontalFlip, \\\n",
        "                                   RandomResizedCrop, RandomRotation, Normalize, ToTensor\n",
        "\n",
        "def train_test_dataloaders(folder_path, output_dim, model, data_augmentation=False, BATCH_SIZE=32, test_split=0.2):\n",
        "\n",
        "    dataset = CustomDataset(database_path = folder_path, num_classes = output_dim, image_transform = model.preprocess, audio_transform = None)\n",
        "\n",
        "    train_idx, test_idx = train_test_split(list(range(len(dataset))), test_size=test_split,\n",
        "                                           stratify=dataset.database_info.classID, random_state=42)\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "\n",
        "    # test_subset = Subset(dataset, test_idx) # En caso de que quisiéramos un Dataset y no un Dataloader\n",
        "    test_sampler = SubsetRandomSampler(test_idx)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=test_sampler)\n",
        "\n",
        "    # En caso de tener data augmentation, cambiamos el dataset para el Dataloader de train\n",
        "    if data_augmentation:\n",
        "\n",
        "      augmentation = Compose([\n",
        "            RandomHorizontalFlip(p=0.3),\n",
        "            RandomRotation(degrees=(0, 45), fill=0),\n",
        "            RandomResizedCrop(size=(224, 224), scale=(0.2, 1.0), ratio=(0.8, 1.2)),\n",
        "            # ColorJitter(brightness=.3, contrast=.1, saturation=.1, hue=.1),\n",
        "            ToTensor(),\n",
        "            Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
        "        ])\n",
        "\n",
        "      dataset = CustomDataset(database_path = folder_path, num_classes = output_dim, image_transform = augmentation, audio_transform = None)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
        "\n",
        "    return train_loader, test_loader, dataset.labelencoder.classes_\n",
        "\n",
        "# Por si hay que meter la data augmentation para los audios\n",
        "# aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "id": "q-bNbcxUz9xm",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721320931417,
          "user_tz": -120,
          "elapsed": 5,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "gather": {
          "logged": 1723230354287
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargamos el modelo pre-entrenado y procesador de CLIP\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "model = AudioCLIP(selected_model, device, output_dim).to(device)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "train_loader, test_loader, classes = train_test_dataloaders(pd.read_csv(f'{folder_path}/finalDB_train.csv'),\n",
        "                                                            output_dim, model, data_augmentation, BATCH_SIZE, 0.2)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Device: cuda:0\n"
        }
      ],
      "execution_count": 42,
      "metadata": {
        "id": "yhcgIsqp-HBB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721320941710,
          "user_tz": -120,
          "elapsed": 4570,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "outputId": "bd8cc195-90e5-4b90-b254-bacbf1004ef8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, test_loader, classes = train_test_dataloaders('../Final_Database_mini', output_dim, model, data_augmentation, BATCH_SIZE, 0.1)\n",
        "text_desc = torch.cat([clip.tokenize(f\"a photo of {c}\") for c in classes]).to(device)\n",
        "print(classes)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "['Genesis Reyes Arteaga' 'Jose Alberto Azorin Puche' 'Juan Cuesta Lopez'\n 'Juanjo Bautista Ibanez' 'Maria Jose Morales Forte'\n 'Noelia Sanchez Alonso']\n"
        }
      ],
      "execution_count": 43,
      "metadata": {
        "id": "BBzGYEfC6H-4",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721320944636,
          "user_tz": -120,
          "elapsed": 281,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d520688-c149-4b9e-ce2f-4f1212c2817b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paso 5. Entrenamiento del modelo"
      ],
      "metadata": {
        "id": "Dsqfl1eWziyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicializa el optimizador\n",
        "\n",
        "# from torch.optim.lr_scheduler import StepLR\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
        "# scheduler = StepLR(optimizer, step_size=5, gamma=0.05)\n",
        "# loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "early_stopping = EarlyStopping(patience=10, verbose=True, path=model_parameters_file)\n",
        "\n",
        "train_loss = {}\n",
        "test_loss = {}\n",
        "train_acc = {}\n",
        "test_acc = {}\n",
        "\n",
        "# wandb.watch(model, log=\"all\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    train_steps = tqdm(train_loader, unit=\"batch\")\n",
        "\n",
        "    for images, audios, labels in train_steps:\n",
        "\n",
        "        train_steps.set_description(f\"Epoch [{epoch+1}/{num_epochs}]. Training\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        text_desc = text_desc.to(device)\n",
        "        audios = audios.to(device)\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        output = model(images, text_desc, audios)\n",
        "\n",
        "        # Cálculo de la accuracy\n",
        "        predictions = output.argmax(dim=1, keepdim=True).squeeze()\n",
        "        correct = (predictions == labels).sum().item()\n",
        "\n",
        "        total_samples += labels.size(0)\n",
        "        total_correct += correct\n",
        "\n",
        "        # Cálculo de la función de pérdida y actualización del modelo\n",
        "        loss = loss_fn(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #scheduler.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        train_steps.set_postfix(mean_loss=epoch_loss/total_samples, mean_accuracy = total_correct / total_samples)\n",
        "\n",
        "    train_loss[epoch+1] = epoch_loss / len(train_loader)\n",
        "    train_acc[epoch+1] = total_correct / total_samples\n",
        "\n",
        "    # Evaluación en el conjunto de prueba\n",
        "    model.eval()  # Cambiamos al modo de evaluación\n",
        "    epoch_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    test_steps = tqdm(test_loader, unit=\"batch\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, audios, labels in test_steps:  # Itera sobre los datos de prueba\n",
        "\n",
        "            test_steps.set_description(f\"Epoch [{epoch+1}/{num_epochs}]. Evaluation\")\n",
        "\n",
        "            text_desc = text_desc.to(device)\n",
        "            audios = audios.to(device)\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            output = model(images, text_desc, audios)\n",
        "\n",
        "            # Cálculo de la accuracy\n",
        "            predictions = output.argmax(dim=1, keepdim=True).squeeze()\n",
        "            correct = (predictions == labels).sum().item()\n",
        "\n",
        "            total_samples += labels.size(0)\n",
        "            total_correct += correct\n",
        "\n",
        "            # Cálculo de la función de pérdida y actualización del modelo\n",
        "            loss = loss_fn(output, labels)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            test_steps.set_postfix(mean_loss=epoch_loss/total_samples, mean_accuracy = total_correct / total_samples)\n",
        "\n",
        "        test_loss[epoch+1] = epoch_loss / len(test_loader)\n",
        "        test_acc[epoch+1] = total_correct / total_samples\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}]:')\n",
        "        print(f'- Training. Loss = {train_loss[epoch+1]}; Accuracy = {train_acc[epoch+1]}.')\n",
        "        print(f'- Evaluation. Loss = {test_loss[epoch+1]}; Accuracy = {test_acc[epoch+1]}.')\n",
        "        print()\n",
        "\n",
        "        # Llamar a early_stopping con la pérdida de validación actual y el modelo\n",
        "        early_stopping(test_loss[epoch+1], model)\n",
        "        print('')\n",
        "\n",
        "        # Si se alcanza el criterio de early stopping, romper el bucle\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "# wandb.log({'Training loss': train_loss, 'Training accuracy': train_acc, 'Evaluation loss': test_loss, 'Evaluation accuracy': test_acc})\n",
        "\n",
        "# Guardamos el modelo\n",
        "# wandb.save(model_parameters_file)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Epoch [1/100]. Training: 100%|██████████| 35/35 [00:16<00:00,  2.13batch/s, mean_accuracy=0.379, mean_loss=0.1]\nEpoch [1/100]. Evaluation: 100%|██████████| 4/4 [00:01<00:00,  2.66batch/s, mean_accuracy=0.774, mean_loss=0.0581]\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch [1/100]:\n- Training. Loss = 1.5873861057417733; Accuracy = 0.37906137184115524.\n- Evaluation. Loss = 0.9003170728683472; Accuracy = 0.7741935483870968.\n\nValidation loss decreased (inf --> 0.900317).  Saving model ...\n\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Epoch [2/100]. Training: 100%|██████████| 35/35 [00:17<00:00,  1.96batch/s, mean_accuracy=0.903, mean_loss=0.0411]\nEpoch [2/100]. Evaluation: 100%|██████████| 4/4 [00:02<00:00,  1.92batch/s, mean_accuracy=1, mean_loss=0.0302]\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch [2/100]:\n- Training. Loss = 0.6503684895379203; Accuracy = 0.9025270758122743.\n- Evaluation. Loss = 0.46790148317813873; Accuracy = 1.0.\n\nValidation loss decreased (0.900317 --> 0.467901).  Saving model ...\n\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Epoch [3/100]. Training: 100%|██████████| 35/35 [00:17<00:00,  2.05batch/s, mean_accuracy=0.975, mean_loss=0.0251]\nEpoch [3/100]. Evaluation: 100%|██████████| 4/4 [00:01<00:00,  2.44batch/s, mean_accuracy=1, mean_loss=0.0191]\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch [3/100]:\n- Training. Loss = 0.3970865547657013; Accuracy = 0.9747292418772563.\n- Evaluation. Loss = 0.2966003492474556; Accuracy = 1.0.\n\nValidation loss decreased (0.467901 --> 0.296600).  Saving model ...\n\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Epoch [4/100]. Training: 100%|██████████| 35/35 [00:17<00:00,  1.99batch/s, mean_accuracy=0.982, mean_loss=0.0157]\nEpoch [4/100]. Evaluation: 100%|██████████| 4/4 [00:02<00:00,  1.92batch/s, mean_accuracy=1, mean_loss=0.0109]\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch [4/100]:\n- Training. Loss = 0.24847270590918405; Accuracy = 0.9819494584837545.\n- Evaluation. Loss = 0.16871321201324463; Accuracy = 1.0.\n\nValidation loss decreased (0.296600 --> 0.168713).  Saving model ...\n\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "  0%|          | 0/35 [00:00<?, ?batch/s]\n"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-feddf1fd499b>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtrain_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudios\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mtrain_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch [{epoch+1}/{num_epochs}]. Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-8ab0afa8ee69>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     49\u001b[0m       \u001b[0maud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudioUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrechannel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maud\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[0maud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudioAugmentation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_trunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maud\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m       \u001b[0msgram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudioUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspectro_gram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maud\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_mels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_fft\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhop_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/TFM/Proyecto/Scripts/tfm_lib/audio_processing.py\u001b[0m in \u001b[0;36mspectro_gram\u001b[0;34m(aud, n_mels, n_fft, hop_len)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;31m# spec has shape [channel, n_mels, time], where channel is mono, stereo etc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMelSpectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_fft\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_fft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhop_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhop_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_mels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_mels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;31m# Convert to decibels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchaudio/transforms/_transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m    617\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMel\u001b[0m \u001b[0mfrequency\u001b[0m \u001b[0mspectrogram\u001b[0m \u001b[0mof\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mn_mels\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m         \"\"\"\n\u001b[0;32m--> 619\u001b[0;31m         \u001b[0mspecgram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwaveform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m         \u001b[0mmel_specgram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmel_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspecgram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmel_specgram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchaudio/transforms/_transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mFourier\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0mhops\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m         return F.spectrogram(\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0mwaveform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchaudio/functional/functional.py\u001b[0m in \u001b[0;36mspectrogram\u001b[0;34m(waveform, pad, window, n_fft, hop_length, win_length, power, normalized, center, pad_mode, onesided, return_complex)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;31m# default values are consistent with librosa.core.spectrum._spectrogram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     spec_f = torch.stft(\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwaveform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mn_fft\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_fft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mstft\u001b[0;34m(input, n_fft, hop_length, win_length, window, center, pad_mode, normalized, onesided, return_complex)\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextended_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msignal_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m     return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n\u001b[0m\u001b[1;32m    666\u001b[0m                     normalized, onesided, return_complex)\n\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 946
        },
        "id": "jL5OGRZhBXR0",
        "executionInfo": {
          "status": "error",
          "timestamp": 1721321030627,
          "user_tz": -120,
          "elapsed": 82805,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "outputId": "644a3909-3a4e-4a31-8178-2d575aab801c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "170b3beef2d44b0b88a34e0b1f4222b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run <strong style=\"color:#cdcd00\">chocolate-surf-1</strong> at: <a href='https://wandb.ai/josealbertoap/FULL_ViT-B32_6pers_lr1e-06_bs16_100ep_DA.pt/runs/e1rmx4e0' target=\"_blank\">https://wandb.ai/josealbertoap/FULL_ViT-B32_6pers_lr1e-06_bs16_100ep_DA.pt/runs/e1rmx4e0</a><br/> View project at: <a href='https://wandb.ai/josealbertoap/FULL_ViT-B32_6pers_lr1e-06_bs16_100ep_DA.pt' target=\"_blank\">https://wandb.ai/josealbertoap/FULL_ViT-B32_6pers_lr1e-06_bs16_100ep_DA.pt</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Find logs at: <code>./wandb/run-20240718_154233-e1rmx4e0/logs</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
          },
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {
        "id": "w5374GuwbV2Z",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721319542390,
          "user_tz": -120,
          "elapsed": 4285,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "170b3beef2d44b0b88a34e0b1f4222b3",
            "ff72256106fb48cb9ab1927eed418326",
            "be3df9b759d340279bdcfd3936ecf88e",
            "880e7f5d493b49b0874b42644cb53e94",
            "2f7f50e6edd24047b6ddb1aec2ddc61a",
            "f6656948665744d39b587f1fcf5643a3",
            "e1fbf550fe864c559af5522d12a4b050",
            "69ee02a8a33141ef9f324193fd3a5361"
          ]
        },
        "outputId": "ec3a01a7-ffad-486d-e94f-0d2a9918928b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(*zip(*sorted(train_loss.items())))\n",
        "plt.plot(*zip(*sorted(test_loss.items())))\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "A5wJPETiDHZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pruebas"
      ],
      "metadata": {
        "id": "C94fAdr86VD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "import torch\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from IPython.display import Audio\n",
        "import torchaudio\n",
        "from tfm_lib.audio_processing import AudioUtil, AudioAugmentation\n",
        "from tfm_lib.modelos import AudioCLIP\n",
        "\n",
        "# Descarga el modelo pre-entrenado y procesador de CLIP\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = AudioCLIP(selected_model, device, output_dim).to(device)\n",
        "\n",
        "# model_parameters_file = '/content/drive/MyDrive/TFM/Proyecto/Scripts/modelos/FULL_RN50_6pers_lr1e-06_bs16_20epDA.h5'\n",
        "\n",
        "model.load_state_dict(torch.load(model_parameters_file))\n",
        "model.eval()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 39,
          "data": {
            "text/plain": "AudioCLIP(\n  (clip_model): CLIP(\n    (visual): VisionTransformer(\n      (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (transformer): Transformer(\n        (resblocks): Sequential(\n          (0): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (1): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (2): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (3): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (4): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (5): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (6): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (7): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (8): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (9): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (10): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (11): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (transformer): Transformer(\n      (resblocks): Sequential(\n        (0): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (4): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (5): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (6): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (7): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (8): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (9): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (10): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (11): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (token_embedding): Embedding(49408, 512)\n    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  )\n  (audio_classifier): AudioClassifier(\n    (conv1): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n    (relu1): ReLU()\n    (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (relu2): ReLU()\n    (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (relu3): ReLU()\n    (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv4): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (relu4): ReLU()\n    (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (ap): AdaptiveAvgPool2d(output_size=1)\n    (lin): Linear(in_features=64, out_features=6, bias=True)\n    (conv): Sequential(\n      (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n      (1): ReLU()\n      (2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (4): ReLU()\n      (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (6): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (7): ReLU()\n      (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (9): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (10): ReLU()\n      (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (fc1): Linear(in_features=12, out_features=6, bias=True)\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1iJNa356d8_",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721320875599,
          "user_tz": -120,
          "elapsed": 6953,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "outputId": "fe5f577c-3516-4375-aaed-a5b01733603a",
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read_image = Image.open('/content/drive/MyDrive/TFM/Proyecto/Final_Database_mini/image/Jose Alberto Azorin Puche/frame00001.jpg')\n",
        "read_image = Image.open('/content/drive/MyDrive/TFM/Proyecto/IMG_0003.jpg')\n",
        "aud = AudioUtil.open('/content/drive/MyDrive/TFM/Proyecto/Final_Database_mini/audio/Jose Alberto Azorin Puche/audio0000.ogg')\n",
        "aud = AudioUtil.resample(aud, 48000)\n",
        "aud = AudioUtil.rechannel(aud, 1)\n",
        "aud = AudioAugmentation.pad_trunc(aud, 4)\n",
        "sgram = AudioUtil.spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None).unsqueeze(0).to(device)\n",
        "\n",
        "image = model.preprocess(read_image).unsqueeze(0).to(device)\n",
        "people = ['Genesis Reyes Arteaga', 'Jose Alberto Azorin Puche', 'Juan Cuesta Lopez',\n",
        "          'Juanjo Bautista Ibanez', 'Maria Jose Morales Forte', 'Noelia Sanchez Alonso']\n",
        "text = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in people]).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "  output = model(image, text, sgram)\n",
        "\n",
        "print(output.softmax(dim=-1))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "tensor([[0.0019, 0.6802, 0.0247, 0.0222, 0.2579, 0.0132]], device='cuda:0')\n"
        }
      ],
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNVfdBm1ANOt",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721320876288,
          "user_tz": -120,
          "elapsed": 378,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        },
        "outputId": "80ef448c-e67e-4c7f-dfd7-126170ffc0e5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "w95DUfanCeMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, database_path='', num_classes=40, image_transform=None, audio_transform=None):\n",
        "\n",
        "      # Atributos derivados de los parametros\n",
        "      self.database_path = database_path\n",
        "      self.image_transform = image_transform\n",
        "      self.audio_transform = audio_transform\n",
        "      self.num_classes = num_classes\n",
        "      self.database_info = self.filter_classes(pd.read_csv(f'{database_path}/final_db.csv'), num_classes).iloc[::5]\n",
        "      self.database_info.reset_index(drop=True, inplace=True)\n",
        "      self.classes = list(self.database_info['classID'].unique())\n",
        "\n",
        "      # Atributos relacionados con audio\n",
        "      self.duration = 4\n",
        "      self.sr = 48000\n",
        "      self.channel = 1\n",
        "      self.shift_pct = 0.7\n",
        "\n",
        "      # Codificación de las clases en valores numéricos\n",
        "      le = preprocessing.LabelEncoder()\n",
        "      self.labelencoder = le.fit(self.database_info[\"classID\"])\n",
        "\n",
        "    # --------------------------------\n",
        "    # Numero de elementos del dataset\n",
        "    # --------------------------------\n",
        "    def __len__(self):\n",
        "      return len(self.database_info)\n",
        "\n",
        "    # --------------------------------\n",
        "    # Selección del elemento i-esimo\n",
        "    # --------------------------------\n",
        "    def __getitem__(self, idx):\n",
        "      # Definición de los paths donde leer imagen y audio del i-esimo dato\n",
        "      image_file = self.database_info.loc[idx, 'image_path']\n",
        "      audio_file = self.database_info.loc[idx, 'audio_path']\n",
        "\n",
        "      # Obtenemos la etiqueta para el i-esimo dato\n",
        "      db_df = self.database_info.copy()\n",
        "      db_df['classID'] = self.labelencoder.transform(db_df['classID'])\n",
        "      class_id = db_df.loc[idx, 'classID']\n",
        "\n",
        "      # Obtención del tensor correspondiente a la i-esima imagen\n",
        "      read_image = Image.open(image_file)\n",
        "      image = self.image_transform(read_image)\n",
        "\n",
        "      # Obtención del espectrograma del audio correspondiente al i-esimo dato\n",
        "      aud = AudioUtil.open(audio_file)\n",
        "      aud = AudioUtil.resample(aud, self.sr)\n",
        "      aud = AudioUtil.rechannel(aud, self.channel)\n",
        "      aud = AudioAugmentation.pad_trunc(aud, self.duration)\n",
        "      sgram = AudioUtil.spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None)\n",
        "\n",
        "      return image, sgram, class_id\n",
        "\n",
        "    # ----------------------------\n",
        "    # Filter only the number of classes of interest\n",
        "    # ----------------------------\n",
        "    def filter_classes(self, df, num_classes):\n",
        "      # Obtener todas las clases únicas\n",
        "      unique_classes = df['classID'].unique()\n",
        "\n",
        "      # Seleccionar las primeras num_classes clases\n",
        "      selected_classes = list(unique_classes)[:num_classes]\n",
        "\n",
        "      # Filtrar el DataFrame para incluir solo las clases seleccionadas\n",
        "      filtered_df = df[df['classID'].isin(selected_classes)]\n",
        "\n",
        "      return filtered_df"
      ],
      "outputs": [],
      "execution_count": 29,
      "metadata": {
        "id": "03GtrcNBrVE3",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721320232358,
          "user_tz": -120,
          "elapsed": 294,
          "user": {
            "displayName": "JOSÉ ALBERTO AZORIN PUCHE",
            "userId": "07780853208545474625"
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "fHQ9yu3MwO75"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "u5XhgzYSstMq",
        "OJpJhpNRsvqj",
        "1PAe7ILdyoPt",
        "8KjZYpM-zoWn",
        "Dsqfl1eWziyb"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNdoDSWLx0t06VSUizuGF13"
    },
    "accelerator": "GPU",
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "170b3beef2d44b0b88a34e0b1f4222b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_model_module": "@jupyter-widgets/controls",
            "_view_name": "VBoxView",
            "_view_module": "@jupyter-widgets/controls",
            "_dom_classes": [],
            "layout": "IPY_MODEL_880e7f5d493b49b0874b42644cb53e94",
            "_view_count": null,
            "_model_module_version": "1.5.0",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff72256106fb48cb9ab1927eed418326",
              "IPY_MODEL_be3df9b759d340279bdcfd3936ecf88e"
            ]
          }
        },
        "ff72256106fb48cb9ab1927eed418326": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_name": "LabelModel",
            "_model_module": "@jupyter-widgets/controls",
            "_view_name": "LabelView",
            "_view_module": "@jupyter-widgets/controls",
            "_dom_classes": [],
            "layout": "IPY_MODEL_2f7f50e6edd24047b6ddb1aec2ddc61a",
            "value": "0.017 MB of 0.017 MB uploaded\r",
            "style": "IPY_MODEL_f6656948665744d39b587f1fcf5643a3",
            "placeholder": "​",
            "_view_count": null,
            "_model_module_version": "1.5.0",
            "description": ""
          }
        },
        "be3df9b759d340279bdcfd3936ecf88e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_name": "FloatProgressModel",
            "_model_module": "@jupyter-widgets/controls",
            "max": 1,
            "bar_style": "",
            "_view_name": "ProgressView",
            "_view_module": "@jupyter-widgets/controls",
            "_dom_classes": [],
            "layout": "IPY_MODEL_e1fbf550fe864c559af5522d12a4b050",
            "orientation": "horizontal",
            "value": 1,
            "style": "IPY_MODEL_69ee02a8a33141ef9f324193fd3a5361",
            "min": 0,
            "_view_count": null,
            "_model_module_version": "1.5.0",
            "description": ""
          }
        },
        "880e7f5d493b49b0874b42644cb53e94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "grid_row": null,
            "_model_module": "@jupyter-widgets/base",
            "overflow": null,
            "max_height": null,
            "display": null,
            "grid_auto_flow": null,
            "grid_template_rows": null,
            "align_self": null,
            "grid_auto_columns": null,
            "width": null,
            "grid_area": null,
            "align_items": null,
            "_view_name": "LayoutView",
            "left": null,
            "height": null,
            "_view_module": "@jupyter-widgets/base",
            "object_position": null,
            "justify_content": null,
            "bottom": null,
            "max_width": null,
            "border": null,
            "margin": null,
            "order": null,
            "grid_column": null,
            "grid_auto_rows": null,
            "padding": null,
            "grid_template_columns": null,
            "justify_items": null,
            "object_fit": null,
            "visibility": null,
            "_view_count": null,
            "flex_flow": null,
            "min_height": null,
            "top": null,
            "min_width": null,
            "flex": null,
            "_model_module_version": "1.2.0",
            "grid_template_areas": null,
            "overflow_x": null,
            "right": null,
            "overflow_y": null,
            "grid_gap": null,
            "align_content": null
          }
        },
        "2f7f50e6edd24047b6ddb1aec2ddc61a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "grid_row": null,
            "_model_module": "@jupyter-widgets/base",
            "overflow": null,
            "max_height": null,
            "display": null,
            "grid_auto_flow": null,
            "grid_template_rows": null,
            "align_self": null,
            "grid_auto_columns": null,
            "width": null,
            "grid_area": null,
            "align_items": null,
            "_view_name": "LayoutView",
            "left": null,
            "height": null,
            "_view_module": "@jupyter-widgets/base",
            "object_position": null,
            "justify_content": null,
            "bottom": null,
            "max_width": null,
            "border": null,
            "margin": null,
            "order": null,
            "grid_column": null,
            "grid_auto_rows": null,
            "padding": null,
            "grid_template_columns": null,
            "justify_items": null,
            "object_fit": null,
            "visibility": null,
            "_view_count": null,
            "flex_flow": null,
            "min_height": null,
            "top": null,
            "min_width": null,
            "flex": null,
            "_model_module_version": "1.2.0",
            "grid_template_areas": null,
            "overflow_x": null,
            "right": null,
            "overflow_y": null,
            "grid_gap": null,
            "align_content": null
          }
        },
        "f6656948665744d39b587f1fcf5643a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1fbf550fe864c559af5522d12a4b050": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "grid_row": null,
            "_model_module": "@jupyter-widgets/base",
            "overflow": null,
            "max_height": null,
            "display": null,
            "grid_auto_flow": null,
            "grid_template_rows": null,
            "align_self": null,
            "grid_auto_columns": null,
            "width": null,
            "grid_area": null,
            "align_items": null,
            "_view_name": "LayoutView",
            "left": null,
            "height": null,
            "_view_module": "@jupyter-widgets/base",
            "object_position": null,
            "justify_content": null,
            "bottom": null,
            "max_width": null,
            "border": null,
            "margin": null,
            "order": null,
            "grid_column": null,
            "grid_auto_rows": null,
            "padding": null,
            "grid_template_columns": null,
            "justify_items": null,
            "object_fit": null,
            "visibility": null,
            "_view_count": null,
            "flex_flow": null,
            "min_height": null,
            "top": null,
            "min_width": null,
            "flex": null,
            "_model_module_version": "1.2.0",
            "grid_template_areas": null,
            "overflow_x": null,
            "right": null,
            "overflow_y": null,
            "grid_gap": null,
            "align_content": null
          }
        },
        "69ee02a8a33141ef9f324193fd3a5361": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "ProgressStyleModel",
            "_model_module": "@jupyter-widgets/controls",
            "description_width": "",
            "_view_name": "StyleView",
            "_view_module": "@jupyter-widgets/base",
            "_view_count": null,
            "bar_color": null,
            "_model_module_version": "1.5.0"
          }
        }
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.18",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}